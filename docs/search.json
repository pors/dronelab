[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hey, I‚Äôm Mark. On this blog, dronelab.dev, I‚Äôm exploring machine learning by teaching a drone how to fly itself. I break things down into small steps and share what I discover along the way.\nHope you find it interesting! Drop me a line if you do: mark [at] pors [dot] net.\nFind me on socials below üëá"
  },
  {
    "objectID": "posts/gamesir-t1d-controller/index.html",
    "href": "posts/gamesir-t1d-controller/index.html",
    "title": "GameSir T1d controller & pygame",
    "section": "",
    "text": "Mark Pors\n\n2025-05-02\nControlling a drone with a keyboard is not great, neither is using the touch-screen of a phone. That‚Äôs why I bought the recommended controller for the Tello: the GameSir T1d."
  },
  {
    "objectID": "posts/gamesir-t1d-controller/index.html#hook-up-the-gamesir-t1d",
    "href": "posts/gamesir-t1d-controller/index.html#hook-up-the-gamesir-t1d",
    "title": "GameSir T1d controller & pygame",
    "section": "Hook up the GameSir T1d",
    "text": "Hook up the GameSir T1d\nInitially this seemed like a no-brainer, and I was surprised why no-one else hadn‚Äôt done it yet: replace the keyboard strokes with signals from the GameSir T1d controller. It turns out that this controller was specifically modified to connect ONLY through the Tello app.\nNormally the code below is enough to connect to a game controller from a python script:\nimport pygame\n\npygame.init()\npygame.joystick.init()\n\n# Check for connected controllers\njoystick_count = pygame.joystick.get_count()\n\nif joystick_count == 0:\n    print(\"No controller detected. Please connect your GameSir T1d controller.\")\n    return None\n\n# Initialize the first controller detected\njoystick = pygame.joystick.Joystick(0)\njoystick.init()\nIn our case the output was:\nNo controller detected. Please connect your GameSir T1d controller.\nOh oh, problem!"
  },
  {
    "objectID": "posts/gamesir-t1d-controller/index.html#hack-it",
    "href": "posts/gamesir-t1d-controller/index.html#hack-it",
    "title": "GameSir T1d controller & pygame",
    "section": "Hack it!",
    "text": "Hack it!\nAfter some searching and LLM‚Äôing it became clear that there is a way to make it work.\nPfew!\nFor Internet historians, here is the evolution of the hack:\n\noriginal Python 2 script\nported to Python 3\nusing bleak\nrefactor"
  },
  {
    "objectID": "posts/gamesir-t1d-controller/index.html#gamesir-t1d-hack-version-5",
    "href": "posts/gamesir-t1d-controller/index.html#gamesir-t1d-hack-version-5",
    "title": "GameSir T1d controller & pygame",
    "section": "GameSir T1D hack version 5",
    "text": "GameSir T1D hack version 5\nI‚Äôm honored to step in the footsteps of these four hackers and try to get iteration five to work!\nFirst we need to get the controller to connect. Simply pairing via Bluetooth won‚Äôt work.\nThe GameSir T1d gets into pairing mode by simply powering it on. The four blue power LEDs start blinking when trying to connect. At that state we run the script below (on github: gamesirT1d-connect.py).\n\n\n\n\n\n\nTip\n\n\n\nThere is a tiny button labeled pair above another tiny button labeled C1. This pair button can be used to pair a new device (great UX!). Clicking it while connected, will cause the Bluetooth connection to be dropped. So don‚Äôt click it while controlling a drone!\n\n\n\nBLE Controller Connection PreviewFull scriptOutput\n\n\nimport asyncio\nfrom bleak import BleakClient, BleakScanner\n\n# The name our controller should broadcast as\nCONTROLLER_NAME = \"Gamesir-T1d\"\n\nasync def main():\n    print(\"Starting BLE scan for GameSir-T1d controller...\")\n\n\nimport asyncio\nfrom bleak import BleakClient, BleakScanner\n\n# The name our controller should broadcast as\nCONTROLLER_NAME = \"Gamesir-T1d\"\n\nasync def main():\n    print(\"Starting BLE scan for GameSir-T1d controller...\")\n\n    # First, scan for all available BLE devices\n    print(\"Scanning for BLE devices...\")\n    devices = await BleakScanner.discover()\n\n    # Print all found devices to help with debugging\n    print(f\"Found {len(devices)} Bluetooth devices:\")\n    for i, device in enumerate(devices):\n        print(f\"{i+1}. Name: {device.name}, Address: {device.address}\")\n\n    # Try to find our controller\n    target_device = None\n    for device in devices:\n        if device.name and CONTROLLER_NAME.lower() in device.name.lower():\n            target_device = device\n            print(f\"Found controller: {device.name}, Address: {device.address}\")\n            break\n\n    if not target_device:\n        print(f\"No device found with name containing '{CONTROLLER_NAME}'\")\n        print(\"Is the controller turned on and in pairing mode?\")\n        return\n\n    # Try to connect to the controller\n    print(f\"Attempting to connect to {target_device.name}...\")\n    try:\n        async with BleakClient(target_device.address, timeout=10.0) as client:\n            if client.is_connected:\n                print(f\"Successfully connected to {target_device.name}!\")\n\n                # List available services and characteristics\n                print(\"\\nAvailable services and characteristics:\")\n                for service in client.services:\n                    print(f\"Service: {service.uuid}\")\n                    for char in service.characteristics:\n                        print(f\"  Characteristic: {char.uuid}\")\n                        print(f\"    Properties: {char.properties}\")\n\n                # Wait a moment so we can see the connection is established\n                print(\"\\nConnection successful. Press Ctrl+C to exit...\")\n                await asyncio.sleep(10)\n            else:\n                print(\"Failed to connect\")\n    except Exception as e:\n        print(f\"Error connecting to device: {e}\")\n\n\nif __name__ == \"__main__\":\n    # Make sure controller is in pairing mode before running this\n    print(\"Make sure the GameSir-T1d controller is turned on and in pairing mode.\")\n    print(\"(Typically hold power button until LEDs flash rapidly)\")\n    input(\"Press Enter to start scanning...\")\n\n    # Run the async main function\n    asyncio.run(main())\n\n\nMake sure the GameSir-T1d controller is turned on and in pairing mode.\n(Typically hold power button until LEDs flash rapidly)\nPress Enter to start scanning...\nStarting BLE scan for GameSir-T1d controller...\nScanning for BLE devices...\nFound 11 Bluetooth devices:\n1. Name: Gamesir-T1d-39BD, Address: FDF00BC3-1DEE-1525-0B34-7E2D3391C401\n2. Name: None, Address: 3A2C8191-D3F5-F471-BC81-75AFE2DB0D60\n3. Name: None, Address: 772F5433-AAE9-D456-209C-DEA32D192E10\n...\n11. Name: None, Address: 80334171-1943-B3DE-7DDD-773753B852C3\nFound controller: Gamesir-T1d-39BD, Address: FDF00BC3-1DEE-1525-0B34-7E2D3391C401\nAttempting to connect to Gamesir-T1d-39BD...\nSuccessfully connected to Gamesir-T1d-39BD!\n\nAvailable services and characteristics:\nService: 00008650-0000-1000-8000-00805f9b34fb\n  Characteristic: 00008651-0000-1000-8000-00805f9b34fb\n    Properties: ['read', 'write', 'notify', 'indicate']\n  Characteristic: 00008655-0000-1000-8000-00805f9b34fb\n    Properties: ['read', 'write', 'notify', 'indicate']\n  Characteristic: 0000865f-0000-1000-8000-00805f9b34fb\n    Properties: ['read', 'write', 'notify', 'indicate']\nService: 0000180a-0000-1000-8000-00805f9b34fb\n  Characteristic: 00002a24-0000-1000-8000-00805f9b34fb\n    Properties: ['read']\n  Characteristic: 00002a25-0000-1000-8000-00805f9b34fb\n    Properties: ['read']\n  Characteristic: 00002a27-0000-1000-8000-00805f9b34fb\n    Properties: ['read']\n  Characteristic: 00002a26-0000-1000-8000-00805f9b34fb\n    Properties: ['read']\n  Characteristic: 00002a50-0000-1000-8000-00805f9b34fb\n    Properties: ['read']\n\nConnection successful. Press Ctrl+C to exit...\n\n\n\nIf you have inspected the output tab, you see we succeeded! Yay, let‚Äôs move on."
  },
  {
    "objectID": "posts/gamesir-t1d-controller/index.html#read-the-t1d-controller-state",
    "href": "posts/gamesir-t1d-controller/index.html#read-the-t1d-controller-state",
    "title": "GameSir T1d controller & pygame",
    "section": "Read the T1d controller state",
    "text": "Read the T1d controller state\nNow the controller is connected to the computer, let‚Äôs see if we can read joystick and button changes.\nThere‚Äôs a heap of input elements that need to be read and made compatible with gamepy:\n\nBoth joysticks (LX, LY, RX, RY)\nAnalog triggers (L2, R2)\nButtons (A, B, X, Y, L1, R1, C1, C2, Menu)\nD-pad (Up, Down, Left, Right)\n\nThe script below prints the real-time value of each of these inputs while using the controller (on github: gamesirT1d-read.py).\nFrom here on we don‚Äôt need to pair the controller, we just use the device name that was identified in the previous step.\n\nRead Controller State PreviewFull scriptOutput\n\n\nimport asyncio\nfrom bleak import BleakClient, BleakScanner\n\n# The exact name our controller showed up as\nCONTROLLER_NAME = \"Gamesir-T1d-39BD\"\n# The characteristic we want to read\nCHARACTERISTIC_UUID = \"00008651-0000-1000-8000-00805f9b34fb\"\n\nclass GameSirT1d:\n\n\nimport asyncio\nfrom bleak import BleakClient, BleakScanner\n\n# The exact name our controller showed up as\nCONTROLLER_NAME = \"Gamesir-T1d-39BD\"\n# The characteristic we want to read\nCHARACTERISTIC_UUID = \"00008651-0000-1000-8000-00805f9b34fb\"\n\nclass GameSirT1d:\n    def __init__(self):\n        # Joystick values (0-1023, with 512 as center)\n        self.lx = 512\n        self.ly = 512\n        self.rx = 512\n        self.ry = 512\n        \n        # Analog triggers (0-255)\n        self.l2 = 0\n        self.r2 = 0\n        \n        # Digital buttons (0 or 1)\n        self.a = 0\n        self.b = 0\n        self.x = 0\n        self.y = 0\n        self.l1 = 0\n        self.r1 = 0\n        self.c1 = 0\n        self.c2 = 0\n        self.menu = 0\n        \n        # D-pad\n        self.dpad_up = 0\n        self.dpad_down = 0\n        self.dpad_left = 0\n        self.dpad_right = 0\n        \n        # Connection state\n        self.connected = False\n        self._client = None\n    \n    def parse_data(self, data):\n        \"\"\"Parse the raw data from the controller\"\"\"\n        if len(data) &lt; 12:\n            return False\n        \n        # Parse joysticks\n        self.lx = ((data[2]) &lt;&lt; 2) | (data[3] &gt;&gt; 6)\n        self.ly = ((data[3] & 0x3f) &lt;&lt; 4) + (data[4] &gt;&gt; 4)\n        self.rx = ((data[4] & 0xf) &lt;&lt; 6) | (data[5] &gt;&gt; 2)\n        self.ry = ((data[5] & 0x3) &lt;&lt; 8) + ((data[6]))\n        \n        # Parse triggers\n        self.l2 = data[7]\n        self.r2 = data[8]\n        \n        # Parse buttons from byte 9\n        buttons = data[9]\n        self.a = int(bool(buttons & 0x01))\n        self.b = int(bool(buttons & 0x02))\n        self.menu = int(bool(buttons & 0x04))\n        self.x = int(bool(buttons & 0x08))\n        self.y = int(bool(buttons & 0x10))\n        self.l1 = int(bool(buttons & 0x40))\n        self.r1 = int(bool(buttons & 0x80))\n        \n        # Parse more buttons from byte 10\n        buttons2 = data[10]\n        self.c1 = int(bool(buttons2 & 0x04))\n        self.c2 = int(bool(buttons2 & 0x08))\n        \n        # Parse D-pad from byte 11\n        dpad = data[11]\n        self.dpad_up = int(dpad == 0x01)\n        self.dpad_right = int(dpad == 0x03)\n        self.dpad_down = int(dpad == 0x05)\n        self.dpad_left = int(dpad == 0x07)\n        \n        return True\n    \n    def __str__(self):\n        \"\"\"Return a string representation of the controller state\"\"\"\n        return (\n            f\"Joysticks: LX={self.lx}, LY={self.ly}, RX={self.rx}, RY={self.ry}\\n\"\n            f\"Triggers: L2={self.l2}, R2={self.r2}\\n\"\n            f\"Buttons: A={self.a}, B={self.b}, X={self.x}, Y={self.y}, \"\n            f\"L1={self.l1}, R1={self.r1}, C1={self.c1}, C2={self.c2}, Menu={self.menu}\\n\"\n            f\"D-pad: Up={self.dpad_up}, Down={self.dpad_down}, Left={self.dpad_left}, Right={self.dpad_right}\"\n        )\n    \n    # Add methods to get normalized values (-1.0 to 1.0) for joysticks\n    def get_left_stick(self):\n        \"\"\"Get normalized values for left stick (-1.0 to 1.0)\"\"\"\n        x = (self.lx - 512) / 512  # -1.0 to 1.0\n        y = (self.ly - 512) / 512  # -1.0 to 1.0\n        return (x, y)\n    \n    def get_right_stick(self):\n        \"\"\"Get normalized values for right stick (-1.0 to 1.0)\"\"\"\n        x = (self.rx - 512) / 512  # -1.0 to 1.0\n        y = (self.ry - 512) / 512  # -1.0 to 1.0\n        return (x, y)\n\nasync def main():\n    controller = GameSirT1d()\n    \n    print(f\"Scanning for {CONTROLLER_NAME}...\")\n    device = await BleakScanner.find_device_by_name(CONTROLLER_NAME)\n    \n    if not device:\n        print(f\"Could not find {CONTROLLER_NAME}. Is it turned on?\")\n        return\n    \n    print(f\"Found {CONTROLLER_NAME} at {device.address}\")\n    print(\"Connecting...\")\n    \n    try:\n        async with BleakClient(device.address) as client:\n            print(\"Connected!\")\n            controller.connected = True\n            controller._client = client\n            \n            try:\n                while controller.connected:\n                    # Read current state\n                    data = await client.read_gatt_char(CHARACTERISTIC_UUID)\n                    \n                    # Parse the data\n                    if controller.parse_data(data):\n                        # Get normalized stick values for easier use\n                        left_x, left_y = controller.get_left_stick()\n                        right_x, right_y = controller.get_right_stick()\n                        \n                        # Clear the line and print current state\n                        print(f\"\\rLeft: ({left_x:.2f}, {left_y:.2f}) Right: ({right_x:.2f}, {right_y:.2f}) | \"\n                              f\"A:{controller.a} B:{controller.b} X:{controller.x} Y:{controller.y} \"\n                              f\"L1:{controller.l1} R1:{controller.r1} L2:{controller.l2} R2:{controller.r2}\", end=\"\")\n                    \n                    # Wait a bit before next reading\n                    await asyncio.sleep(0.05)  # 20Hz polling rate\n                    \n            except KeyboardInterrupt:\n                print(\"\\nStopping...\")\n                controller.connected = False\n    \n    except Exception as e:\n        print(f\"\\nError: {e}\")\n        controller.connected = False\n\nif __name__ == \"__main__\":\n    print(\"GameSir T1d Controller Test\")\n    print(\"Move joysticks and press buttons to see values\")\n    print(\"Press Ctrl+C to exit\")\n    \n    asyncio.run(main())\n\n\nGameSir T1d Controller Test\nMove joysticks and press buttons to see values\nPress Ctrl+C to exit\nScanning for Gamesir-T1d-39BD...\nFound Gamesir-T1d-39BD at FDF00BC3-1DEE-1525-0B34-7E2D3391C401\nConnecting...\nConnected!\nLeft: (0.05, 0.03) Right: (-0.10, -0.13) | A:1 B:0 X:0 Y:0 L1:0 R1:0 L2:3 R2:155\n\n\n\nThat seems to work pretty good! Now we can move on and create a wrapper that behaves as if it was part of a gamepy compatible controller."
  },
  {
    "objectID": "posts/gamesir-t1d-controller/index.html#gamesir-t1d-pygame-compatible-wrapper",
    "href": "posts/gamesir-t1d-controller/index.html#gamesir-t1d-pygame-compatible-wrapper",
    "title": "GameSir T1d controller & pygame",
    "section": "GameSir T1d Pygame-Compatible Wrapper",
    "text": "GameSir T1d Pygame-Compatible Wrapper\nThe wrapper functions as a bridge between two worlds:\n\nThe BLE communication layer that talks directly to our GameSir T1d\nA pygame-compatible interface that provides familiar methods like get_axis() and get_button()\n\nThis allows our drone control code to interact with the controller as if it were a standard pygame joystick, while the BLE communication happens behind the scenes.\nThe wrapper code consists of two classes:\n\nGameSirT1d: this class parses the raw inputs from the controller and converts it to the format and ranges we expect in a pygame controller.\nGameSirT1dPygame: this class implements the BLE interface and provides the pygame compatible wrapper.\n\nThe code can be found here: gamesir_t1d_pygame.py."
  },
  {
    "objectID": "posts/gamesir-t1d-controller/index.html#lets-try-it-out",
    "href": "posts/gamesir-t1d-controller/index.html#lets-try-it-out",
    "title": "GameSir T1d controller & pygame",
    "section": "Let‚Äôs try it out!",
    "text": "Let‚Äôs try it out!\nWith this little example script we can see if the controller (at least the thumpsticks) does what we expect:\nimport pygame\nfrom gamesir_t1d_pygame import GameSirT1dPygame\n\n\ndef main(controller_name):\n    # Initialize pygame for window and graphics\n    pygame.init()\n    screen = pygame.display.set_mode((640, 480))\n    pygame.display.set_caption(\"GameSir T1d Test\")\n    clock = pygame.time.Clock()\n\n    # Initialize our custom controller\n    controller = GameSirT1dPygame(controller_name)\n    print(\"Connecting to controller...\")\n    if not controller.init():\n        print(\"Failed to connect to controller\")\n        return\n\n    running = True\n    while running:\n        # Process pygame events\n        for event in pygame.event.get():\n            if event.type == pygame.QUIT:\n                running = False\n\n        # Read joystick values\n        left_x = controller.get_axis(0)\n        left_y = controller.get_axis(1)\n        right_x = controller.get_axis(2)\n        right_y = controller.get_axis(3)\n\n        # Clear screen\n        screen.fill((0, 0, 0))\n\n        # Draw joystick positions\n        pygame.draw.circle(\n            screen, (50, 50, 50), (160, 240), 100\n        )  # Left stick background\n        pygame.draw.circle(\n            screen, (0, 255, 0), (160 + int(left_x * 80), 240 + int(left_y * 80)), 20\n        )  # Left stick position\n\n        pygame.draw.circle(\n            screen, (50, 50, 50), (480, 240), 100\n        )  # Right stick background\n        pygame.draw.circle(\n            screen, (0, 255, 0), (480 + int(right_x * 80), 240 + int(right_y * 80)), 20\n        )  # Right stick position\n\n        # Update display\n        pygame.display.flip()\n\n        # Control frame rate\n        clock.tick(60)\n\n    # Clean up\n    controller.quit()\n    pygame.quit()\n\nif __name__ == \"__main__\":\n    main(\"Gamesir-T1d-39BD\") # replace with the name of your T1d\nLook at that! Pretty responsive!"
  },
  {
    "objectID": "posts/gamesir-t1d-controller/index.html#whats-next",
    "href": "posts/gamesir-t1d-controller/index.html#whats-next",
    "title": "GameSir T1d controller & pygame",
    "section": "What‚Äôs next?",
    "text": "What‚Äôs next?\nNow that we have the T1d working with pygame we can implement the interface with the Tello. Read on‚Ä¶\n\n\n\n\n\n\n\nSeries: Code, Fly & AI\n\n\n\n‚Üê Previous: Key navigation & video snapshots\n\n\nNext: Tello controller navigation - Part 1 ‚Üí"
  },
  {
    "objectID": "posts/enter-the-tello/index.html",
    "href": "posts/enter-the-tello/index.html",
    "title": "Enter the Tello",
    "section": "",
    "text": "Mark Pors\n\n2025-04-20"
  },
  {
    "objectID": "posts/enter-the-tello/index.html#combine-the-stuff-you-like",
    "href": "posts/enter-the-tello/index.html#combine-the-stuff-you-like",
    "title": "Enter the Tello",
    "section": "Combine the stuff you like",
    "text": "Combine the stuff you like\n‚ÄúCombine the stuff you like‚Äù is good advice when looking for a new project to work on. I like to follow good advice and so I brainstormed a bit with ChatGPT to find interesting intersections between my interests. Some of my favorite activities ‚Äî like sex, meditation, and reading ‚Äî didn‚Äôt quite lend themselves to computer vision projects. Others had more potential: hiking in nature, coding, learning about new tech."
  },
  {
    "objectID": "posts/enter-the-tello/index.html#autonomous-scouting-drone",
    "href": "posts/enter-the-tello/index.html#autonomous-scouting-drone",
    "title": "Enter the Tello",
    "section": "Autonomous scouting drone",
    "text": "Autonomous scouting drone\nHaving a drone accompany us on hikes to do some scouting ahead hits the mark for me. It includes hiking and coding, and most important: I need to learn a lot to make a drone do what I want. I have zero experience with drones and very limited experience with computer vision models."
  },
  {
    "objectID": "posts/enter-the-tello/index.html#getting-ready-big-time",
    "href": "posts/enter-the-tello/index.html#getting-ready-big-time",
    "title": "Enter the Tello",
    "section": "Getting ready big time",
    "text": "Getting ready big time\nLike any self-respecting tech nerd, I decided to kick things off in style:\n\nBuy a domain name (most important!)\nGet a logo (as important!)\nBuy the best dev/DIY drone out there\nBuy a top line PC to train models\nBuy another PC to run simulation environments\n\nOK, guilty ‚Äî I bought the domain (hello dronelab.dev), but tried to stop myself there 1 2. I don‚Äôt know anything about drones, and might get bored with the project after a couple of weeks, so better start small."
  },
  {
    "objectID": "posts/enter-the-tello/index.html#enter-the-tello",
    "href": "posts/enter-the-tello/index.html#enter-the-tello",
    "title": "Enter the Tello",
    "section": "Enter the Tello",
    "text": "Enter the Tello\nI stumbled upon a modest little drone from way back in 2018 3 ‚Äî the DJI Tello ‚Äî and it was perfect for my first steps. It can fly indoors, is programmable and has a camera.\nThere is an even better version of it with some extra goodies that might come in handy:\n\nan onboard processor\na matrix display\na distance sensor\n\nSo I bought the DJI RoboMaster TT. TT stands for Tello Talent ‚Äî basically smart kids coding drones at an age I was still figuring out crayons.\nLook at that beauty!\n\n\n\nDJI RoboMaster TT"
  },
  {
    "objectID": "posts/enter-the-tello/index.html#whats-next",
    "href": "posts/enter-the-tello/index.html#whats-next",
    "title": "Enter the Tello",
    "section": "What‚Äôs next?",
    "text": "What‚Äôs next?\nIn the next episode I will play with the RoboMaster TT, both with and without code to control it. It includes different ways to crash it. Read on‚Ä¶\n\n\n\n\n\n\n\nSeries: Code, Fly & AI\n\n\n\n\n\n\nNext: Crash the Tello ‚Üí"
  },
  {
    "objectID": "posts/enter-the-tello/index.html#footnotes",
    "href": "posts/enter-the-tello/index.html#footnotes",
    "title": "Enter the Tello",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI always buy the domain first, even before I know what I‚Äôm doing.‚Ü©Ô∏é\nI didn‚Äôt really stop myself there, I also created a logo. Technically ChatGPT did: ‚Ü©Ô∏é\n2018, also the last time I blogged something: decentralized.blog.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/tello-controller-navigation-part-1/index.html",
    "href": "posts/tello-controller-navigation-part-1/index.html",
    "title": "Tello controller navigation - Part 1",
    "section": "",
    "text": "Mark Pors\n\n2025-05-07"
  },
  {
    "objectID": "posts/tello-controller-navigation-part-1/index.html#what-happened-with-the-tello",
    "href": "posts/tello-controller-navigation-part-1/index.html#what-happened-with-the-tello",
    "title": "Tello controller navigation - Part 1",
    "section": "What happened with the Tello?",
    "text": "What happened with the Tello?\nThe whole point was to hook up the controller with the Tello/TT, but it was not as simple as I thought. Now that is behind us we can again focus on our original goal: control the RoboMaster TT with the GameSir T1d through a Python script running on a computer.\n\n\n\n\nGameSir T1d controlling the Tello (simulation)"
  },
  {
    "objectID": "posts/tello-controller-navigation-part-1/index.html#simulation-first",
    "href": "posts/tello-controller-navigation-part-1/index.html#simulation-first",
    "title": "Tello controller navigation - Part 1",
    "section": "Simulation first",
    "text": "Simulation first\nI am traveling right now, and didn‚Äôt bring my drone, but I did bring the controller so I could at least get some work done. What can we do without the Tello drone? We ask Claude.ai to whip up a simple simulator!\nI am interested in the using simulators anyway. The software/model development cycle with an actual drone in the loop is a bit of a hassle. Especially later on with a larger drone. So, in the style of this blog, we start very simple with the most basic of simulators."
  },
  {
    "objectID": "posts/tello-controller-navigation-part-1/index.html#the-end-result",
    "href": "posts/tello-controller-navigation-part-1/index.html#the-end-result",
    "title": "Tello controller navigation - Part 1",
    "section": "The end result",
    "text": "The end result\nBefore we dive into some of the code, let‚Äôs see how it looks like:\n\n\n\nTello simulation\n\n\nAnd you can see it in action here: Gamesir T1d controller with Tello drone simulation demo\nIt all looks a bit lame, but it has already been very useful!\nFirst of all there was a nasty bug in the controller package that periodically told the drone to land (it pressed the A key out of nowhere every now and then).\nSecondly, I needed to think about the architecture of the code, which doesn‚Äôt really change when we connect the actual drone.\nThe code is here: tello_controller_sim.py."
  },
  {
    "objectID": "posts/tello-controller-navigation-part-1/index.html#how-does-it-work",
    "href": "posts/tello-controller-navigation-part-1/index.html#how-does-it-work",
    "title": "Tello controller navigation - Part 1",
    "section": "How does it work?",
    "text": "How does it work?\nThe application simulates a Tello drone‚Äôs physics and behavior, providing a visual interface that shows:\n\nTop-down and side views of the drone\nTelemetry data (position, rotation, battery)\nController state visualization\nFlight path trail\n\nThe simulation includes realistic features like:\n\nGradual takeoff and landing sequences\nBattery consumption\nPhysical constraints (can‚Äôt go below ground)\nInput filtering for smoother control\n\nThe code consists of three classes:\n\nTelloSimulator - Simulates the physics and state of a virtual drone\nFlightController - Handles controller input and sends commands to the drone\nDroneSimulatorApp - Main app that integrates everything with visualization\n\nLet‚Äôs have a look at each class in detail:\n\nTelloSimulator\nThis class creates a virtual model of a Tello drone with (more or less) realistic physics. This will be swapped out with the actual drone through the djitellopy library in my next post.\n\nKey Properties:\n\nposition - 3D position vector [x, y, z] in meters\nrotation - Yaw rotation in degrees (0-360¬∞)\nvelocity - 4D vector [left/right, forward/back, up/down, yaw]\nis_connected, is_flying, battery - Drone state tracking\nis_taking_off, is_landing - Transitional states\n\n\n\nKey Methods:\n\nupdate(dt) - Updates position and state based on time delta\ntakeoff() - Initiates gradual ascent to target height\nland() - Initiates gradual descent to ground\nemergency() - Immediately stops motors (safety feature)\nsend_rc_control() - Accepts control values and updates velocity\n\nThe update() method handles all physics calculations:\n\nDifferent behavior during takeoff/landing phases\nVelocity-based position updates with proper trigonometry for directional movement\nBattery drain simulation\nPrevents clipping through the ground\n\n\n\n\nFlightController\nThis class processes raw controller inputs and translates them into drone commands.\n\nKey Features:\n\nInput smoothing with filter_strength (0.8 = heavy smoothing)\ndeadband (0.03) to ignore tiny accidental joystick movements\nSpeed control with speed_multiplier (adjustable via L1/R1 buttons)\nFixed rate command sending (20Hz)\nButton edge detection (reacts to press, not hold)\nMapping follows ‚ÄúEuropean style‚Äù (right stick for primary movement)\n\n\n\nMethods:\n\nprocess_input() - Processes controller inputs with filtering\nprocess_buttons() - Handles button presses with edge detection\n\nThe control flow works like this:\n\nRead raw joystick values\nApply deadband (zero out very small inputs)\nApply smoothing filter\nConvert to integer values (-100 to 100)\nSend commands to drone at fixed intervals\n\n\n\n\nDroneSimulatorApp\nThe main application class that brings everything together. It is more or less a pygame application that takes care of the control loop and visualization. The details are not too interesting."
  },
  {
    "objectID": "posts/tello-controller-navigation-part-1/index.html#whats-next",
    "href": "posts/tello-controller-navigation-part-1/index.html#whats-next",
    "title": "Tello controller navigation - Part 1",
    "section": "What‚Äôs next?",
    "text": "What‚Äôs next?\nIn Part 2 of the Tello controller navigation I will replace the simulator with my RoboMaster TT. Read on‚Ä¶\n\n\n\n\n\n\n\nSeries: Code, Fly & AI\n\n\n\n‚Üê Previous: GameSir T1d controller & pygame\n\n\nNext: Tello controller navigation - Part 2 ‚Üí"
  },
  {
    "objectID": "posts/i-am-a-pilot/index.html",
    "href": "posts/i-am-a-pilot/index.html",
    "title": "Interlude: I‚Äôm a Pilot!",
    "section": "",
    "text": "Mark Pors\n\n2025-04-25"
  },
  {
    "objectID": "posts/i-am-a-pilot/index.html#fulfilling-a-dream",
    "href": "posts/i-am-a-pilot/index.html#fulfilling-a-dream",
    "title": "Interlude: I‚Äôm a Pilot!",
    "section": "Fulfilling a dream",
    "text": "Fulfilling a dream\nAs a kid I always wanted to be a pilot (not true, it just makes for a more interesting story), and now I just passed the EASA A1/A3 pilot exam!\nYeah, I know, this is a certification for UAV‚Äôs. UAV stands for Unmanned Aerial Vehicle, where you as a pilot stay on the ground. We can‚Äôt have it all.\nI will need this certification to fly drones outdoors, even the tiny Tello requires the ground-bound pilot to have the authorization to fly drones."
  },
  {
    "objectID": "posts/i-am-a-pilot/index.html#easa-a1a3-exam-practice-app",
    "href": "posts/i-am-a-pilot/index.html#easa-a1a3-exam-practice-app",
    "title": "Interlude: I‚Äôm a Pilot!",
    "section": "EASA A1/A3 Exam Practice App",
    "text": "EASA A1/A3 Exam Practice App\nAnyway, to help me study, I (Claude.ai) created a practice app to go over all possible questions for the exam. I thought it might be handy for others so I put it up online here: EASA A1/A3 exam test app. The source code is here.\nBeware: the source PDF with questions was used as the basis for the app, and the LLM made some mistakes. I think I have corrected most/all of them, but if you want to be sure please double check."
  },
  {
    "objectID": "posts/which-image-models-are-best-updated/index.html",
    "href": "posts/which-image-models-are-best-updated/index.html",
    "title": "Interlude: Which image models are best? UPDATED",
    "section": "",
    "text": "Mark Pors\n\n2025-05-19\nIn my attempts to create an image classifier, following the Fast.ai method as learned in lesson 2, I wanted to try other base-models. A way to help pick the most suitable model was presented in lesson 3.\nJeremy Howard created this amazing notebook that helps selecting a model for your use case: Which image models are best?. Unfortunately the notebook is two years old and is no longer working.\nThe notebook gets its data from the timm computer vision library. It is currently part of Huggingface here."
  },
  {
    "objectID": "posts/which-image-models-are-best-updated/index.html#what-is-timm",
    "href": "posts/which-image-models-are-best-updated/index.html#what-is-timm",
    "title": "Interlude: Which image models are best? UPDATED",
    "section": "What is timm?",
    "text": "What is timm?\nAs far as I understand it, timm is a collection of PyTorch compatible models focussed on computer vision, so all about images: classification, segmentation and more. Apart from a lot of models (old and new, small and large) it has other helpful stuff (helpful for people who know what this all about, not me‚Ä¶, yet! :)).\nFrom the Huggingface docs:\n\ntimm is a library containing SOTA computer vision models, layers, utilities, optimizers, schedulers, data-loaders, augmentations, and training/evaluation scripts.\nIt comes packaged with &gt;700 pretrained models, and is designed to be flexible and easy to use.\n\nIt started as Ross Wightman‚Äôs solo project, but now it‚Äôs part of the Hugging Face ecosystem.\nFor me timm is relevant, because it allows me to try different models that can improve the is there a lamp classifier, and in the future, for more advanced stuff we will do."
  },
  {
    "objectID": "posts/which-image-models-are-best-updated/index.html#great-visualization-outdated",
    "href": "posts/which-image-models-are-best-updated/index.html#great-visualization-outdated",
    "title": "Interlude: Which image models are best? UPDATED",
    "section": "Great visualization (outdated)",
    "text": "Great visualization (outdated)\nJeremy Howard used the timm data to make is easier to select models to try out. He created a notebook that includes graphs like this:\n\n\n\nScreenshot: Inference top1 accuracy / speed scatter\n\n\nAs said, unfortunately it no longer works if you copy it to your own account (the data sources have moved to Huggingface), and worst: it doesn‚Äôt contain information from more recent models.\nThe timm leaderboard is meant to replace it, but it is not as clear IMO. I want the same happy colorful interactive bubbles!\nShould be easy to fix, so let‚Äôs get to it!\n\n\n\n\n\n\nWhy not use an LLM?\n\n\n\nYeah, great question! We could also dump all data into an LLM and ask it to create some useful visualizations. It has the advantage that you can chat about the data as well to make the right choice.\nSometimes, however, it feels just easier to play around in a notebook IMO. Also, in this specific case, it is not so easy to get the same results: you are basically describing to the LLM in text what we (Jeremy) did here with code."
  },
  {
    "objectID": "posts/which-image-models-are-best-updated/index.html#visualization-repaired",
    "href": "posts/which-image-models-are-best-updated/index.html#visualization-repaired",
    "title": "Interlude: Which image models are best? UPDATED",
    "section": "Visualization repaired",
    "text": "Visualization repaired\nAs said, it was mostly a matter of pointing to the new data sources and to include a couple of newer models that are SOTA (I asked ChatGPT DeepSearch which ones are). I also found the original data sources and recreated the original charts next to the new ones.\nThe notebook I used for my experiments, with both the new and the old data sources, is here: Which image models are best? (updated).\nI also created a copy on Kaggle here. Kaggle has some problems rendering the plotly charts (which I fixed in part): they don‚Äôt show up unless in edit mode‚Ä¶\nFinally I created a version with less text, and more flexibility:\n\nyou can select the benchmark file\nlimit the included model families\nchange what charts will display on x- and y-axis\nwhen new benchmarks are published, they wil be included\n\nIt can be found here: Which image models are best? (improved)."
  },
  {
    "objectID": "posts/which-image-models-are-best-updated/index.html#deploying-it-on-huggingface-with-gradio",
    "href": "posts/which-image-models-are-best-updated/index.html#deploying-it-on-huggingface-with-gradio",
    "title": "Interlude: Which image models are best? UPDATED",
    "section": "Deploying it on Huggingface with Gradio",
    "text": "Deploying it on Huggingface with Gradio\nI asked Claude to port the notebook to a Gradio app, and after some tweaks it seems to work just fine:\nImage Model Performance Analysis\n\n\n\nThe Gradio app on Huggingface\n\n\nPretty cool, no?"
  },
  {
    "objectID": "posts/which-image-models-are-best-updated/index.html#whats-next",
    "href": "posts/which-image-models-are-best-updated/index.html#whats-next",
    "title": "Interlude: Which image models are best? UPDATED",
    "section": "What‚Äôs next",
    "text": "What‚Äôs next\nThis interlude got a bit out of hand. Time to go back to our image classifier and pick a better model. Read on‚Ä¶"
  },
  {
    "objectID": "series/index.html",
    "href": "series/index.html",
    "title": "Code, Fly & AI Series",
    "section": "",
    "text": "Welcome to the Code, Fly & AI series ‚Äî a blog journey into coding autonomous drones with baby steps.\nFollow along as I:\n\nLearn to fly (and crash) drones\nHack with Python and SDKs\nUse AI to give them brains\n\nüëá Posts in this series:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEnter the Tello\n\n\nDiscover how to start building an autonomous scouting drone for hiking using the DJI RoboMaster TT (Tello Talent). Follow a beginner‚Äôs journey into drones, coding, and computer vision.\n\n\n\n\n\nApr 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCrash the Tello (with and without code)\n\n\nLearn how to fly and program the DJI Tello (RoboMaster TT) drone, avoid common beginner crashes, and get started with Python coding using the Tello SDK.\n\n\n\n\n\nApr 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nKey navigation & Video snapshots\n\n\nHands-on guide to controlling a Tello drone with Python and pygame: learn responsive keyboard navigation, smooth velocity-based movement, live video streaming, and how to capture snapshots from the drone feed.\n\n\n\n\n\nApr 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGameSir T1d controller & pygame\n\n\nLearn how to connect and use the GameSir T1d controller with your computer for Tello drone control using Python, BLE hacking, and a pygame-compatible wrapper. Step-by-step guide with working code.\n\n\n\n\n\nMay 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTello controller navigation - Part 1\n\n\nSimulate and control a Tello drone using Python, pygame, and a GameSir T1d controller‚Äîlearn practical code architecture, input smoothing, and debugging techniques for drone development.\n\n\n\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTello controller navigation - Part 2\n\n\nLearn how to connect a GameSir T1d controller to a real Tello drone using Python, integrate live video streaming with Pygame, and set up a foundation for future autonomous drone experiments.\n\n\n\n\n\nMay 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFly a drone with: Image classification\n\n\nLearn how to train a drone to detect obstacles using image classification with fast.ai. This post covers practical steps for building, cleaning, and improving an image classifier, including tips on data collection, augmentation, and model selection.\n\n\n\n\n\nMay 15, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/tello-controller-navigation-part-2/index.html",
    "href": "posts/tello-controller-navigation-part-2/index.html",
    "title": "Tello controller navigation - Part 2",
    "section": "",
    "text": "Mark Pors\n\n2025-05-11"
  },
  {
    "objectID": "posts/tello-controller-navigation-part-2/index.html#time-to-fly",
    "href": "posts/tello-controller-navigation-part-2/index.html#time-to-fly",
    "title": "Tello controller navigation - Part 2",
    "section": "Time to fly",
    "text": "Time to fly\nThis blog is supposed to show my progress with machine learning, but we haven‚Äôt touched that yet at all! That will still be the case in this post, but we are close to wrapping up the drone control.\nIt is time to combine our previous efforts and connect the drone interface (djitellopy) with the controller interface (gamesir-t1d).\n\n\n\n\nGameSir T1d controlling the Tello / RoboMaster TT"
  },
  {
    "objectID": "posts/tello-controller-navigation-part-2/index.html#from-simulator-to-tello",
    "href": "posts/tello-controller-navigation-part-2/index.html#from-simulator-to-tello",
    "title": "Tello controller navigation - Part 2",
    "section": "From simulator to Tello",
    "text": "From simulator to Tello\nLet‚Äôs start with the most bare approach and replace the TelloSimulator with a new class TelloDrone, with a similar interface, but driving the Tello via djitellopy.\nThe FlightController stays as it was, so we have all the deadbanding and smoothing goodies there.\nFinally, we need to glue these classes together in a similar way we did with the simulator, but without the ‚Äúfancy‚Äù visualization.\nThe code looks like this at this stage: tt-fly WIP.\nAnd yes, it flies!"
  },
  {
    "objectID": "posts/tello-controller-navigation-part-2/index.html#add-video-streaming",
    "href": "posts/tello-controller-navigation-part-2/index.html#add-video-streaming",
    "title": "Tello controller navigation - Part 2",
    "section": "Add video streaming",
    "text": "Add video streaming\nWe accomplished what we were set out to do! At least, I was :) Now that we have some momentum, let‚Äôs add the video stream.\nThe djitellopy library makes it straightforward. In the TelloDrone class we initialize video streaming:\n# Make sure stream is off before turning it on\ntry:\n    self.tello.streamoff()\n    print(\"Stopped any existing video stream\")\nexcept Exception as e:\n    print(f\"Note: {e}\")\n\n# Initialize frame reading with low quality settings\nself.tello.set_video_resolution(self.tello.RESOLUTION_480P)\nself.tello.set_video_fps(self.tello.FPS_30)\nself.tello.set_video_bitrate(self.tello.BITRATE_4MBPS)\n\n# Now turn on the stream\nself.tello.streamon()\n\n# Get the frame reader\nself.frame_read = self.tello.get_frame_read()\n\nprint(\"Video stream initialized successfully\")\nAlso, we add a method to expose the frame reader to the main app:\ndef get_video_frame(self):\n    \"\"\"Get the current video frame from the drone.\"\"\"\n    if self.frame_read is None:\n        return None\n\n    try:\n        frame = self.frame_read.frame\n        if frame is None or frame.size == 0:\n            print(\"Warning: Received empty frame\")\n            return None\n        return frame\n    except Exception as e:\n        print(f\"Error getting video frame: {e}\")\n        return None\nFinally, we read frames from the main app and display it in the pygame window:\n# Display video frame if available\nframe = self.drone.get_video_frame()\nif frame is not None:\n    # Track FPS\n    self.frame_count += 1\n    now = time.time()\n    if now - self.last_frame_time &gt;= 1.0:  # Calculate FPS every second\n        fps = self.frame_count / (now - self.last_frame_time)\n        self.fps_stats.append(fps)\n        if len(self.fps_stats) &gt; 10:\n            self.fps_stats.pop(0)\n        self.frame_count = 0\n        self.last_frame_time = now\n\n    # Convert numpy array to pygame surface\n    try:\n        # Ensure frame has the right format for pygame\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n        # Create a PyGame surface\n        h, w = frame.shape[:2]\n        pygame_frame = pygame.Surface((w, h))\n        pygame.surfarray.blit_array(pygame_frame, np.swapaxes(frame, 0, 1))\n\n        # Scale if needed\n        if pygame_frame.get_size() != (\n            self.video_rect.width,\n            self.video_rect.height,\n        ):\n            pygame_frame = pygame.transform.scale(\n                pygame_frame, (self.video_rect.width, self.video_rect.height)\n            )\n\n        self.screen.blit(pygame_frame, self.video_rect)\n    except Exception as e:\n        print(f\"Error displaying frame: {e}\")\n        # Draw a red border to indicate error\n        pygame.draw.rect(self.screen, (255, 0, 0), self.video_rect, 2)\nelse:\n    # Draw a placeholder for video\n    pygame.draw.rect(self.screen, (40, 40, 60), self.video_rect)\n    no_video = self.font.render(\"No Video Feed\", True, (150, 150, 150))\n    text_rect = no_video.get_rect(center=self.video_rect.center)\n    self.screen.blit(no_video, text_rect)\nSource for the complete code base: tt-fly"
  },
  {
    "objectID": "posts/tello-controller-navigation-part-2/index.html#test-flight",
    "href": "posts/tello-controller-navigation-part-2/index.html#test-flight",
    "title": "Tello controller navigation - Part 2",
    "section": "Test flight!",
    "text": "Test flight!\nTime to try it out with an audience:\n\nI think I worked on this more than enough, and I have a plenty of experience with remote controlling a drone now. Time to move on‚Ä¶"
  },
  {
    "objectID": "posts/tello-controller-navigation-part-2/index.html#whats-next",
    "href": "posts/tello-controller-navigation-part-2/index.html#whats-next",
    "title": "Tello controller navigation - Part 2",
    "section": "What‚Äôs next?",
    "text": "What‚Äôs next?\nIt is about time to introduce some machine learning. If the goal is to work towards an autonomous drone, at some point I have to get my feet wet. I have plenty of ideas on what to experiment with. Read on for the first one‚Ä¶\n\n\n\n\n\n\n\nSeries: Code, Fly & AI\n\n\n\n‚Üê Previous: Tello controller navigation - Part 1\n\n\nNext: Fly a drone with: Image classification ‚Üí"
  },
  {
    "objectID": "posts/getting-reproducible-results/index.html",
    "href": "posts/getting-reproducible-results/index.html",
    "title": "Getting reproducible training results with Fast.ai + PyTorch",
    "section": "",
    "text": "Mark Pors\n\n2025-05-25\nWhile training an image classifier, I became a bit annoyed by the fact that subsequent runs have different results, even though I didn‚Äôt change anything.\nHow can we conduct experiments if we don‚Äôt know whether the one parameter I modified caused the change, or if something else was responsible, under the hood?\nJump to Conclusion"
  },
  {
    "objectID": "posts/getting-reproducible-results/index.html#what-have-i-tried",
    "href": "posts/getting-reproducible-results/index.html#what-have-i-tried",
    "title": "Getting reproducible training results with Fast.ai + PyTorch",
    "section": "What have I tried?",
    "text": "What have I tried?\nWhen I did my experiments, I used this at the start of my notebooks:\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_everything(42)\nAnd in the DataBlock, I provided a seed for the splitter like:\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\nI picked up these snippets in the Fast.ai course, but I don‚Äôt really know what each of these statements does in detail. I understand the general idea: ensure that when a random number is generated, it starts from the same seed, so that the random number is consistent each time. Like:\nimport random\n\nprint(\"Without seed:\", random.randint(1, 100), random.randint(1, 100), random.randint(1, 100))\nrandom.seed(42)\nprint(\"With seed 42:\", random.randint(1, 100), random.randint(1, 100), random.randint(1, 100))\nrandom.seed(42)\nprint(\"With seed 42 again:\", random.randint(1, 100), random.randint(1, 100), random.randint(1, 100))\n\nWithout seed: 15 60 38\nWith seed 42: 81 14 3\nWith seed 42 again: 81 14 3\n\nUnfortunately, it is not that simple, proven by the fact that it didn‚Äôt work for me. Here is an experiment that shows that loss numbers are not reproducible despite seeding ‚Äúeverything‚Äù: Getting reproducible results with Fast.ai / PyTorch - Attempt #1.\nPerhaps a good time to read the documentation."
  },
  {
    "objectID": "posts/getting-reproducible-results/index.html#rtfm",
    "href": "posts/getting-reproducible-results/index.html#rtfm",
    "title": "Getting reproducible training results with Fast.ai + PyTorch",
    "section": "RTFM",
    "text": "RTFM\n\nFast.ai docs\nThere is currently no section to be found in the Fast.ai docs covering reproducibility, but there was in version one (deprecated): Getting reproducible results:\n\nIn some situations you may want to remove randomness for your tests. To get identical reproducible results set, you‚Äôll need to set num_workers=1 (or 0) in your DataLoader/DataBunch, and depending on whether you are using torch‚Äôs random functions, or python‚Äôs (numpy) or both:\n\nseed = 42\n\n# python RNG\nimport random\nrandom.seed(seed)\n\n# pytorch RNGs\nimport torch\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\nif torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n\n# numpy RNG\nimport numpy as np\nnp.random.seed(seed)\nThe Python and numpy parts speak for themselves. I will dive into the PyTorch statements in a moment.\nIf we compare this code with what I did in my first attempt, the only thing that is new is num_workers=1 for dataloaders.\nI tried that, and nope: still not reproducible.\nThe current docs don‚Äôt have a section like the one above, but there is a function set_seed (only available in Fast.ai). Let‚Äôs have a look at the source code:\ndef set_seed(s, reproducible=False):\n    \"Set random seed for `random`, `torch`, and `numpy` (where available)\"\n    try: torch.manual_seed(s)\n    except NameError: pass\n    try: torch.cuda.manual_seed_all(s)\n    except NameError: pass\n    try: np.random.seed(s%(2**32-1))\n    except NameError: pass\n    random.seed(s)\n    if reproducible:\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\nSo, mostly the same stuff, except: torch.backends.cudnn.benchmark = False. Which leads us to the PyTorch docs‚Ä¶\n\n\nPyTorch docs\nLuckily, there is a note about reproducibility in the PyTorch docs: Reproducibility.\nThere is a warning at the top of the document that I will repeat here.\n\n\n\n\n\n\nWarning\n\n\n\nDeterministic operations are often slower than nondeterministic operations, so single-run performance may decrease for your model. However, determinism may save time in development by facilitating experimentation, debugging, and regression testing.\n\n\nSo it might be worth trying to make everything deterministic during our early experiments (with a smaller dataset and smaller base model), and when we are confident we are on the right track, we switch to nondeterministic.\nSounds great! But first we have to get the deterministic part working in the first place‚Ä¶\nThe PyTorch doc consists of three sections. Let‚Äôs go through them one by one.\n\n1. Controlling sources of randomness\nApart from reiterating the assignment of a fixed seed for Python, NumPy, and PyTorch, it also discusses the cudnn.benchmark feature we saw earlier. cudnn is a library built on top of CUDA that accelerates the training of neural networks. Apparently, it starts by trying out a couple of approaches (that‚Äôs the benchmarking), and picks the winner for the rest of the training. There is randomness involved in this benchmarking, so setting it to False should make the process deterministic.\nWhat happened when I tried this?\n-&gt; Still no reproducible results!\n\n\n2. Avoiding nondeterministic algorithms\nNow it gets interesting! PyTorch provides a method that might solve our problems: torch.use_deterministic_algorithms(True).\nThis statement instructs all other Torch code to use the deterministic variant of its algorithms, and if this is not possible, to throw an exception.\nAnd that is exactly what I got, an exception:\n\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-12-25c053374517&gt; in &lt;cell line: 0&gt;()\n      1 learn = vision_learner(dls, resnet18, metrics=error_rate)\n----&gt; 2 learn.fine_tune(3)\n\n21 frames\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n    123 \n    124     def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 125         return F.linear(input, self.weight, self.bias)\n    126 \n    127     def extra_repr(self) -&gt; str:\n\nRuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA &gt;= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility\n\nIt didn‚Äôt happen in some exotic part of the library, but at the most elemental level that even I understand: return F.linear(input, self.weight, self.bias).\nThe error message is super specific and helpful:\nDeterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA &gt;= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility\nLet‚Äôs see what version of CUDA we are using on Colab:\nprint(torch.version.cuda)\n\n12.4\n\nFollowing the advice and setting this at the top of the notebook\nos.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'   # or ':16:8'\ngot me over this hurdle!\n\n\n\n\n\n\nTip\n\n\n\nJust a ‚ÄúRestart session and run all‚Äù is not enough after introducing this environment var. You have to actually disconnect from the colab runtime, connect again, and run all.\n\n\nWe got a step further, but another error pops up, now in the backward pass:\n\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-11-af2d886d9870&gt; in &lt;cell line: 0&gt;()\n      1 print(os.environ['CUBLAS_WORKSPACE_CONFIG'])\n      2 learn = vision_learner(dls, resnet18, metrics=error_rate)\n----&gt; 3 learn.fine_tune(3)\n\n22 frames\n/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py in _engine_run_backward(t_outputs, *args, **kwargs)\n    821         unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n    822     try:\n--&gt; 823         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    824             t_outputs, *args, **kwargs\n    825         )  # Calls into the C++ engine to run the backward pass\n\nRuntimeError: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True)'. You can turn off determinism just for this operation, or you can use the 'warn_only=True' option, if that's acceptable for your application. You can also file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation.\n\nOh, oh, bad news. We have no other choice now to set the warn_only flag to True on the use_deterministic_algorithms call. This indeed allows the fine-tuning run to finish, but still not reproducible :(\n\n\n3. DataLoader\nFinally, the PyTorch doc about reproducibility mentions the possibility of seeding dataloader workers identically. As we previously used only one worker, I assume this won‚Äôt have any impact on our case. And guess what: it doesn‚Äôt 1!\nHere is the notebook where I did all the experiments as recommended in the docs: Getting reproducible results with Fast.ai / PyTorch - Attempt #2. BTW the Colab GPU was of type L4.\nSpeaking of GPUs: would we get reproducible results if we run it on a CPU? The answer is NO (I tried) 2."
  },
  {
    "objectID": "posts/getting-reproducible-results/index.html#lets-get-some-help-hello-claude",
    "href": "posts/getting-reproducible-results/index.html#lets-get-some-help-hello-claude",
    "title": "Getting reproducible training results with Fast.ai + PyTorch",
    "section": "Let‚Äôs get some help (hello Claude)",
    "text": "Let‚Äôs get some help (hello Claude)\nThe documentation didn‚Äôt really help. All the tricks to keep everything deterministic failed. It could be the backward pass, in which case we don‚Äôt have much hope of resolving it. But how do people train their models if you can‚Äôt do any experiments where just one meta-parameter changes?\nAnyway, we are not giving up! Let‚Äôs ask our friend the LLM.\nClaude speaks:\n\nWhen you create a DataLoader in PyTorch, it establishes its own internal random state that becomes decoupled from the global random seed. The DataLoader contains several stateful components that persist between training runs:\nThe DataLoader‚Äôs internal random number generator gets initialized when you first create it, and this generator maintains its own state independently of any subsequent global seed resets. Even though you call seed_everything(42) before each run, the DataLoader‚Äôs internal RNG has already advanced its state from the previous run and won‚Äôt reset back to its initial position.\nThe data shuffling mechanism also maintains state. Even with a fixed seed in your RandomSplitter, the underlying shuffling operations within the DataLoader can accumulate state changes that persist across runs, leading to subtle differences in the exact order that batches are presented to your model.\n\nAh, of course, state is the key thing here! Random generators can start nicely with the same seed again, but if the state is not the same as it was before, that has no impact!\nSo let‚Äôs apply that wisdom by setting the seeds and recreate the DataLoader before starting a new training run‚Ä¶\nVictory, we have reproducible results now! Have a look at this winning notebook: Getting reproducible results with Fast.ai / PyTorch - Attempt #3.\n\n\n\n\n\n\nWarning\n\n\n\nEven with the most conservative setup, like in this notebook, there can be small differences between runs. I noticed this when changing the GPU model from L4 to A100. This is acknowledged in the PyTorch documentation.\n\n\nThere is just one problem with it: it has become really slow."
  },
  {
    "objectID": "posts/getting-reproducible-results/index.html#hhhhh",
    "href": "posts/getting-reproducible-results/index.html#hhhhh",
    "title": "Getting reproducible training results with Fast.ai + PyTorch",
    "section": "^H^H^H^H^H",
    "text": "^H^H^H^H^H\nFor you younger kids: ^H is the backspace character in Unix-type terminals. In other words, let‚Äôs delete some of the things we have done to increase reproducibility at the cost of performance.\n\nObservations\n\nnum_workers:\nChanging from num_workers=0 to num_workers=12 3 maintains reproducibility, but only if the workers are seeded as follows (from the PyTorch docs):\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    numpy.random.seed(worker_seed)\n    random.seed(worker_seed)\n\ng = torch.Generator()\ng.manual_seed(42)\n\nDataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    num_workers=num_workers,\n    worker_init_fn=seed_worker,\n    generator=g,\n)\nThis also maintains reproducibility across sessions.\nI‚Äôm very happy this works as it speeds up things significantly!\n\n\nuse_deterministic_algorithms:\nRemoving torch.use_deterministic_algorithms(True) also maintains reproducibility, both in the notebook and across sessions 4.\n\n\nset_seed:\nSetting the second parameter of set_seed, reproducible to False (which is the default) basically sets torch.backends.cudnn.benchmark = False in our case. This also maintains reproducibility, both in the notebook and across sessions.\n\n\ncudnn.deterministic:\nSetting torch.backends.cudnn.deterministic = False breaks reproducibility, so it is essential when running meaningful experiments. I hardly saw any performance degradation by setting this to True, but that might be different for other use cases.\nKey findings: as long as you seed every DataLoader worker and keep torch.backends.cudnn.deterministic = True, you can crank num_workers back up, drop torch.use_deterministic_algorithms(True), and rely on the default set_seed(..., reproducible=False): reproducibility still holds across notebook sessions while you recover full training speed.\nThe notebook that has applied the above, and is fast and reproducible, is here: Getting reproducible results with Fast.ai / PyTorch - Attempt #4"
  },
  {
    "objectID": "posts/getting-reproducible-results/index.html#conclusion",
    "href": "posts/getting-reproducible-results/index.html#conclusion",
    "title": "Getting reproducible training results with Fast.ai + PyTorch",
    "section": "Conclusion",
    "text": "Conclusion\n\n\n\n\n\n\nNote\n\n\n\nThese conclusions are probably not generic for all training pipelines and base models, but they probably are for convolutional networks like Resnet18. It might be different for transformer-based models, who knows. At least we know what knobs we can turn to get reproducible results.\n\n\nReproducible training comes down to three levers:\n\nSeed every RNG ‚Äì Python random, NumPy, and each DataLoader worker (worker_init_fn + torch.Generator).\nForce deterministic kernels torch.backends.cudnn.deterministic = True (leave benchmarking off while you experiment).\nStart from the same state: rebuild the DataLoader before each fresh run so its internal sampler is reset.\n\nIf you are using Fast.ai, use the Fast.ai notebook as a reference for reproducible training.\nIn the end it comes down to:\n# First run\nseed_everything(42)\ng = torch.Generator()\ng.manual_seed(42)\ntorch.backends.cudnn.deterministic = True\ndls = create_dataloaders(g)\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(3)\n\n# Next run(s)\nseed_everything(42)\ng = torch.Generator()\ng.manual_seed(42)\ndls = create_dataloaders(g)\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(3)\nIf you prefer to just use PyTorch, use the PyTorch notebook (Claude generated this based on the Fast.ai version).\nOnce you move from experimentation to full-scale training, you can flip torch.backends.cudnn.deterministic = False to potentially regain speed. Just remember that it will sacrifice strict repeatability."
  },
  {
    "objectID": "posts/getting-reproducible-results/index.html#whats-next",
    "href": "posts/getting-reproducible-results/index.html#whats-next",
    "title": "Getting reproducible training results with Fast.ai + PyTorch",
    "section": "What‚Äôs next",
    "text": "What‚Äôs next\nAnother interlude that got a bit out of hand. Time to go back to our image classifier and see if we can improve it. Read on‚Ä¶"
  },
  {
    "objectID": "posts/getting-reproducible-results/index.html#footnotes",
    "href": "posts/getting-reproducible-results/index.html#footnotes",
    "title": "Getting reproducible training results with Fast.ai + PyTorch",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs I found out later, it certainly does matter when using more than one worker, which is what you always want in order to have acceptable performance.‚Ü©Ô∏é\nWith what I know now, this is because I didn‚Äôt recreate the dataloaders between runs. If we did that and used a CPU, the results would be reproducible.‚Ü©Ô∏é\nThis is the default in my case; it is calculated as min(16, os.cpu_count()).‚Ü©Ô∏é\nThis stays repeatable unless your model calls a layer that can‚Äôt guarantee identical results on the GPU; in that case, the numbers may shift a bit between runs, or PyTorch will throw a warning.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/crash-the-tello/index.html",
    "href": "posts/crash-the-tello/index.html",
    "title": "Crash the Tello (with and without code)",
    "section": "",
    "text": "Mark Pors\n\n2025-04-24"
  },
  {
    "objectID": "posts/crash-the-tello/index.html#unboxed",
    "href": "posts/crash-the-tello/index.html#unboxed",
    "title": "Crash the Tello (with and without code)",
    "section": "Unboxed",
    "text": "Unboxed\nNow that the drone is out of the box, we‚Äôre gonna take it for a spin (and yes it crashed). First with the phone app, then with code.\n\n\n\nCrashed RoboMaster TT (generated by ChatGPT)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThese blog posts are intended to be read in order. If you want to follow along on my journey, start here: Enter the Tello. Or ignore this and keep on reading below."
  },
  {
    "objectID": "posts/crash-the-tello/index.html#first-tt-flight",
    "href": "posts/crash-the-tello/index.html#first-tt-flight",
    "title": "Crash the Tello (with and without code)",
    "section": "First TT flight",
    "text": "First TT flight\nTo fly a drone for the first time it is good to have a basic understanding how these quadcopters work, and what the basic movements are:\n\n\n\n\n\n\n\nMovement\nDescription\n\n\n\n\nthrottle\nincreases or decreases the height of the drone by adjusting all propellers equally\n\n\nyaw\nturns the drone clockwise or counterclockwise by varying propeller speeds\n\n\npitch\nmoves the drone forward or backward by changing speeds between front and back propellers\n\n\nroll\nmoves the drone left or right by varying left and right propeller speeds\n\n\n\n This video explains very clearly how it works: \nThe Tello doesn‚Äôt come with a controller, we fly it through the Tello app. I used the Tello app for iPhone. This app hasn‚Äôt been updated for a long time and unfortunately it is useless as it crashes all the time. When the app crashes the drone keeps on hovering in the air, so this first crash is just a software crash.\n\n\n\n\n\n\nTip\n\n\n\nIf you want to fly the TT or Tello manually I recommend you get the TelloFPV. It‚Äôs not free, but doesn‚Äôt crash either.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo really enjoy flying the TT manually you can get a controller that is customized for the Tello: the GameSir T1d. I got one, and it works straight out of the box ‚Äî even with the TelloFPV app.\n\n\n\nBeware of the ceiling effect\nWhile flying the drone around a bit in my office, I went up a bit too high and the drone crashed when it came near the ceiling. This phenomenon even has a name and is called the ‚Äúceiling effect.‚Äù Basically, the drone gets sucked up to the ceiling because there is not enough air to push down through the propellers.\nAlright, now for the real fun ‚Äî controlling the drone with code."
  },
  {
    "objectID": "posts/crash-the-tello/index.html#the-tello-python-sdk",
    "href": "posts/crash-the-tello/index.html#the-tello-python-sdk",
    "title": "Crash the Tello (with and without code)",
    "section": "The Tello Python SDK",
    "text": "The Tello Python SDK\nDJI provides an SDK for the Tello drone that can be used with Python. The SDK can be used to control the drone through its API. The API is fairly basic but supports:\n\nFlight control with takeoff, landing and movement commands\nFlight status information (battery level, height, acceleration, speed)\nStream camera feed\n\nThe Python library we will be using for this SDK is DJITelloPy.\nA couple of useful resources:\n\nSDK 3.0 user guide (includes the API spec).\nRoboMaster Developer Guide (also for RoboMasters on wheels).\nRoboMaster SDK (as provided by DJI).\nDJITelloPy API Reference.\n\nTo give it a try, let‚Äôs first install djitellopy:\n\nimport sys\n!{sys.executable} -m pip install djitellopy\n\n\nTake-off, fly, land\nLet‚Äôs start with a simple script to take-off, fly, and land the drone.\nTo make this work the computer running the script needs to be connected to the Wifi Access Point the Tello provides. The SSID is of the form TELLO-XXXXXX.\n\nfrom djitellopy import tello\nfrom time import sleep\n\nt = tello.Tello()\nt.connect()\n\nprint(f\"Bat: {t.get_battery()}\")\nprint(f\"Temp: {t.get_temperature()}\")\n\nt.takeoff()\n\n\"\"\"Send RC control via four channels. Command is sent every self.TIME_BTW_RC_CONTROL_COMMANDS seconds.\nArguments:\n    left_right_velocity: -100~100 (left/right)\n    forward_backward_velocity: -100~100 (forward/backward)\n    up_down_velocity: -100~100 (up/down)\n    yaw_velocity: -100~100 (yaw)\n\"\"\"\nt.send_rc_control(0, 50, 0, 0)\nsleep(2)\nt.send_rc_control(30, 0, 0, 0)\nsleep(2)\nt.send_rc_control(0, 0, 0, 0) # don't forget this!\n\nt.land()\nt.end()\n\n\n[INFO] tello.py - 129 - Tello instance was initialized. Host: '192.168.1.85'. Port: '8889'.\n[INFO] tello.py - 438 - Send command: 'command'\n[INFO] tello.py - 462 - Response command: 'ok'\nBat: 100\nTemp: 45.0\n[INFO] tello.py - 438 - Send command: 'takeoff'\n[INFO] tello.py - 462 - Response takeoff: 'ok'\n[INFO] tello.py - 471 - Send command (no response expected): 'rc 0 50 0 0'\n[INFO] tello.py - 471 - Send command (no response expected): 'rc 30 0 0 0'\n[INFO] tello.py - 471 - Send command (no response expected): 'rc 0 0 0 0'\n[INFO] tello.py - 438 - Send command: 'land'\n[INFO] tello.py - 462 - Response land: 'ok'\n\nIt is pretty straightforward. My first attempt made the drone crash though: not having the t.send_rc_control(0, 0, 0, 0) command there tells the drone to move sideways (to the right) while landing, with the expected result.\n\n\n\n\n\n\nNote\n\n\n\nThe code above was executed in my code editor and the output you see there is the actual output. Similar to a frozen Jupyter notebook. This is possible because this blog is powered by Quarto. If you haven‚Äôt tried Quarto, maybe check it out.\n\n\n\n\nUsing router mode\nIt is quite annoying that the computer you are working on is not connected to the Internet while testing the code (it is connected to the wifi of the drone). Every five seconds I MUST check with Claude/ChatGPT if I am doing things right, no?\nFor the original Tello there is nothing we can do about that, but the RoboMaster TT has two wifi connection modes:\n\nDirect connection mode (aka AP mode): this is what we did so far, the Tello provides an access point and the computer connects to that.\nRouter mode (aka STA mode): both the TT and the computer connect to the same wifi router, so we are still online if that router is our home router.\n\nThe TT has an expansion kit that contains a small microprocessor that provides wifi and Bluetooth connectivity: ESP32-D2WD. We will have a look later to see what we can do with it, for now we just use the wifi in router mode. There is a tiny switch on the expansion unit that we can toggle between the two modes. It does involve a couple of other steps though, which are outlined here: Connection examples. It didn‚Äôt work for me (there is no QR code I can get from the Tello app), so I will show you what I did:\n\n\n\n\n\n\nTip\n\n\n\nBefore continuing, the controller needs to be activated from the Tello app! This happens after the firmware update of this controller. This cost me a couple of days of my life, so next time I‚Äôll RTFM.\n\n\n\nStep 1.\nSet the switch to AP mode first (down).\n\n\nStep 2.\nConnect the PC to the RMTT-xxx network. We are now in direct connection mode, provided by the Wifi on the expansion kit.\n\n\nStep 4:\nRun this script:\n\nimport socket\nimport time\nimport getpass\nimport os\n\n# Environment variables for Wi-Fi credentials just to make it run in a notebook\nos.environ[\"WIFI_SSID\"] = \"MyNetwork\"\nos.environ[\"WIFI_PASSWORD\"] = \"SuperSecret\"\n\nsock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\nsock.settimeout(5)\n\n\ndef get_wifi_credentials():\n    try:\n        ssid = input(\"Wi-Fi SSID: \")\n        password = getpass.getpass(\"Wi-Fi Password: \")\n    except (EOFError, getpass.GetPassWarning, Exception):\n        # Fall back to environment variables\n        print(\"Interactive input not available. Falling back to environment variables.\")\n        ssid = os.getenv(\"WIFI_SSID\")\n        password = os.getenv(\"WIFI_PASSWORD\")\n\n        if not ssid or not password:\n            raise RuntimeError(\n                \"Missing WIFI_SSID or WIFI_PASSWORD environment variable.\"\n            )\n\n    return ssid, password\n\n\n# Step 1: Enter SDK mode\nsock.sendto(b\"command\", (\"192.168.10.1\", 8889))\ntry:\n    response, _ = sock.recvfrom(1024)\n    print(\"Response 1:\", response)\nexcept Exception as e:\n    print(\"No response to command:\", e)\n\ntime.sleep(1)\n\n# Step 2: Send ap command\nssid, password = get_wifi_credentials()\nsock.sendto(b\"ap %s %s\" % (ssid.encode(), password.encode()), (\"192.168.10.1\", 8889))\ntry:\n    response, _ = sock.recvfrom(1024)\n    print(\"Response 2:\", response)\nexcept Exception as e:\n    print(\"No response to ap:\", e)\n\n\n\nStep 5:\nToggle the switch to STA mode (up).\n\n\nStep 6:\nConnect the PC to the wifi network you provided in the script above.\n\n\nStep 7:\nFind the IP address that was assigned to the TT (e.g.¬†in your home router admin settings).\n\n\nStep 8:\nUse this IP address every time you connect to the Tello in your code:\n\nt = tello.Tello(host=\"192.168.1.85\") # the IP address from step 7\n\nt.connect()\n\nFrom here on we can connect to the TT and also be connected to the Internet. Yay!\n\n\n\nThanks!\nI want to thank Murtaza Hassan for getting me started through this video: Drone Programming With Python Course."
  },
  {
    "objectID": "posts/crash-the-tello/index.html#whats-next",
    "href": "posts/crash-the-tello/index.html#whats-next",
    "title": "Crash the Tello (with and without code)",
    "section": "What‚Äôs Next",
    "text": "What‚Äôs Next\nIn the next episode I‚Äôm going to implement keyboard control and stream video frames to the PC and save them on disk on demand (‚Äúinspired‚Äù by Murtaza 1).\n\n\n\n\n\n\n\nSeries: Code, Fly & AI\n\n\n\n‚Üê Previous: Enter the Tello\n\n\nNext: Key navigation & Video snapshots ‚Üí"
  },
  {
    "objectID": "posts/crash-the-tello/index.html#footnotes",
    "href": "posts/crash-the-tello/index.html#footnotes",
    "title": "Crash the Tello (with and without code)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBorrowed is the real word here. No: stolen!‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/fly-drone-with-image-classification/index.html",
    "href": "posts/fly-drone-with-image-classification/index.html",
    "title": "Fly a drone with: Image classification",
    "section": "",
    "text": "Mark Pors\n\n2025-05-15"
  },
  {
    "objectID": "posts/fly-drone-with-image-classification/index.html#the-path-to-autonomous-flying",
    "href": "posts/fly-drone-with-image-classification/index.html#the-path-to-autonomous-flying",
    "title": "Fly a drone with: Image classification",
    "section": "The path to autonomous flying",
    "text": "The path to autonomous flying\nNow that we have the Tello / RoboMaster TT under control with a mobile app, a keyboard, and a game controller we can make the first steps to remove control all together.\nI‚Äôm sure there have been written many books about this subject and there is loads of scientific research:\n\n\n\nGoogle scholar search for ‚Äúautonomous drones‚Äù\n\n\nOver 17k results since 2024; OMG! Good thing that LLM‚Äôs have read all that :)\nI‚Äôm not going to do anything with it for now, let‚Äôs just try out a couple of naive ideas and see were we end up."
  },
  {
    "objectID": "posts/fly-drone-with-image-classification/index.html#naive-approach-1-image-classification",
    "href": "posts/fly-drone-with-image-classification/index.html#naive-approach-1-image-classification",
    "title": "Fly a drone with: Image classification",
    "section": "Naive approach #1: Image classification",
    "text": "Naive approach #1: Image classification\nNo, of course image classification is not the solution to autonomous flying, but probably it is in there somewhere, and I am following the Fast.ai course which starts off with image classification.\nJeremy Howard is the man behind both the Fast.ai library and the course, and his approach is to create a simple baseline model first:\n\nBaseline: A simple model which you are confident should perform reasonably well. It should be very simple to implement, and very easy to test, so that you can then test each of your improved ideas, and make sure they are always better than your baseline. Without starting with a sensible baseline, it is very difficult to know whether your super-fancy models are actually any good.\n\nSo, that‚Äôs what I am going to do: fine-tune an image classifier with a limited amount of data and with a simple base model.\nThe testing ground for my drone has a couple of very obvious obstacles: lamps handing from the ceiling. So this is going to be a lamp or no-lamp in the room image classifier."
  },
  {
    "objectID": "posts/fly-drone-with-image-classification/index.html#lamp-or-no-lamp-image-classifier",
    "href": "posts/fly-drone-with-image-classification/index.html#lamp-or-no-lamp-image-classifier",
    "title": "Fly a drone with: Image classification",
    "section": "Lamp or no-lamp image classifier üòû ‚Üí üòê ‚Üí üôÇ ‚Üí üòä ‚Üí üòÉ ‚Üí üòÅ",
    "text": "Lamp or no-lamp image classifier üòû ‚Üí üòê ‚Üí üôÇ ‚Üí üòä ‚Üí üòÉ ‚Üí üòÅ\n\nFirst attempt üòû\nMy first attempt can be seen here: Baseline model: Is there a lamp?.\nNot a great result (understatement), it over-fits from the start, and the reason is obvious: label noise (jargon for have data labelled wrong).\nHere are some wonderful samples from the training set:\n\n\n\nlabel noise\n\n\nIt is a bit hard to Google/DDG for interior without hanging lamps, so let‚Äôs try training the model another time after manual cleanup.\n\n\nCleanup of the image labels\nI said manual cleanup, but I‚Äôm going to cheat and use some advanced AI model (CLIP) to be able to train a very simple model.\nCode is written by chatgpt: Lamp Dataset Cleaner (CLIP-based).\nAnd the result is‚Ä¶. MEH! Not much better than what I started with.\nSo, back to manual cleaning. I‚Äôll be back soon‚Ä¶\nOne hour later: DONE!\nThe result is a huge amount of images with a lamp, and very few without it. Reason being: if you use ceiling in a search term, lamps will show up very often. The query home interior wall  -lamp was quite a bit better, not as many lamps! :)\nMore manual cleaning to do‚Ä¶\nAnother hour later: DONE!\n\n\nSecond attempt üòê\nWith the cleaned up data I did another run, see: Baseline model: Is there a lamp? Second attempt.\nThe result I quite a bit better with a ~10% error rate, but there still is over-fitting starting at epoch 2. The 16 top losses look like this:\n\n\n\nTop losses: Prediction/Actual/Loss/Probability\n\n\nTo be fair, some of these images are pretty tough to classify IMO. Anyway, let‚Äôs see if we can do better.\n\n\nThird attempt üòê: Better resizing and Augmentation\nThere‚Äôs a couple of things we can try, here is what I did try:\nResizing with different methods:\n* item_tfms=[Resize(400, ResizeMethod.Pad, pad_mode='zeros')]\n* item_tfms=[Resize(400, ResizeMethod.Squish)]\n* item_tfms=[Resize(400)]\n* item_tfms=[RandomResizedCrop(400, min_scale=0.6)]\n* item_tfms=[RandomResizedCrop(400, min_scale=0.3)]\n* item_tfms=[RandomResizedCrop(400, min_scale=0.8)]\nAugmentation:\n* item_tfms=[Resize(400, ResizeMethod.Pad, pad_mode='zeros')],\n* batch_tfms=aug_transforms()\nThe runs can be seen here: Baseline model: Is there a lamp? Third attempt.\nSomething very annoying is that subsequent runs - without changing anything - have different results, which makes doing experiments impossible. I asked Gemini about it and it said:\n\nYou‚Äôve done a good job providing a seed for reproducibility with seed_everything(42). However, even with careful seeding, variations in results can sometimes occur in deep learning training for a few reasons, especially within an environment like Colab:\n\nGPU Non-Determinism: While you‚Äôve set torch.backends.cudnn.deterministic = True, not all operations on the GPU are guaranteed to be completely deterministic across different runs or hardware configurations. Some highly optimized kernels might involve non-deterministic algorithms for performance reasons.\nBackground Processes in Colab: Colab is a shared environment. While unlikely to cause significant variations, very minor background processes or resource allocation differences could theoretically have a minuscule impact on timing and execution flow, though this is generally not the primary cause of noticeable differences.\n\n\nHmm, not great. What is great, is that Gemini can see the notebook you are working with, so it can be very helpful when needed. Not this time though :)\nConclusion: nothing really helps. The error rate stays around 10%.\nProbably we got everything out of the data we have? Let‚Äôs try a couple more things.\n\n\nFourth attempt üôÇ: smaller batch size\nSo far we had a mini-batch size of 32. Let‚Äôs have look what happens when we decrease that:\n\nNo more over-fitting with 3 epochs.\nError-rate goes down to 6-8%!\n\nMaybe I am over-fitting with my hyper-parameters? It feels a bit like it, but I don‚Äôt have a test set to confirm that‚Ä¶ Let‚Äôs try that later, when we have a bit more data.\nHere is the notebook: Baseline model: Is there a lamp? Fourth attempt.\nI think we have a good baseline here. So let‚Äôs go and throw some more power behind it:\n\nUse a better base model\nUse more training data\n\n\n\nFifth attempt üòä: use a better base model\nI tried a larger resnet model: resnet34, which (with a smaller mini-batch size of 8) resulted in a 4% error rate, without over-fitting for the first few epochs.\nI also had a look here: Which image models are best?, and picked convnext_tiny. This model resulted in a 3% error rate, but is significantly slower. Because we want speedy inference later on, we might want to stick with the resnet models.\nNotebook for this experiment: Is there a lamp? Fifth attempt. I removed the phrase Baseline model, because we are now starting to ramp up the actual base model. Not sure if this is the way, but so far it seems to work. Still a bit worried about the test set that is coming up though‚Ä¶\n\n\nSixth attempt ü§®: collect more data\nMy plan was to now collect more data and do some of the previous steps again, but I changed my mind. The possibility that I am over-fitting by trying different hyper-parameters and picking the winners is substantial.\nThe logical next step would then be to use a test set and see what is going on.\nAlso: I don‚Äôt need to create the best lamp classifier in the world, we are here to learn!"
  },
  {
    "objectID": "posts/fly-drone-with-image-classification/index.html#whats-next",
    "href": "posts/fly-drone-with-image-classification/index.html#whats-next",
    "title": "Fly a drone with: Image classification",
    "section": "What‚Äôs next?",
    "text": "What‚Äôs next?\nAs said, in the next post I will create a test set (from drone frames), figure out test-set best practices, and finalize our classifier. Read on‚Ä¶\n\n\n\n\n\n\n\nSeries: Code, Fly & AI\n\n\n\n‚Üê Previous: Tello controller navigation - Part 2"
  },
  {
    "objectID": "posts/key-navigation-video-snapshots/index.html",
    "href": "posts/key-navigation-video-snapshots/index.html",
    "title": "Key navigation & Video snapshots",
    "section": "",
    "text": "Mark Pors\n\n2025-04-27"
  },
  {
    "objectID": "posts/key-navigation-video-snapshots/index.html#keyboard-navigation",
    "href": "posts/key-navigation-video-snapshots/index.html#keyboard-navigation",
    "title": "Key navigation & Video snapshots",
    "section": "Keyboard navigation",
    "text": "Keyboard navigation\nNow that we can control the TT with code, let‚Äôs extend that to a human in the ground-bound pilot seat. The most simple approach is to use specific keyboard strokes to map to drone navigation commands.\nKeyboard navigation is implemented as an example in the DJSTelloPy repo: manual-control-pygame.py. This is quite a cool piece of code. It uses pygame, which I never used before, so lets go through some of the interesting parts of the code.\n\nThe secret sauce: Event-based controls\nThe example uses pygame‚Äôs event system rather than simple polling, and this creates nice and responsive controls.\n\n# Set up a timer that triggers events at the specified frame rate\npygame.time.set_timer(pygame.USEREVENT + 1, 1000 // FPS)\n\n# In the main loop, process all pending events\nfor event in pygame.event.get():\n    if event.type == pygame.USEREVENT + 1:\n        self.update()\n    elif event.type == pygame.KEYDOWN:\n        self.keydown(event.key)\n    elif event.type == pygame.KEYUP:\n        self.keyup(event.key)\n\nInstead of constantly checking ‚Äúis this key pressed?‚Äù in a loop, the code waits for pygame to tell it when keys are pressed or released. This means:\n\nno missed keypresses, even if they happen very quickly\nimmediate detection of key events\nclear separation between ‚Äúkey is pressed‚Äù and ‚Äúkey is released‚Äù logic\n\nPretty cool.\n\n\nSmooth flying with velocity controls\nAnother interesting bit from the example code is how smoothly it makes the drone fly. This is done through velocity-based control (not up, down, left etc. commands).\n# Initialize velocity variables\nself.for_back_velocity = 0   # Forward/backward\nself.left_right_velocity = 0  # Left/right\nself.up_down_velocity = 0     # Up/down\nself.yaw_velocity = 0         # Rotation\n\n# When UP arrow is pressed\ndef keydown(self, key):\n    if key == pygame.K_UP:\n        self.for_back_velocity = S  # Set to speed value (60)\n    # ...other keys...\n\n# When UP arrow is released\ndef keyup(self, key):\n    if key == pygame.K_UP or key == pygame.K_DOWN:\n        self.for_back_velocity = 0  # Stop movement\n    # ...other keys...\nInstead of sending a ‚Äúmove forward‚Äù command when we press UP, it sets a forward velocity that remains until we release the key. This creates a very natural movement, especially when:\n\nmoving diagonally (pressing UP and RIGHT simultaneously)\ntransitioning between movements (release UP while still holding RIGHT)\nmaking subtle adjustments to flight path\n\n\n\nThe Heartbeat: Why frame rate matters\nSomething that reminds me of my hardware/telco days: the pygame library work synchronously with fixed time intervals. This line sets the heartbeat:\npygame.time.set_timer(pygame.USEREVENT + 1, 1000 // FPS)\nBasically at each interval it listens for user events and handles them as needed. At 120 FPS, it generates an event every 8.3 milliseconds, triggering e.g.¬†our update function:\ndef update(self):\n    if self.send_rc_control:\n        self.tello.send_rc_control(\n            self.left_right_velocity,\n            self.for_back_velocity,\n            self.up_down_velocity, \n            self.yaw_velocity\n        )\n\n\nWhy not send commands immediately?\nThe naive alternative is to have a while True: loop that reads key strokes and react to that instantly. What‚Äôs wrong with that?\n\nCommand Rate Control: Drones can get overwhelmed if we send too many commands too quickly\nCommand Combination: If we press multiple keys in one frame, they‚Äôre combined into a single efficient command\nSmooth Motion: Even, regular command timing creates more natural drone movement\n\nI tried the while True approach and it doesn‚Äôt work that great. There is no feel between pressing keys and the drone following up on that.\nSo, I learned something new: pygame, which I‚Äôm sure will come in handy soon (spoiler alert: after this I want to use the GameSire controller to fly the Tello. Guess which library is great to speak to gaming consoles!)."
  },
  {
    "objectID": "posts/key-navigation-video-snapshots/index.html#video-streaming-snapshots",
    "href": "posts/key-navigation-video-snapshots/index.html#video-streaming-snapshots",
    "title": "Key navigation & Video snapshots",
    "section": "Video streaming & snapshots",
    "text": "Video streaming & snapshots\nThere is no autonomous flying without vision, and now is the first time we can have a peek at streaming video and capturing it.\nThe same example from above contains the video streaming as well.\n\nVideo streaming\n\nSetting Up the Stream\nFirst, the code initializes the video stream:\n# Make sure streaming is off before we start\nself.tello.streamoff()\n# Then turn streaming on\nself.tello.streamon()\n\n# Get the object that will give us frames\nframe_read = self.tello.get_frame_read()\nThis pattern of turning streaming off then on again is a good practice to ensure we‚Äôre starting with a clean slate. I had it hang a couple of times before applying this trick.\n\n\nCapturing and processing frames\nIn the main loop, the code grabs frames from the drone and processes them:\n# Get the latest frame\nframe = frame_read.frame\n\n# Add battery information to the frame\ntext = \"Battery: {}%\".format(self.tello.get_battery())\ncv2.putText(frame, text, (5, 720 - 5),\n    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\nThis overlays the battery percentage in red text at the bottom left of the frame. A handy feature when we‚Äôre flying!\n\n\nFrame transformation\nThe next three lines are needed for displaying the frame correctly in pygame:\n# OpenCV uses BGR, pygame needs RGB\nframe = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n# Rotate the frame 90 degrees\nframe = np.rot90(frame)\n\n# Flip the frame upside down\nframe = np.flipud(frame)\nWhy all this transformation? Cameras often capture images in orientations or color formats that aren‚Äôt immediately displayable. OpenCV uses BGR color format while pygame expects RGB, and the Tello camera orientation needs adjusting to appear correctly on screen.\n\n\nDisplaying in pygame\nFinally, the frame is displayed in the pygame window:\n# Convert numpy array to a pygame surface\nframe = pygame.surfarray.make_surface(frame)\n\n# Draw the surface to the screen\nself.screen.blit(frame, (0, 0))\n\n# Update the display\npygame.display.update()\nThis process happens every frame (120 times per second with the default settings), creating a smooth video feed.\n\n\n\nVideo snapshots\nNot included in the example code is the ability to take a snapshot by pressing a key and saving the current frame to disk.\nSo let‚Äôs add it:\n\n1. First, add the necessary import\nAt the top of the file, make sure we have:\nimport os\n\n\n2. Create a directory to store images\nAdd this near the beginning of our __init__ method:\n# Create a directory to store images if it doesn't exist\nif not os.path.exists('tello_images'):\n    os.makedirs('tello_images')\n\n\n3. Add a key handler for taking snapshots\nIn the keyup method, add a case for a new key (I‚Äôll use ‚Äòp‚Äô for ‚Äúpicture‚Äù):\ndef keyup(self, key):\n    \"\"\" Update velocities based on key released \"\"\"\n    if key == pygame.K_UP or key == pygame.K_DOWN:  # set zero forward/backward velocity\n        self.for_back_velocity = 0\n    elif key == pygame.K_LEFT or key == pygame.K_RIGHT:  # set zero left/right velocity\n        self.left_right_velocity = 0\n    elif key == pygame.K_w or key == pygame.K_s:  # set zero up/down velocity\n        self.up_down_velocity = 0\n    elif key == pygame.K_a or key == pygame.K_d:  # set zero yaw velocity\n        self.yaw_velocity = 0\n    elif key == pygame.K_t:  # takeoff\n        self.tello.takeoff()\n        self.send_rc_control = True\n    elif key == pygame.K_l:  # land\n        not self.tello.land()\n        self.send_rc_control = False\n    elif key == pygame.K_p:  # take a snapshot\n        self.take_snapshot()\n\n\n4. Add the snapshot method\nAdd this new method to our FrontEnd class:\ndef take_snapshot(self):\n    \"\"\"\n    Take a snapshot of the current frame and save it to disk\n    \"\"\"\n    # Get the current frame\n    frame = self.tello.get_frame_read().frame\n    \n    if frame is not None:\n        # Create a filename with timestamp\n        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n        filename = f\"tello_images/tello_snapshot_{timestamp}.jpg\"\n        \n        # Save the image - note we save the original frame before any transformations\n        cv2.imwrite(filename, frame)\n        \n        print(f\"Snapshot saved: {filename}\")\nThe resulting code can be found here.\n\n\n\nLet‚Äôs try it out!\nIt all works just fine, here are some low res snapshots I made:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNavigating with a keyboard is a disaster, so‚Ä¶."
  },
  {
    "objectID": "posts/key-navigation-video-snapshots/index.html#whats-next",
    "href": "posts/key-navigation-video-snapshots/index.html#whats-next",
    "title": "Key navigation & Video snapshots",
    "section": "What‚Äôs next?",
    "text": "What‚Äôs next?\nTo improve navigation we are going to hook up the GameSir T1d, read on...\n\n\n\n\n\n\n\nSeries: Code, Fly & AI\n\n\n\n‚Üê Previous: Crash the Tello\n\n\nNext: GameSir T1d & pygame ‚Üí"
  },
  {
    "objectID": "posts/gamesir-t1d-package/index.html",
    "href": "posts/gamesir-t1d-package/index.html",
    "title": "Interlude: GameSir T1d Python package",
    "section": "",
    "text": "Mark Pors\n\n2025-05-04"
  },
  {
    "objectID": "posts/gamesir-t1d-package/index.html#lets-package-up-the-gamesir-t1d-wrapper",
    "href": "posts/gamesir-t1d-package/index.html#lets-package-up-the-gamesir-t1d-wrapper",
    "title": "Interlude: GameSir T1d Python package",
    "section": "Let‚Äôs package up the Gamesir T1d wrapper",
    "text": "Let‚Äôs package up the Gamesir T1d wrapper\nFor some context: this post is a tangent on this article about making the GameSir T1d controller work with pygame.\nI know, no one is ever going to use it apart from me. The Gamesir T1d is ancient, and the use case (using it with pygame to control a Tello) is also not very common. But I‚Äôm here to learn and try things out, and this gives me the opportunity to play with:\n\nuv: this ‚Äúnew‚Äù package manager is pretty great, and I need to use it more often to get rid of old habits (pip, venv, pyenv, etc.).\nCreate a pypi package: I have never created and published a pypi package, so fun to give that a shot.\nClaude code: I have used this in the past and it seems promising, so let‚Äôs vibe-code our way to a package.\n\nBut first, a logo! :P\n\n\n\nGameSir T1d Python package"
  },
  {
    "objectID": "posts/gamesir-t1d-package/index.html#six-steps-to-a-published-package",
    "href": "posts/gamesir-t1d-package/index.html#six-steps-to-a-published-package",
    "title": "Interlude: GameSir T1d Python package",
    "section": "Six steps to a published package",
    "text": "Six steps to a published package\n\nStep 1: Ditch Claude code\nIt is really very easy to create a pypi package, but Claude code overcomplicated things, and in the end it got stuck. So the first step: ditch Claude code. I‚Äôm not a big fan of vibe coding (at least not today), and this confirms again why.\n\n\nStep 2: Create a project\nuv has a built-in way to create projects we want to publish as a package, using the --lib switch:\nuv init --lib gamesir-t1d\ncd gamesir-t1d\nNext, we need to add bleak as a dependency:\nuv add bleak\nFor the example code to work we also need to add pygame.\nuv add pygame\nIt creates the files (and virtual env) we need as a starting point:\n.\n‚îú‚îÄ‚îÄ pyproject.toml\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ src\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ gamesir_t1d\n‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ __init__.py\n‚îî‚îÄ‚îÄ uv.lock\n\n\nStep 3: Add modules\nThis is simply moving the code we created in the previous post post into the right spots. Which results in:\n.\n‚îú‚îÄ‚îÄ LICENSE\n‚îú‚îÄ‚îÄ pyproject.toml\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ src\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ gamesir_t1d\n‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ controller.py\n‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ examples\n‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ pygame_example.py\n‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ tools\n‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ¬†¬†         ‚îî‚îÄ‚îÄ ble_scanner.py\n‚îî‚îÄ‚îÄ uv.lock\nMake sure the pyproject.toml has the right metadata. The latest version is here: pyproject.toml.\nAn interesting feature is to include scripts in a package. After installing a package you can run these scripts on the command line. This is done in the pyproject.toml as:\n[project.scripts]\ngamesir-scan = \"gamesir_t1d.tools.ble_scanner:run_scanner\"\nThis is perfect for us to make the BLE scanner available, so users can figure out the name of their controller.\n\n\nStep 4: Build and publish\nuv build\nThis creates two files in the /dist directory:\n\ngamesir_t1d-0.1.1-py3-none-any.whl\ngamesir_t1d-0.1.1.tar.gz\n\nwhich are the binary and the source version of the package.\nLet‚Äôs publish it to test.pypi.org first:\n\nCreate an account on test.pypi.org\nObtain an API token\nRun this command:\n\nuv uv publish --token pypi-TEST_TOKEN_HERE --publish-url https://test.pypi.org/legacy/\n\n\nStep 5: Test\nTo test it, we create a new project and install the package there.\nuv init gstest\ncd gstest\npip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple gamesir-t1d\npython -c \"import gamesir_t1d; print(gamesir_t1d.__version__)\"\nIf that prints the version number, it all works!\nNow, let‚Äôs try the scanner (make sure the controller is switched on):\n$ gamesir-scan\n\nMake sure the GameSir-T1d controller is turned on and in pairing mode.\n(Typically hold power button until LEDs flash rapidly)\nPress Enter to start scanning...\nStarting BLE scan for GameSir-T1d controller...\nScanning for BLE devices (timeout: 3.0s)...\nFound 14 Bluetooth devices:\n1. Name: None, Address: E6682D99-DC5A-EE5A-9E95-DAC5BF163FC1\n...\n6. Name: Gamesir-T1d-39BD, Address: FDF00BC3-1DEE-1525-0B34-7E2D3391C401\n...\n13. Name: None, Address: 19EC6BCE-BE63-CD90-E9D6-9C91EA838008\n14. Name: None, Address: 3222F9B7-2970-77BF-6814-9FB82F843839\nFound controller: Gamesir-T1d-39BD, Address: FDF00BC3-1DEE-1525-0B34-7E2D3391C401\nAttempting to connect to Gamesir-T1d-39BD...\nSuccessfully connected to Gamesir-T1d-39BD!\n\nAvailable services and characteristics:\nService: 00008650-0000-1000-8000-00805f9b34fb\n  Characteristic: 00008651-0000-1000-8000-00805f9b34fb\n    Properties: ['read', 'write', 'notify', 'indicate']\n  Characteristic: 00008655-0000-1000-8000-00805f9b34fb\n    Properties: ['read', 'write', 'notify', 'indicate']\n  Characteristic: 0000865f-0000-1000-8000-00805f9b34fb\n    Properties: ['read', 'write', 'notify', 'indicate']\nService: 0000180a-0000-1000-8000-00805f9b34fb\n  Characteristic: 00002a24-0000-1000-8000-00805f9b34fb\n    Properties: ['read']\n  Characteristic: 00002a25-0000-1000-8000-00805f9b34fb\n    Properties: ['read']\n# Assuming you've installed the package with the [examples] extra\n  Characteristic: 00002a27-0000-1000-8000-00805f9b34fb\n    Properties: ['read']\n  Characteristic: 00002a26-0000-1000-8000-00805f9b34fb\n    Properties: ['read']\n  Characteristic: 00002a50-0000-1000-8000-00805f9b34fb\n    Properties: ['read']\n\nConnection successful. Press Ctrl+C to exit...\nNow we have the controller ID, let‚Äôs run a basic script:\nfrom gamesir_t1d import GameSirT1dPygame\n\n# Create the controller object with your controller's name\ncontroller = GameSirT1dPygame(\"Gamesir-T1d-39BD\")  # Replace XXXX with your controller ID\n\n# Initialize the controller (starts BLE connection)\ncontroller.init()\n\n# Read axes and buttons using the pygame-compatible interface\nleft_x = controller.get_axis(0)  # Range: -1.0 to 1.0\nleft_y = controller.get_axis(1)  # Range: -1.0 to 1.0\na_button = controller.get_button(0)  # 1 for pressed, 0 for not pressed\n\n# Check if the controller is connected\nif controller.is_connected():\n    print(\"Controller is connected!\")\n\n# Clean up when done\ncontroller.quit()\n\nScanning for Gamesir-T1d-39BD...\nFound Gamesir-T1d-39BD at FDF00BC3-1DEE-1525-0B34-7E2D3391C401\nConnecting...\nConnected!\nController is connected!\n\n\n\nStep 6: Publish for real!\nIt all seems to work, so now we can publish the package on pypi.org! Again, create an account and generate an API token first, then:\nuv publish --token pypi-YourToken\nAnd there we have it, our package: pypi.org/project/gamesir-t1d/!\nWe repeat the same steps as before for testing:\nuv init gstest2\ncd gstest2\nuv add gamesir-t1d\npython -c \"import gamesir_t1d; print(gamesir_t1d.__version__)\"\nNo need to run the scanner, the controller ID is still what is was :)\nNow, run the examples included in the package:\nfrom gamesir_t1d.examples.pygame_example import test_without_pygame\n\ntest_without_pygame(\"Gamesir-T1d-XXXX\")  # Replace XXXX with your controller ID\nand\nfrom gamesir_t1d.examples import run\n\nrun(\"Gamesir-T1d-XXXX\")  # Replace XXXX with your controller ID\nThis will run the test as shown in this video.\nThe source code can be found here."
  },
  {
    "objectID": "posts/gamesir-t1d-package/index.html#wrapping-it-up",
    "href": "posts/gamesir-t1d-package/index.html#wrapping-it-up",
    "title": "Interlude: GameSir T1d Python package",
    "section": "Wrapping it up!",
    "text": "Wrapping it up!\nHaha, dad joke there.\nThere is nothing to wrap up anyway.\nIf you are interested in reading more about drones, coding, and machine learning (soon), start here: Code, Fly & AI."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DroneLab - Coding Autonomous Drones in Baby Steps",
    "section": "",
    "text": "Getting reproducible training results with Fast.ai + PyTorch\n\n\n\nmachine learning\n\ncolab\n\ncomputer vision\n\nfast.ai\n\npytorch\n\n\n\nWhy your Fast.ai or PyTorch experiments keep giving different results even when you think you‚Äôve seeded everything properly. Spoiler: it‚Äôs not just about random seeds - DataLoaders have hidden state that screws things up. Here‚Äôs how to actually fix it.\n\n\n\n\n\nMay 25, 2025\n\n\nMark Pors\n\n\n\n\n\n\n\n\n\n\n\n\nInterlude: Which image models are best? UPDATED\n\n\n\nmachine learning\n\ntimm\n\ncomputer vision\n\nfast.ai\n\nopen-source\n\n\n\nA practical guide to choosing the best image classification models using updated timm data, interactive notebooks, and a Gradio app. Learn how to compare, visualize, and select top-performing models for your computer vision projects.\n\n\n\n\n\nMay 19, 2025\n\n\nMark Pors\n\n\n\n\n\n\n\n\n\n\n\n\nFly a drone with: Image classification\n\n\n\ndrone\n\ncode\n\ntello\n\nmachine learning\n\nimage classification\n\nfast.ai\n\n\n\nLearn how to train a drone to detect obstacles using image classification with fast.ai. This post covers practical steps for building, cleaning, and improving an image classifier, including tips on data collection, augmentation, and model selection.\n\n\n\n\n\nMay 15, 2025\n\n\nMark Pors\n\n\n\n\n\n\n\n\n\n\n\n\nTello controller navigation - Part 2\n\n\n\ndrone\n\ncode\n\npygame\n\ntello\n\n\n\nLearn how to connect a GameSir T1d controller to a real Tello drone using Python, integrate live video streaming with Pygame, and set up a foundation for future autonomous drone experiments.\n\n\n\n\n\nMay 11, 2025\n\n\nMark Pors\n\n\n\n\n\n\n\n\n\n\n\n\nTello controller navigation - Part 1\n\n\n\ndrone\n\ncode\n\npygame\n\nsimulator\n\ntello\n\n\n\nSimulate and control a Tello drone using Python, pygame, and a GameSir T1d controller‚Äîlearn practical code architecture, input smoothing, and debugging techniques for drone development.\n\n\n\n\n\nMay 7, 2025\n\n\nMark Pors\n\n\n\n\n\n\n\n\n\n\n\n\nInterlude: GameSir T1d Python package\n\n\n\npygame\n\ncode\n\nopen-source\n\ntello\n\n\n\nStep-by-step guide to packaging, publishing, and using a Python library for the GameSir T1d Bluetooth controller with pygame, using the modern uv toolchain.\n\n\n\n\n\nMay 4, 2025\n\n\nMark Pors\n\n\n\n\n\n\n\n\n\n\n\n\nGameSir T1d controller & pygame\n\n\n\ndrone\n\ncode\n\npygame\n\ntello\n\n\n\nLearn how to connect and use the GameSir T1d controller with your computer for Tello drone control using Python, BLE hacking, and a pygame-compatible wrapper. Step-by-step guide with working code.\n\n\n\n\n\nMay 2, 2025\n\n\nMark Pors\n\n\n\n\n\n\n\n\n\n\n\n\nKey navigation & Video snapshots\n\n\n\ndrone\n\ncode\n\npygame\n\ntello\n\n\n\nHands-on guide to controlling a Tello drone with Python and pygame: learn responsive keyboard navigation, smooth velocity-based movement, live video streaming, and how to capture snapshots from the drone feed.\n\n\n\n\n\nApr 27, 2025\n\n\nMark Pors\n\n\n\n\n\n\n\n\n\n\n\n\nInterlude: I‚Äôm a Pilot!\n\n\n\ndrone\n\nopen-source\n\n\n\nPassed the EASA A1/A3 drone pilot exam and built a free web-based practice app. Learn how to prepare for the EASA UAV certification and access open-source study tools.\n\n\n\n\n\nApr 25, 2025\n\n\nMark Pors\n\n\n\n\n\n\n\n\n\n\n\n\nCrash the Tello (with and without code)\n\n\n\ndrone\n\ncode\n\ntello\n\n\n\nLearn how to fly and program the DJI Tello (RoboMaster TT) drone, avoid common beginner crashes, and get started with Python coding using the Tello SDK.\n\n\n\n\n\nApr 24, 2025\n\n\nMark Pors\n\n\n\n\n\n\n\n\n\n\n\n\nEnter the Tello\n\n\n\ndrone\n\ntello\n\n\n\nDiscover how to start building an autonomous scouting drone for hiking using the DJI RoboMaster TT (Tello Talent). Follow a beginner‚Äôs journey into drones, coding, and computer vision.\n\n\n\n\n\nApr 20, 2025\n\n\nMark Pors\n\n\n\n\n\nNo matching items"
  }
]