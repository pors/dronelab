[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hey, I’m Mark. On this blog, dronelab.dev, I’m exploring machine learning by teaching a drone how to fly itself. I break things down into small steps and share what I discover along the way.\nHope you find it interesting! Drop me a line if you do: mark [at] pors [dot] net.\nFind me on socials below 👇"
  },
  {
    "objectID": "posts/gamesir-t1d-controller/index.html",
    "href": "posts/gamesir-t1d-controller/index.html",
    "title": "GameSir T1d controller & pygame",
    "section": "",
    "text": "Mark Pors\n\n2025-05-02\nControlling a drone with a keyboard is not great, neither is using the touch-screen of a phone. That’s why I bought the recommended controller for the Tello: the GameSir T1d."
  },
  {
    "objectID": "posts/gamesir-t1d-controller/index.html#hook-up-the-gamesir-t1d",
    "href": "posts/gamesir-t1d-controller/index.html#hook-up-the-gamesir-t1d",
    "title": "GameSir T1d controller & pygame",
    "section": "Hook up the GameSir T1d",
    "text": "Hook up the GameSir T1d\nInitially this seemed like a no-brainer, and I was surprised why no-one else hadn’t done it yet: replace the keyboard strokes with signals from the GameSir T1d controller. It turns out that this controller was specifically modified to connect ONLY through the Tello app.\nNormally the code below is enough to connect to a game controller from a python script:\nimport pygame\n\npygame.init()\npygame.joystick.init()\n\n# Check for connected controllers\njoystick_count = pygame.joystick.get_count()\n\nif joystick_count == 0:\n    print(\"No controller detected. Please connect your GameSir T1d controller.\")\n    return None\n\n# Initialize the first controller detected\njoystick = pygame.joystick.Joystick(0)\njoystick.init()\nIn our case the output was:\nNo controller detected. Please connect your GameSir T1d controller.\nOh oh, problem!"
  },
  {
    "objectID": "posts/gamesir-t1d-controller/index.html#hack-it",
    "href": "posts/gamesir-t1d-controller/index.html#hack-it",
    "title": "GameSir T1d controller & pygame",
    "section": "Hack it!",
    "text": "Hack it!\nAfter some searching and LLM’ing it became clear that there is a way to make it work.\nPfew!\nFor Internet historians, here is the evolution of the hack:\n\noriginal Python 2 script\nported to Python 3\nusing bleak\nrefactor"
  },
  {
    "objectID": "posts/gamesir-t1d-controller/index.html#gamesir-t1d-hack-version-5",
    "href": "posts/gamesir-t1d-controller/index.html#gamesir-t1d-hack-version-5",
    "title": "GameSir T1d controller & pygame",
    "section": "GameSir T1D hack version 5",
    "text": "GameSir T1D hack version 5\nI’m honored to step in the footsteps of these four hackers and try to get iteration five to work!\nFirst we need to get the controller to connect. Simply pairing via Bluetooth won’t work.\nThe GameSir T1d gets into pairing mode by simply powering it on. The four blue power LEDs start blinking when trying to connect. At that state we run the script below (on github: gamesirT1d-connect.py).\n\n\n\n\n\n\nTip\n\n\n\nThere is a tiny button labeled pair above another tiny button labeled C1. This pair button can be used to pair a new device (great UX!). Clicking it while connected, will cause the Bluetooth connection to be dropped. So don’t click it while controlling a drone!\n\n\n\nBLE Controller Connection PreviewFull scriptOutput\n\n\nimport asyncio\nfrom bleak import BleakClient, BleakScanner\n\n# The name our controller should broadcast as\nCONTROLLER_NAME = \"Gamesir-T1d\"\n\nasync def main():\n    print(\"Starting BLE scan for GameSir-T1d controller...\")\n\n\nimport asyncio\nfrom bleak import BleakClient, BleakScanner\n\n# The name our controller should broadcast as\nCONTROLLER_NAME = \"Gamesir-T1d\"\n\nasync def main():\n    print(\"Starting BLE scan for GameSir-T1d controller...\")\n\n    # First, scan for all available BLE devices\n    print(\"Scanning for BLE devices...\")\n    devices = await BleakScanner.discover()\n\n    # Print all found devices to help with debugging\n    print(f\"Found {len(devices)} Bluetooth devices:\")\n    for i, device in enumerate(devices):\n        print(f\"{i+1}. Name: {device.name}, Address: {device.address}\")\n\n    # Try to find our controller\n    target_device = None\n    for device in devices:\n        if device.name and CONTROLLER_NAME.lower() in device.name.lower():\n            target_device = device\n            print(f\"Found controller: {device.name}, Address: {device.address}\")\n            break\n\n    if not target_device:\n        print(f\"No device found with name containing '{CONTROLLER_NAME}'\")\n        print(\"Is the controller turned on and in pairing mode?\")\n        return\n\n    # Try to connect to the controller\n    print(f\"Attempting to connect to {target_device.name}...\")\n    try:\n        async with BleakClient(target_device.address, timeout=10.0) as client:\n            if client.is_connected:\n                print(f\"Successfully connected to {target_device.name}!\")\n\n                # List available services and characteristics\n                print(\"\\nAvailable services and characteristics:\")\n                for service in client.services:\n                    print(f\"Service: {service.uuid}\")\n                    for char in service.characteristics:\n                        print(f\"  Characteristic: {char.uuid}\")\n                        print(f\"    Properties: {char.properties}\")\n\n                # Wait a moment so we can see the connection is established\n                print(\"\\nConnection successful. Press Ctrl+C to exit...\")\n                await asyncio.sleep(10)\n            else:\n                print(\"Failed to connect\")\n    except Exception as e:\n        print(f\"Error connecting to device: {e}\")\n\n\nif __name__ == \"__main__\":\n    # Make sure controller is in pairing mode before running this\n    print(\"Make sure the GameSir-T1d controller is turned on and in pairing mode.\")\n    print(\"(Typically hold power button until LEDs flash rapidly)\")\n    input(\"Press Enter to start scanning...\")\n\n    # Run the async main function\n    asyncio.run(main())\n\n\nMake sure the GameSir-T1d controller is turned on and in pairing mode.\n(Typically hold power button until LEDs flash rapidly)\nPress Enter to start scanning...\nStarting BLE scan for GameSir-T1d controller...\nScanning for BLE devices...\nFound 11 Bluetooth devices:\n1. Name: Gamesir-T1d-39BD, Address: FDF00BC3-1DEE-1525-0B34-7E2D3391C401\n2. Name: None, Address: 3A2C8191-D3F5-F471-BC81-75AFE2DB0D60\n3. Name: None, Address: 772F5433-AAE9-D456-209C-DEA32D192E10\n...\n11. Name: None, Address: 80334171-1943-B3DE-7DDD-773753B852C3\nFound controller: Gamesir-T1d-39BD, Address: FDF00BC3-1DEE-1525-0B34-7E2D3391C401\nAttempting to connect to Gamesir-T1d-39BD...\nSuccessfully connected to Gamesir-T1d-39BD!\n\nAvailable services and characteristics:\nService: 00008650-0000-1000-8000-00805f9b34fb\n  Characteristic: 00008651-0000-1000-8000-00805f9b34fb\n    Properties: ['read', 'write', 'notify', 'indicate']\n  Characteristic: 00008655-0000-1000-8000-00805f9b34fb\n    Properties: ['read', 'write', 'notify', 'indicate']\n  Characteristic: 0000865f-0000-1000-8000-00805f9b34fb\n    Properties: ['read', 'write', 'notify', 'indicate']\nService: 0000180a-0000-1000-8000-00805f9b34fb\n  Characteristic: 00002a24-0000-1000-8000-00805f9b34fb\n    Properties: ['read']\n  Characteristic: 00002a25-0000-1000-8000-00805f9b34fb\n    Properties: ['read']\n  Characteristic: 00002a27-0000-1000-8000-00805f9b34fb\n    Properties: ['read']\n  Characteristic: 00002a26-0000-1000-8000-00805f9b34fb\n    Properties: ['read']\n  Characteristic: 00002a50-0000-1000-8000-00805f9b34fb\n    Properties: ['read']\n\nConnection successful. Press Ctrl+C to exit...\n\n\n\nIf you have inspected the output tab, you see we succeeded! Yay, let’s move on."
  },
  {
    "objectID": "posts/gamesir-t1d-controller/index.html#read-the-t1d-controller-state",
    "href": "posts/gamesir-t1d-controller/index.html#read-the-t1d-controller-state",
    "title": "GameSir T1d controller & pygame",
    "section": "Read the T1d controller state",
    "text": "Read the T1d controller state\nNow the controller is connected to the computer, let’s see if we can read joystick and button changes.\nThere’s a heap of input elements that need to be read and made compatible with gamepy:\n\nBoth joysticks (LX, LY, RX, RY)\nAnalog triggers (L2, R2)\nButtons (A, B, X, Y, L1, R1, C1, C2, Menu)\nD-pad (Up, Down, Left, Right)\n\nThe script below prints the real-time value of each of these inputs while using the controller (on github: gamesirT1d-read.py).\nFrom here on we don’t need to pair the controller, we just use the device name that was identified in the previous step.\n\nRead Controller State PreviewFull scriptOutput\n\n\nimport asyncio\nfrom bleak import BleakClient, BleakScanner\n\n# The exact name our controller showed up as\nCONTROLLER_NAME = \"Gamesir-T1d-39BD\"\n# The characteristic we want to read\nCHARACTERISTIC_UUID = \"00008651-0000-1000-8000-00805f9b34fb\"\n\nclass GameSirT1d:\n\n\nimport asyncio\nfrom bleak import BleakClient, BleakScanner\n\n# The exact name our controller showed up as\nCONTROLLER_NAME = \"Gamesir-T1d-39BD\"\n# The characteristic we want to read\nCHARACTERISTIC_UUID = \"00008651-0000-1000-8000-00805f9b34fb\"\n\nclass GameSirT1d:\n    def __init__(self):\n        # Joystick values (0-1023, with 512 as center)\n        self.lx = 512\n        self.ly = 512\n        self.rx = 512\n        self.ry = 512\n        \n        # Analog triggers (0-255)\n        self.l2 = 0\n        self.r2 = 0\n        \n        # Digital buttons (0 or 1)\n        self.a = 0\n        self.b = 0\n        self.x = 0\n        self.y = 0\n        self.l1 = 0\n        self.r1 = 0\n        self.c1 = 0\n        self.c2 = 0\n        self.menu = 0\n        \n        # D-pad\n        self.dpad_up = 0\n        self.dpad_down = 0\n        self.dpad_left = 0\n        self.dpad_right = 0\n        \n        # Connection state\n        self.connected = False\n        self._client = None\n    \n    def parse_data(self, data):\n        \"\"\"Parse the raw data from the controller\"\"\"\n        if len(data) &lt; 12:\n            return False\n        \n        # Parse joysticks\n        self.lx = ((data[2]) &lt;&lt; 2) | (data[3] &gt;&gt; 6)\n        self.ly = ((data[3] & 0x3f) &lt;&lt; 4) + (data[4] &gt;&gt; 4)\n        self.rx = ((data[4] & 0xf) &lt;&lt; 6) | (data[5] &gt;&gt; 2)\n        self.ry = ((data[5] & 0x3) &lt;&lt; 8) + ((data[6]))\n        \n        # Parse triggers\n        self.l2 = data[7]\n        self.r2 = data[8]\n        \n        # Parse buttons from byte 9\n        buttons = data[9]\n        self.a = int(bool(buttons & 0x01))\n        self.b = int(bool(buttons & 0x02))\n        self.menu = int(bool(buttons & 0x04))\n        self.x = int(bool(buttons & 0x08))\n        self.y = int(bool(buttons & 0x10))\n        self.l1 = int(bool(buttons & 0x40))\n        self.r1 = int(bool(buttons & 0x80))\n        \n        # Parse more buttons from byte 10\n        buttons2 = data[10]\n        self.c1 = int(bool(buttons2 & 0x04))\n        self.c2 = int(bool(buttons2 & 0x08))\n        \n        # Parse D-pad from byte 11\n        dpad = data[11]\n        self.dpad_up = int(dpad == 0x01)\n        self.dpad_right = int(dpad == 0x03)\n        self.dpad_down = int(dpad == 0x05)\n        self.dpad_left = int(dpad == 0x07)\n        \n        return True\n    \n    def __str__(self):\n        \"\"\"Return a string representation of the controller state\"\"\"\n        return (\n            f\"Joysticks: LX={self.lx}, LY={self.ly}, RX={self.rx}, RY={self.ry}\\n\"\n            f\"Triggers: L2={self.l2}, R2={self.r2}\\n\"\n            f\"Buttons: A={self.a}, B={self.b}, X={self.x}, Y={self.y}, \"\n            f\"L1={self.l1}, R1={self.r1}, C1={self.c1}, C2={self.c2}, Menu={self.menu}\\n\"\n            f\"D-pad: Up={self.dpad_up}, Down={self.dpad_down}, Left={self.dpad_left}, Right={self.dpad_right}\"\n        )\n    \n    # Add methods to get normalized values (-1.0 to 1.0) for joysticks\n    def get_left_stick(self):\n        \"\"\"Get normalized values for left stick (-1.0 to 1.0)\"\"\"\n        x = (self.lx - 512) / 512  # -1.0 to 1.0\n        y = (self.ly - 512) / 512  # -1.0 to 1.0\n        return (x, y)\n    \n    def get_right_stick(self):\n        \"\"\"Get normalized values for right stick (-1.0 to 1.0)\"\"\"\n        x = (self.rx - 512) / 512  # -1.0 to 1.0\n        y = (self.ry - 512) / 512  # -1.0 to 1.0\n        return (x, y)\n\nasync def main():\n    controller = GameSirT1d()\n    \n    print(f\"Scanning for {CONTROLLER_NAME}...\")\n    device = await BleakScanner.find_device_by_name(CONTROLLER_NAME)\n    \n    if not device:\n        print(f\"Could not find {CONTROLLER_NAME}. Is it turned on?\")\n        return\n    \n    print(f\"Found {CONTROLLER_NAME} at {device.address}\")\n    print(\"Connecting...\")\n    \n    try:\n        async with BleakClient(device.address) as client:\n            print(\"Connected!\")\n            controller.connected = True\n            controller._client = client\n            \n            try:\n                while controller.connected:\n                    # Read current state\n                    data = await client.read_gatt_char(CHARACTERISTIC_UUID)\n                    \n                    # Parse the data\n                    if controller.parse_data(data):\n                        # Get normalized stick values for easier use\n                        left_x, left_y = controller.get_left_stick()\n                        right_x, right_y = controller.get_right_stick()\n                        \n                        # Clear the line and print current state\n                        print(f\"\\rLeft: ({left_x:.2f}, {left_y:.2f}) Right: ({right_x:.2f}, {right_y:.2f}) | \"\n                              f\"A:{controller.a} B:{controller.b} X:{controller.x} Y:{controller.y} \"\n                              f\"L1:{controller.l1} R1:{controller.r1} L2:{controller.l2} R2:{controller.r2}\", end=\"\")\n                    \n                    # Wait a bit before next reading\n                    await asyncio.sleep(0.05)  # 20Hz polling rate\n                    \n            except KeyboardInterrupt:\n                print(\"\\nStopping...\")\n                controller.connected = False\n    \n    except Exception as e:\n        print(f\"\\nError: {e}\")\n        controller.connected = False\n\nif __name__ == \"__main__\":\n    print(\"GameSir T1d Controller Test\")\n    print(\"Move joysticks and press buttons to see values\")\n    print(\"Press Ctrl+C to exit\")\n    \n    asyncio.run(main())\n\n\nGameSir T1d Controller Test\nMove joysticks and press buttons to see values\nPress Ctrl+C to exit\nScanning for Gamesir-T1d-39BD...\nFound Gamesir-T1d-39BD at FDF00BC3-1DEE-1525-0B34-7E2D3391C401\nConnecting...\nConnected!\nLeft: (0.05, 0.03) Right: (-0.10, -0.13) | A:1 B:0 X:0 Y:0 L1:0 R1:0 L2:3 R2:155\n\n\n\nThat seems to work pretty good! Now we can move on and create a wrapper that behaves as if it was part of a gamepy compatible controller."
  },
  {
    "objectID": "posts/gamesir-t1d-controller/index.html#gamesir-t1d-pygame-compatible-wrapper",
    "href": "posts/gamesir-t1d-controller/index.html#gamesir-t1d-pygame-compatible-wrapper",
    "title": "GameSir T1d controller & pygame",
    "section": "GameSir T1d Pygame-Compatible Wrapper",
    "text": "GameSir T1d Pygame-Compatible Wrapper\nThe wrapper functions as a bridge between two worlds:\n\nThe BLE communication layer that talks directly to our GameSir T1d\nA pygame-compatible interface that provides familiar methods like get_axis() and get_button()\n\nThis allows our drone control code to interact with the controller as if it were a standard pygame joystick, while the BLE communication happens behind the scenes.\nThe wrapper code consists of two classes:\n\nGameSirT1d: this class parses the raw inputs from the controller and converts it to the format and ranges we expect in a pygame controller.\nGameSirT1dPygame: this class implements the BLE interface and provides the pygame compatible wrapper.\n\nThe code can be found here: gamesir_t1d_pygame.py."
  },
  {
    "objectID": "posts/gamesir-t1d-controller/index.html#lets-try-it-out",
    "href": "posts/gamesir-t1d-controller/index.html#lets-try-it-out",
    "title": "GameSir T1d controller & pygame",
    "section": "Let’s try it out!",
    "text": "Let’s try it out!\nWith this little example script we can see if the controller (at least the thumpsticks) does what we expect:\nimport pygame\nfrom gamesir_t1d_pygame import GameSirT1dPygame\n\n\ndef main(controller_name):\n    # Initialize pygame for window and graphics\n    pygame.init()\n    screen = pygame.display.set_mode((640, 480))\n    pygame.display.set_caption(\"GameSir T1d Test\")\n    clock = pygame.time.Clock()\n\n    # Initialize our custom controller\n    controller = GameSirT1dPygame(controller_name)\n    print(\"Connecting to controller...\")\n    if not controller.init():\n        print(\"Failed to connect to controller\")\n        return\n\n    running = True\n    while running:\n        # Process pygame events\n        for event in pygame.event.get():\n            if event.type == pygame.QUIT:\n                running = False\n\n        # Read joystick values\n        left_x = controller.get_axis(0)\n        left_y = controller.get_axis(1)\n        right_x = controller.get_axis(2)\n        right_y = controller.get_axis(3)\n\n        # Clear screen\n        screen.fill((0, 0, 0))\n\n        # Draw joystick positions\n        pygame.draw.circle(\n            screen, (50, 50, 50), (160, 240), 100\n        )  # Left stick background\n        pygame.draw.circle(\n            screen, (0, 255, 0), (160 + int(left_x * 80), 240 + int(left_y * 80)), 20\n        )  # Left stick position\n\n        pygame.draw.circle(\n            screen, (50, 50, 50), (480, 240), 100\n        )  # Right stick background\n        pygame.draw.circle(\n            screen, (0, 255, 0), (480 + int(right_x * 80), 240 + int(right_y * 80)), 20\n        )  # Right stick position\n\n        # Update display\n        pygame.display.flip()\n\n        # Control frame rate\n        clock.tick(60)\n\n    # Clean up\n    controller.quit()\n    pygame.quit()\n\nif __name__ == \"__main__\":\n    main(\"Gamesir-T1d-39BD\") # replace with the name of your T1d\nLook at that! Pretty responsive!"
  },
  {
    "objectID": "posts/gamesir-t1d-controller/index.html#whats-next",
    "href": "posts/gamesir-t1d-controller/index.html#whats-next",
    "title": "GameSir T1d controller & pygame",
    "section": "What’s next?",
    "text": "What’s next?\nNow that we have the T1d working with pygame we can implement the interface with the Tello. Read on…\n\n\n\n\n\n\n\nSeries: Code, Fly & AI\n\n\n\n← Previous: Key navigation & video snapshots\n\n\nNext: Tello controller navigation - Part 1 →"
  },
  {
    "objectID": "posts/mnist-basics/index.html",
    "href": "posts/mnist-basics/index.html",
    "title": "MNIST basics - Training a Digit Classifier",
    "section": "",
    "text": "Mark Pors\n\n2025-06-11"
  },
  {
    "objectID": "posts/mnist-basics/index.html#pixels-the-foundations-of-computer-vision",
    "href": "posts/mnist-basics/index.html#pixels-the-foundations-of-computer-vision",
    "title": "MNIST basics - Training a Digit Classifier",
    "section": "Pixels: The Foundations of Computer Vision",
    "text": "Pixels: The Foundations of Computer Vision\n\nfrom fastai.vision.all import *\nmatplotlib.rc('image', cmap='Greys')\n\nFor this initial tutorial we are just going to try to create a model that can classify any image as a 3 or a 7. So let’s download a sample of MNIST that contains images of just these digits:\n\npath = untar_data(URLs.MNIST_SAMPLE)\n\n\n\n\n\n\n    \n      \n      100.14% [3219456/3214948 00:01&lt;00:00]\n    \n    \n\n\n\n#hide\nPath.BASE_PATH = path\n\n\n(path/'train').ls()\n\n(#2) [Path('train/7'),Path('train/3')]\n\n\n\nthrees = (path/'train'/'3').ls().sorted()\nsevens = (path/'train'/'7').ls().sorted()\nthrees\n\n(#6131) [Path('train/3/10.png'),Path('train/3/10000.png'),Path('train/3/10011.png'),Path('train/3/10031.png'),Path('train/3/10034.png'),Path('train/3/10042.png'),Path('train/3/10052.png'),Path('train/3/1007.png'),Path('train/3/10074.png'),Path('train/3/10091.png')...]\n\n\n\nim3_path = threes[1]\nim3 = Image.open(im3_path)\nim3\n\n\n\n\n\n\n\n\n\narray(im3)[4:10,4:10]\n\narray([[  0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,  29],\n       [  0,   0,   0,  48, 166, 224],\n       [  0,  93, 244, 249, 253, 187],\n       [  0, 107, 253, 253, 230,  48],\n       [  0,   3,  20,  20,  15,   0]], dtype=uint8)\n\n\n\ntensor(im3)[4:10,4:10]\n\ntensor([[  0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,  29],\n        [  0,   0,   0,  48, 166, 224],\n        [  0,  93, 244, 249, 253, 187],\n        [  0, 107, 253, 253, 230,  48],\n        [  0,   3,  20,  20,  15,   0]], dtype=torch.uint8)\n\n\n\nim3_t = tensor(im3)\ndf = pd.DataFrame(im3_t[4:15,4:22])\ndf.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')\n\n\n\n\n\n\n \n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n29\n150\n195\n254\n255\n254\n176\n193\n150\n96\n0\n0\n0\n\n\n2\n0\n0\n0\n48\n166\n224\n253\n253\n234\n196\n253\n253\n253\n253\n233\n0\n0\n0\n\n\n3\n0\n93\n244\n249\n253\n187\n46\n10\n8\n4\n10\n194\n253\n253\n233\n0\n0\n0\n\n\n4\n0\n107\n253\n253\n230\n48\n0\n0\n0\n0\n0\n192\n253\n253\n156\n0\n0\n0\n\n\n5\n0\n3\n20\n20\n15\n0\n0\n0\n0\n0\n43\n224\n253\n245\n74\n0\n0\n0\n\n\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n249\n253\n245\n126\n0\n0\n0\n0\n\n\n7\n0\n0\n0\n0\n0\n0\n0\n14\n101\n223\n253\n248\n124\n0\n0\n0\n0\n0\n\n\n8\n0\n0\n0\n0\n0\n11\n166\n239\n253\n253\n253\n187\n30\n0\n0\n0\n0\n0\n\n\n9\n0\n0\n0\n0\n0\n16\n248\n250\n253\n253\n253\n253\n232\n213\n111\n2\n0\n0\n\n\n10\n0\n0\n0\n0\n0\n0\n0\n43\n98\n98\n208\n253\n253\n253\n253\n187\n22\n0"
  },
  {
    "objectID": "posts/mnist-basics/index.html#first-try-pixel-similarity",
    "href": "posts/mnist-basics/index.html#first-try-pixel-similarity",
    "title": "MNIST basics - Training a Digit Classifier",
    "section": "First Try: Pixel Similarity",
    "text": "First Try: Pixel Similarity\n\njargon: Baseline: A simple model which you are confident should perform reasonably well. It should be very simple to implement, and very easy to test, so that you can then test each of your improved ideas, and make sure they are always better than your baseline. Without starting with a sensible baseline, it is very difficult to know whether your super-fancy models are actually any good. One good approach to creating a baseline is doing what we have done here: think of a simple, easy-to-implement model. Another good approach is to search around to find other people that have solved similar problems to yours, and download and run their code on your dataset. Ideally, try both of these!\n\n&lt;mark&gt;I love this advice ^^^. I’ll try to follow it as much as possible.&lt;/mark&gt;\n\nseven_tensors = [tensor(Image.open(o)) for o in sevens]\nthree_tensors = [tensor(Image.open(o)) for o in threes]\nlen(three_tensors),len(seven_tensors)\n\n(6131, 6265)\n\n\n\nshow_image(three_tensors[1]);\n\n\n\n\n\n\n\n\n\nstacked_sevens = torch.stack(seven_tensors).float()/255\nstacked_threes = torch.stack(three_tensors).float()/255\nstacked_threes.shape\n\ntorch.Size([6131, 28, 28])\n\n\n\nlen(stacked_threes.shape)\n\n3\n\n\n\nstacked_threes.ndim\n\n3\n\n\n\nmean3 = stacked_threes.mean(0)\nshow_image(mean3);\n\n\n\n\n\n\n\n\n\nmean7 = stacked_sevens.mean(0)\nshow_image(mean7);\n\n\n\n\n\n\n\n\n\na_3 = stacked_threes[1]\nshow_image(a_3);\n\n\n\n\n\n\n\n\nThere are two main ways data scientists measure distance in this context:\n\nTake the mean of the absolute value of differences (absolute value is the function that replaces negative values with positive values). This is called the mean absolute difference or L1 norm\nTake the mean of the square of differences (which makes everything positive) and then take the square root (which undoes the squaring). This is called the root mean squared error (RMSE) or L2 norm.\n\n\ndist_3_abs = (a_3 - mean3).abs().mean()\ndist_3_sqr = ((a_3 - mean3)**2).mean().sqrt()\ndist_3_abs,dist_3_sqr\n\n(tensor(0.1114), tensor(0.2021))\n\n\n\ndist_7_abs = (a_3 - mean7).abs().mean()\ndist_7_sqr = ((a_3 - mean7)**2).mean().sqrt()\ndist_7_abs,dist_7_sqr\n\n(tensor(0.1586), tensor(0.3021))\n\n\nPyTorch already provides both of these as loss functions. You’ll find these inside torch.nn.functional, which the PyTorch team recommends importing as F (and is available by default under that name in fastai):\n\nF.l1_loss(a_3.float(),mean7), F.mse_loss(a_3,mean7).sqrt()\n\n(tensor(0.1586), tensor(0.3021))\n\n\n\nS: Intuitively, the difference between L1 norm and mean squared error (MSE) is that the latter will penalize bigger mistakes more heavily than the former (and be more lenient with small mistakes)."
  },
  {
    "objectID": "posts/mnist-basics/index.html#computing-metrics-using-broadcasting",
    "href": "posts/mnist-basics/index.html#computing-metrics-using-broadcasting",
    "title": "MNIST basics - Training a Digit Classifier",
    "section": "Computing Metrics Using Broadcasting",
    "text": "Computing Metrics Using Broadcasting\n\nvalid_3_tens = torch.stack([tensor(Image.open(o))\n                            for o in (path/'valid'/'3').ls()])\nvalid_3_tens = valid_3_tens.float()/255\nvalid_7_tens = torch.stack([tensor(Image.open(o))\n                            for o in (path/'valid'/'7').ls()])\nvalid_7_tens = valid_7_tens.float()/255\nvalid_3_tens.shape,valid_7_tens.shape\n\n(torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))\n\n\n\ndef mnist_distance(a,b): return (a-b).abs().mean((-1,-2))\nmnist_distance(a_3, mean3)\n\ntensor(0.1114)\n\n\n&lt;mark&gt;This tuple (-1,-2) as an argument for the mean() method is not directly clear to me, so let’s have a look at the docs:&lt;/mark&gt;\n\n?torch.mean\n\n\nDocstring:\n\nmean(input, *, dtype=None) -&gt; Tensor\n\n\n\nReturns the mean value of all elements in the :attr:`input` tensor.\n\n\n\nArgs:\n\n    input (Tensor): the input tensor.\n\n\n\nKeyword args:\n\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n\n        If specified, the input tensor is casted to :attr:`dtype` before the operation\n\n        is performed. This is useful for preventing data type overflows. Default: None.\n\n\n\nExample::\n\n\n\n    &gt;&gt;&gt; a = torch.randn(1, 3)\n\n    &gt;&gt;&gt; a\n\n    tensor([[ 0.2294, -0.5481,  1.3288]])\n\n    &gt;&gt;&gt; torch.mean(a)\n\n    tensor(0.3367)\n\n\n\n.. function:: mean(input, dim, keepdim=False, *, dtype=None, out=None) -&gt; Tensor\n\n   :noindex:\n\n\n\nReturns the mean value of each row of the :attr:`input` tensor in the given\n\ndimension :attr:`dim`. If :attr:`dim` is a list of dimensions,\n\nreduce over all of them.\n\n\n\n\n\nIf :attr:`keepdim` is ``True``, the output tensor is of the same size\n\nas :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1.\n\nOtherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the\n\noutput tensor having 1 (or ``len(dim)``) fewer dimension(s).\n\n\n\n\n\nArgs:\n\n    input (Tensor): the input tensor.\n\n    dim (int or tuple of ints): the dimension or dimensions to reduce.\n\n    keepdim (bool): whether the output tensor has :attr:`dim` retained or not.\n\n\n\nKeyword args:\n\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n\n        If specified, the input tensor is casted to :attr:`dtype` before the operation\n\n        is performed. This is useful for preventing data type overflows. Default: None.\n\n    out (Tensor, optional): the output tensor.\n\n\n\n.. seealso::\n\n\n\n    :func:`torch.nanmean` computes the mean value of `non-NaN` elements.\n\n\n\nExample::\n\n\n\n    &gt;&gt;&gt; a = torch.randn(4, 4)\n\n    &gt;&gt;&gt; a\n\n    tensor([[-0.3841,  0.6320,  0.4254, -0.7384],\n\n            [-0.9644,  1.0131, -0.6549, -1.4279],\n\n            [-0.2951, -1.3350, -0.7694,  0.5600],\n\n            [ 1.0842, -0.9580,  0.3623,  0.2343]])\n\n    &gt;&gt;&gt; torch.mean(a, 1)\n\n    tensor([-0.0163, -0.5085, -0.4599,  0.1807])\n\n    &gt;&gt;&gt; torch.mean(a, 1, True)\n\n    tensor([[-0.0163],\n\n            [-0.5085],\n\n            [-0.4599],\n\n            [ 0.1807]])\n\nType:      builtin_function_or_method\n\n\n\n&lt;mark&gt;So the tuple represents a list of dimensions to reduce. In our case the last and second last dimension, which are the dimensions that contain the image pixel values (and the only two dimensions). In other words, this method calculates the average over all elements in the 24x24 matrix.&lt;/mark&gt;\n\n(a_3-mean3).shape\n\ntorch.Size([28, 28])\n\n\n\nsm = (a_3-mean3).abs().sum()\navg = sm / (a_3.shape[0] * a_3.shape[1])\navg\n\ntensor(0.1114)\n\n\n&lt;mark&gt;Same result!&lt;/mark&gt;\n\nvalid_3_dist = mnist_distance(valid_3_tens, mean3)\nvalid_3_dist, valid_3_dist.shape\n\n(tensor([0.1634, 0.1145, 0.1363,  ..., 0.1105, 0.1111, 0.1640]),\n torch.Size([1010]))\n\n\n&lt;mark&gt;So here we did the same, but as the first argument we pass a whole stack of images from the validation set. What happened? This works because of a feature called broadcasting. Let’s dive into that:&lt;/mark&gt;\n\nprint(f\"valid_3_tens.shape: {valid_3_tens.shape}\")\nprint(f\"mean3.shape: {mean3.shape}\")\ndiff = (valid_3_tens-mean3)\ndiff.shape\n\nvalid_3_tens.shape: torch.Size([1010, 28, 28])\nmean3.shape: torch.Size([28, 28])\n\n\ntorch.Size([1010, 28, 28])\n\n\n&lt;mark&gt;valid_3_tens has shape [1010, 28, 28], and mean3 has shape [28, 28]. When we subtract them we don’t get an error, but a tensor with shape [1010, 28, 28].&lt;/mark&gt;\n&lt;mark&gt;So the broadcasting feature automatically expands the shape of mean3 to [1010, 28, 28] so that it can be subtracted from valid_3_tens. There is a couple of conditions that need to be met for this to made possible. Time to experiment a bit with the help of Gemini:&lt;/mark&gt;\n\nBroadcasting is PyTorch’s way of making operations work between tensors of different but compatible shapes. Instead of you writing explicit loops or manually reshaping/copying data, PyTorch does it conceptually (and efficiently in C++/CUDA).\nThe Core Idea: Pretend the smaller tensor is expanded (like stretching or copying) to match the shape of the larger tensor, then perform the operation elementwise. Crucially, no actual memory duplication happens.\nRules for Compatibility: PyTorch compares the shapes of two tensors elementwise, starting from the trailing dimensions (the rightmost ones) and working backward. Two dimensions are compatible if:\n\nThey are equal.\nOne of them is 1.\n\nIf these conditions aren’t met for any dimension pair, the tensors are not broadcast-compatible, and you’ll get an error. If one tensor runs out of dimensions during the comparison (e.g., comparing a 3D tensor to a 2D tensor), dimensions of size 1 are assumed at the beginning of its shape.\n\n&lt;mark&gt;Now that we know that, we can see that in our example, the two rightmost dimensions have the same size [28, 28], and then the mean3 tensor runs out of dimensions, so it is turned into [1, 28, 28], which makes it compatible for broadcasting. Subsequently the mean3 tensor is expanded into [1010, 28, 28], by repeating the same data 1010 times, so we can subtract the two tensors.&lt;/mark&gt;\n&lt;mark&gt;Let’s play a bit with that…&lt;/mark&gt;\n\n# Create a tensor A with shape (5, 1, 4)\nA = torch.zeros(5, 1, 4)\n# Create a tensor B with shape (5, 3, 1)\nB = torch.ones(5, 3, 1)\n# Create a tensor C with shape (3, 4)\nC = torch.rand(3, 4)\n\n# Can we add A and B?\ntry:\n    result = A + B\n    print(\"A + B result shape:\", result.shape)\nexcept RuntimeError as e:\n    print(f\"Error adding A and B: {e}\")\n# Can we add A and C?\ntry:\n    result = A + C\n    print(\"A + C result shape:\", result.shape)\nexcept RuntimeError as e:\n    print(f\"Error adding A and C: {e}\")\n# Can we add B and C?\ntry:\n    result = B + C\n    print(\"B + C result shape:\", result.shape)\nexcept RuntimeError as e:\n    print(f\"Error adding B and C: {e}\")\n    \n    \n\nA + B result shape: torch.Size([5, 3, 4])\nA + C result shape: torch.Size([5, 3, 4])\nB + C result shape: torch.Size([5, 3, 4])\n\n\n&lt;mark&gt;I think I’ve got that down now!&lt;/mark&gt;\n\ndef is_3(x): return mnist_distance(x,mean3) &lt; mnist_distance(x,mean7)\n\n\nis_3(a_3), is_3(a_3).float()\n\n(tensor(True), tensor(1.))\n\n\n\nis_3(valid_3_tens)\n\ntensor([True, True, True,  ..., True, True, True])\n\n\n\naccuracy_3s =      is_3(valid_3_tens).float() .mean()\naccuracy_7s = (1 - is_3(valid_7_tens).float()).mean()\n\naccuracy_3s,accuracy_7s,(accuracy_3s+accuracy_7s)/2\n\n(tensor(0.9168), tensor(0.9854), tensor(0.9511))"
  },
  {
    "objectID": "posts/mnist-basics/index.html#stochastic-gradient-descent-sgd",
    "href": "posts/mnist-basics/index.html#stochastic-gradient-descent-sgd",
    "title": "MNIST basics - Training a Digit Classifier",
    "section": "Stochastic Gradient Descent (SGD)",
    "text": "Stochastic Gradient Descent (SGD)\nTo be more specific, here are the steps that we are going to require, to turn this function into a machine learning classifier:\n\nInitialize the weights.\nFor each image, use these weights to predict whether it appears to be a 3 or a 7.\nBased on these predictions, calculate how good the model is (its loss).\nCalculate the gradient, which measures for each weight, how changing that weight would change the loss\nStep (that is, change) all the weights based on that calculation.\nGo back to the step 2, and repeat the process.\nIterate until you decide to stop the training process (for instance, because the model is good enough or you don’t want to wait any longer).\n\nThese seven steps, illustrated below, are the key to the training of all deep learning models. That deep learning turns out to rely entirely on these steps is extremely surprising and counterintuitive. It’s amazing that this process can solve such complex problems. But, as you’ll see, it really does!\n\n\n\nGradient descent\n\n\nThere are many different ways to do each of these seven steps, and we will be learning about them throughout the rest of this book. These are the details that make a big difference for deep learning practitioners, but it turns out that the general approach to each one generally follows some basic principles. Here are a few guidelines:\n\nInitialize:: We initialize the parameters to random values. This may sound surprising. There are certainly other choices we could make, such as initializing them to the percentage of times that pixel is activated for that category—but since we already know that we have a routine to improve these weights, it turns out that just starting with random weights works perfectly well.\nLoss:: This is what Samuel referred to when he spoke of testing the effectiveness of any current weight assignment in terms of actual performance. We need some function that will return a number that is small if the performance of the model is good (the standard approach is to treat a small loss as good, and a large loss as bad, although this is just a convention).\nStep:: A simple way to figure out whether a weight should be increased a bit, or decreased a bit, would be just to try it: increase the weight by a small amount, and see if the loss goes up or down. Once you find the correct direction, you could then change that amount by a bit more, and a bit less, until you find an amount that works well. However, this is slow! As we will see, the magic of calculus allows us to directly figure out in which direction, and by roughly how much, to change each weight, without having to try all these small changes. The way to do this is by calculating gradients. This is just a performance optimization, we would get exactly the same results by using the slower manual process as well.\nStop:: Once we’ve decided how many epochs to train the model for (a few suggestions for this were given in the earlier list), we apply that decision. This is where that decision is applied. For our digit classifier, we would keep training until the accuracy of the model started getting worse, or we ran out of time.\n\n\ndef f(x): return x**2\n\n\nplot_function(f, 'x', 'x**2')\n\n\n\n\n\n\n\n\n\nplot_function(f, 'x', 'x**2')\nplt.scatter(-1.5, f(-1.5), color='red');\n\n\n\n\n\n\n\n\n\n\n\nCalculating Gradients\nOne important thing to be aware of is that our function has lots of weights that we need to adjust, so when we calculate the derivative we won’t get back one number, but lots of them—a gradient for every weight. But there is nothing mathematically tricky here; you can calculate the derivative with respect to one weight, and treat all the other ones as constant, then repeat that for each other weight. This is how all of the gradients are calculated, for every weight.\nFirst, let’s pick a tensor value which we want gradients at:\n\nxt = tensor(3.).requires_grad_()\n\n\nyt = f(xt)\nyt\n\ntensor(9., grad_fn=&lt;PowBackward0&gt;)\n\n\n\nyt.backward()\n\n\nxt.grad\n\ntensor(6.)\n\n\n\nxt = tensor([3.,4.,10.]).requires_grad_()\nxt\n\ntensor([ 3.,  4., 10.], requires_grad=True)\n\n\n\ndef f(x): return (x**2).sum()\n\nyt = f(xt)\nyt\n\ntensor(125., grad_fn=&lt;SumBackward0&gt;)\n\n\n\nyt.backward()\nxt.grad\n\ntensor([ 6.,  8., 20.])\n\n\n\n\nStepping With a Learning Rate\nw -= gradient(w) * lr\n\n\nAn End-to-End SGD Example\n\ntime = torch.arange(0,20).float(); time\n\ntensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n        14., 15., 16., 17., 18., 19.])\n\n\n\nspeed = torch.randn(20)*3 + 0.75*(time-9.5)**2 + 1\nplt.scatter(time,speed);\n\n\n\n\n\n\n\n\n\ndef f(t, params):\n    a,b,c = params\n    return a*(t**2) + (b*t) + c\n\n\ndef mse(preds, targets): return ((preds-targets)**2).mean()\n\n\nStep 1: Initialize the parameters\n\nparams = torch.randn(3).requires_grad_()\n\n\n\nStep 2: Calculate the predictions\n\npreds = f(time, params)\n\n\ndef show_preds(preds, ax=None):\n    if ax is None: ax=plt.subplots()[1]\n    ax.scatter(time, speed)\n    ax.scatter(time, to_np(preds), color='red')\n    ax.set_ylim(-300,100)\n\n\nshow_preds(preds)\n\n\n\n\n\n\n\n\n\n\nStep 3: Calculate the loss\n\nloss = mse(preds, speed)\nloss\n\ntensor(82827.6641, grad_fn=&lt;MeanBackward0&gt;)\n\n\n\n\nStep 4: Calculate the gradients\n\nloss.backward()\nparams.grad\n\ntensor([-96139.7500,  -6190.7744,   -445.6926])\n\n\n\nparams.grad * 1e-5\n\ntensor([-0.9614, -0.0619, -0.0045])\n\n\n\nparams\n\ntensor([-1.4608, -1.6226, -1.2073], requires_grad=True)\n\n\n\n\nStep 5: Step the weights.\n\nlr = 1e-5\nparams.data -= lr * params.grad.data\nparams.grad = None\n\n\npreds = f(time,params)\nmse(preds, speed)\n\ntensor(16233.4893, grad_fn=&lt;MeanBackward0&gt;)\n\n\nAnd take a look at the plot:\n\nshow_preds(preds)\n\n\n\n\n\n\n\n\n\ndef apply_step(params, prn=True):\n    preds = f(time, params)\n    loss = mse(preds, speed)\n    loss.backward()\n    params.data -= lr * params.grad.data\n    params.grad = None\n    if prn: print(loss.item())\n    return preds\n\n&lt;mark&gt;Remember from my previous post that we had to do this: with torch.no_grad(): abc -= abc.grad*0.01, to make sure PyTorch won’t track this operation for calculating the gradient? So, why not here? Because it sets the result directly on the data property of params. That apparently prevents the gradient function tracker to ignore this operation.&lt;/mark&gt;\n\n\nStep 6: Repeat the process\n\nfor i in range(10): apply_step(params)\n\n16233.4892578125\n3631.822265625\n1247.1947021484375\n795.9425659179688\n710.5438842773438\n694.3759765625\n691.3086547851562\n690.7203369140625\n690.6011352539062\n690.5706787109375\n\n\n\n_,axs = plt.subplots(1,4,figsize=(12,3))\nfor ax in axs: show_preds(apply_step(params, False), ax)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\nStep 7: stop\n\n\n\nSummarizing Gradient Descent"
  },
  {
    "objectID": "posts/mnist-basics/index.html#the-mnist-loss-function",
    "href": "posts/mnist-basics/index.html#the-mnist-loss-function",
    "title": "MNIST basics - Training a Digit Classifier",
    "section": "The MNIST Loss Function",
    "text": "The MNIST Loss Function\n\nstacked_sevens.shape, stacked_threes.shape\n\n(torch.Size([6265, 28, 28]), torch.Size([6131, 28, 28]))\n\n\n\ntrain_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)\ntrain_x.shape\n\ntorch.Size([12396, 784])\n\n\n&lt;mark&gt;OK, so each image is now a vector of all 28*28 pixels, and dimension 0 basically points to each single image.&lt;/mark&gt;\n\ntrain_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)\ntrain_x.shape,train_y.shape\n\n(torch.Size([12396, 784]), torch.Size([12396, 1]))\n\n\n&lt;mark&gt;So, [1]*len(threes) + [0]*len(sevens) simply creates a vector with all the “labels” (dependent variables). Now what is this unsqueeze(1) doing? We have a row of labels, but if we want to match them up with train_x we need a column. That is exactly what unsqueeze(1) does. Let’s play a little bit with that:&lt;/mark&gt;\n\nlabels_list = tensor([1]*10 + [0]*10)\nprint(f\"shape: {labels_list.shape}\")\nlabels_list\n\nshape: torch.Size([20])\n\n\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\n\nlabels_list_unsqueezed = labels_list.unsqueeze(1)\nprint(f\"shape: {labels_list_unsqueezed.shape}\")\nlabels_list_unsqueezed\n\nshape: torch.Size([20, 1])\n\n\ntensor([[1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0]])\n\n\n\nlabels_list_unsqueezed0 = labels_list.unsqueeze(0)\nprint(f\"shape: {labels_list_unsqueezed0.shape}\")\nlabels_list_unsqueezed0\n\nshape: torch.Size([1, 20])\n\n\ntensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n\n\n\nlabels_list_unsqueezed0 = labels_list[:, None]\nprint(f\"shape: {labels_list_unsqueezed0.shape}\")\nlabels_list_unsqueezed0\n\nshape: torch.Size([20, 1])\n\n\ntensor([[1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0]])\n\n\n&lt;mark&gt;Ah, look at that! It is the same as the [:, None] trick we saw in the previous post!&lt;/mark&gt;\nA Dataset in PyTorch is required to return a tuple of (x,y) when indexed. Python provides a zip function which, when combined with list, provides a simple way to get this functionality:\n\ndset = list(zip(train_x,train_y))\nx,y = dset[0]\nx.shape,y\n\n(torch.Size([784]), tensor([1]))\n\n\n&lt;mark&gt;So we now have a nice list of tuples of the form: (image vector, label)&lt;/mark&gt;\n\nvalid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28)\nvalid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1)\nvalid_dset = list(zip(valid_x,valid_y))\n\n\ndef init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\n\n\nweights = init_params((28*28,1))\n\n\nbias = init_params(1)\n\n\njargon: Parameters: The weights and biases of a model. The weights are the w in the equation w*x+b, and the biases are the b in that equation.\n\n\n(train_x[0]*weights.T).sum() + bias\n\ntensor([9.3344], grad_fn=&lt;AddBackward0&gt;)\n\n\n\n\ndef linear1(xb): return xb@weights + bias\npreds = linear1(train_x)\npreds\n\ntensor([[ 9.3344],\n        [ 0.8027],\n        [ 7.0013],\n        ...,\n        [-1.8317],\n        [ 3.5838],\n        [-4.9083]], grad_fn=&lt;AddBackward0&gt;)\n\n\n\ncorrects = (preds&gt;0.0).float() == train_y\ncorrects\n\ntensor([[ True],\n        [ True],\n        [ True],\n        ...,\n        [ True],\n        [False],\n        [ True]])\n\n\n\ncorrects.float().mean().item()\n\n0.6630364656448364\n\n\n\nwith torch.no_grad(): weights[0] *= 1.0001\n\n\npreds = linear1(train_x)\n((preds&gt;0.0).float() == train_y).float().mean().item()\n\n0.6630364656448364\n\n\n\ntrgts  = tensor([1,0,1])\nprds   = tensor([0.9, 0.4, 0.2])\n\n\ndef mnist_loss(predictions, targets):\n    return torch.where(targets==1, 1-predictions, predictions).mean()\n\n\ntorch.where(trgts==1, 1-prds, prds)\n\ntensor([0.1000, 0.4000, 0.8000])\n\n\n\nmnist_loss(prds,trgts)\n\ntensor(0.4333)\n\n\n\nmnist_loss(tensor([0.9, 0.4, 0.8]),trgts)\n\ntensor(0.2333)\n\n\n\nSigmoid\n\ndef sigmoid(x): return 1/(1+torch.exp(-x))\n\n\nplot_function(torch.sigmoid, title='Sigmoid', min=-4, max=4)\n\n\n\n\n\n\n\n\n\ndef mnist_loss(predictions, targets):\n    predictions = predictions.sigmoid()\n    return torch.where(targets==1, 1-predictions, predictions).mean()\n\n\n\nSGD and Mini-Batches\n\ncoll = range(15)\ndl = DataLoader(coll, batch_size=5, shuffle=True)\nlist(dl)\n\n[tensor([11,  4,  8,  9, 10]),\n tensor([13,  1,  5,  0, 14]),\n tensor([ 3, 12,  7,  2,  6])]\n\n\n\nds = L(enumerate(string.ascii_lowercase))\nds\n\n(#26) [(0, 'a'),(1, 'b'),(2, 'c'),(3, 'd'),(4, 'e'),(5, 'f'),(6, 'g'),(7, 'h'),(8, 'i'),(9, 'j')...]\n\n\n\ndl = DataLoader(ds, batch_size=6, shuffle=True)\nlist(dl)\n\n[(tensor([17, 22, 12, 20,  9,  6]), ('r', 'w', 'm', 'u', 'j', 'g')),\n (tensor([21,  7, 14, 13,  0, 24]), ('v', 'h', 'o', 'n', 'a', 'y')),\n (tensor([ 3,  8,  4, 16, 19,  5]), ('d', 'i', 'e', 'q', 't', 'f')),\n (tensor([18, 11, 25, 23,  2,  1]), ('s', 'l', 'z', 'x', 'c', 'b')),\n (tensor([10, 15]), ('k', 'p'))]"
  },
  {
    "objectID": "posts/mnist-basics/index.html#putting-it-all-together",
    "href": "posts/mnist-basics/index.html#putting-it-all-together",
    "title": "MNIST basics - Training a Digit Classifier",
    "section": "Putting It All Together",
    "text": "Putting It All Together\n\nweights = init_params((28*28,1))\nbias = init_params(1)\n\n\ndl = DataLoader(dset, batch_size=256)\nxb,yb = first(dl)\nxb.shape,yb.shape\n\n(torch.Size([256, 784]), torch.Size([256, 1]))\n\n\n\nvalid_dl = DataLoader(valid_dset, batch_size=256)\n\n\nbatch = train_x[:4]\nbatch.shape\n\ntorch.Size([4, 784])\n\n\n\npreds = linear1(batch)\npreds\n\ntensor([[0.7288],\n        [2.3106],\n        [2.0486],\n        [1.4359]], grad_fn=&lt;AddBackward0&gt;)\n\n\n\nloss = mnist_loss(preds, train_y[:4])\nloss\n\ntensor(0.1805, grad_fn=&lt;MeanBackward0&gt;)\n\n\n\nloss.backward()\nweights.grad.shape,weights.grad.mean(),bias.grad\n\n(torch.Size([784, 1]), tensor(-0.0212), tensor([-0.1395]))\n\n\n\ndef calc_grad(xb, yb, model):\n    preds = model(xb)\n    loss = mnist_loss(preds, yb)\n    loss.backward()\n\n\ncalc_grad(batch, train_y[:4], linear1)\nweights.grad.mean(),bias.grad\n\n(tensor(-0.0424), tensor([-0.2790]))\n\n\n\ncalc_grad(batch, train_y[:4], linear1)\nweights.grad.mean(),bias.grad\n\n(tensor(-0.0636), tensor([-0.4185]))\n\n\n\nweights.grad.zero_()\nbias.grad.zero_();\n\n\ndef train_epoch(model, lr, params):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        for p in params:\n            p.data -= p.grad*lr\n            p.grad.zero_()\n\n\n(preds&gt;0.0).float() == train_y[:4]\n\ntensor([[True],\n        [True],\n        [True],\n        [True]])\n\n\n\ndef batch_accuracy(xb, yb):\n    preds = xb.sigmoid()\n    correct = (preds&gt;0.5) == yb\n    return correct.float().mean()\n\n\nbatch_accuracy(linear1(batch), train_y[:4])\n\ntensor(1.)\n\n\n\ndef validate_epoch(model):\n    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n    return round(torch.stack(accs).mean().item(), 4)\n\n\nvalidate_epoch(linear1)\n\n0.7649\n\n\n\nlr = 1.\nparams = weights,bias\ntrain_epoch(linear1, lr, params)\nvalidate_epoch(linear1)\n\n0.8446\n\n\n\nfor i in range(20):\n    train_epoch(linear1, lr, params)\n    print(validate_epoch(linear1), end=' ')\n\n0.9144 0.9369 0.9462 0.9545 0.9589 0.9603 0.9628 0.9638 0.9638 0.9643 0.9657 0.9652 0.9657 0.9677 0.9681 0.9681 0.9696 0.9696 0.9706 0.972 \n\n\n\nCreating an Optimizer\n\nlinear_model = nn.Linear(28*28,1)\n\n\nw,b = linear_model.parameters()\nw.shape,b.shape\n\n(torch.Size([1, 784]), torch.Size([1]))\n\n\n\nclass BasicOptim:\n    def __init__(self,params,lr): self.params,self.lr = list(params),lr\n\n    def step(self, *args, **kwargs):\n        for p in self.params: p.data -= p.grad.data * self.lr\n\n    def zero_grad(self, *args, **kwargs):\n        for p in self.params: p.grad = None\n\n\nopt = BasicOptim(linear_model.parameters(), lr)\n\n\ndef train_epoch(model):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        opt.step()\n        opt.zero_grad()\n\n\nvalidate_epoch(linear_model)\n\n0.6292\n\n\n\ndef train_model(model, epochs):\n    for i in range(epochs):\n        train_epoch(model)\n        print(validate_epoch(model), end=' ')\n\n\ntrain_model(linear_model, 20)\n\n0.4932 0.8052 0.8525 0.917 0.9346 0.9487 0.957 0.9624 0.9658 0.9678 0.9692 0.9717 0.9736 0.9746 0.9761 0.9765 0.9775 0.978 0.978 0.979 \n\n\n\nlinear_model = nn.Linear(28*28,1)\nopt = SGD(linear_model.parameters(), lr)\ntrain_model(linear_model, 20)\n\n0.4932 0.7759 0.8603 0.9179 0.9346 0.9516 0.957 0.9634 0.9658 0.9673 0.9697 0.9717 0.9746 0.9751 0.9761 0.977 0.9775 0.978 0.978 0.9785 \n\n\n\ndls = DataLoaders(dl, valid_dl)\n\n\nlearn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\n\n\nlearn.fit(10, lr=lr)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nbatch_accuracy\ntime\n\n\n\n\n0\n0.636803\n0.503571\n0.495584\n00:00\n\n\n1\n0.569794\n0.217214\n0.807655\n00:00\n\n\n2\n0.208501\n0.165452\n0.851816\n00:00\n\n\n3\n0.090031\n0.101155\n0.914622\n00:00\n\n\n4\n0.046536\n0.074791\n0.935231\n00:00\n\n\n5\n0.029671\n0.060303\n0.948970\n00:00\n\n\n6\n0.022822\n0.051260\n0.956330\n00:00\n\n\n7\n0.019821\n0.045237\n0.962218\n00:00\n\n\n8\n0.018322\n0.040985\n0.965653\n00:00\n\n\n9\n0.017435\n0.037833\n0.968106\n00:00"
  },
  {
    "objectID": "posts/mnist-basics/index.html#adding-a-nonlinearity",
    "href": "posts/mnist-basics/index.html#adding-a-nonlinearity",
    "title": "MNIST basics - Training a Digit Classifier",
    "section": "Adding a Nonlinearity",
    "text": "Adding a Nonlinearity\n\ndef simple_net(xb):\n    res = xb@w1 + b1\n    res = res.max(tensor(0.0))\n    res = res@w2 + b2\n    return res\n\n\nw1 = init_params((28*28,30))\nb1 = init_params(30)\nw2 = init_params((30,1))\nb2 = init_params(1)\n\nThe key point about this is that w1 has 30 output activations (which means that w2 must have 30 input activations, so they match). That means that the first layer can construct 30 different features, each representing some different mix of pixels. You can change that 30 to anything you like, to make the model more or less complex.\n&lt;mark&gt;So, the w1 matrix has 30 rows and 784 columns, which means that it can take a vector of 784 pixel values as input (the image we want to classify), and return a vector of 30 values. The w2 matrix has 1 row and 30 columns, which means that it can take a vector of 30 values as input, and return a scalar (which is the probability).&lt;/mark&gt;\n&lt;mark&gt;These 30 output activations are synonym to a layer of 30 nodes. Each of these 30 nodes has it’s own set of 784 weights that we try to optimize. So more nodes means more possible feature detectors or pattern recognizers that our model can learn. We can think of each of those 30 nodes as becoming specialized in detecting different aspects of what makes a digit look like a “3” versus a “7”.&lt;/mark&gt;\n&lt;mark&gt;I like how Claude said it while explaining this to me:&lt;/mark&gt;\n\nHere’s a helpful way to think about it: imagine you’re trying to distinguish between cats and dogs, and you have 30 friends helping you. Each friend specializes in looking for different features - one focuses on ear shape, another on tail length, another on fur texture, and so on. Each friend gives you their opinion (a number between, say, -10 and +10), and then you have to weigh all 30 opinions to make your final decision. That’s essentially what your two-layer network is doing.\nThe beauty is that during training, the network automatically figures out what each of those 30 “friends” should specialize in looking for. You don’t have to tell it “node 1, you focus on curves” - it discovers these useful features on its own through the optimization process.\n\n&lt;mark&gt;I was then wondering: what if some or all nodes try to specialize in the same feature? There are two reasons why this is not likely:\n1. We initialize the weights randomly, so each node starts with a different perspective.\n2. The optimization process will adjust the weights in such a way that it minimizes the loss function, which means that if two nodes try to learn the same feature, one of them will end up being less effective and will be encouraged to learn something else.&lt;/mark&gt;\n&lt;mark&gt;I still have to think hard to get the matrix juggling down, so let’s create a super-tiny net:&lt;/mark&gt;\n\n# Our tiny network layer\nlayer = nn.Linear(4, 3)  # 4 inputs (2x2 image flattened), 3 outputs\n\n# Sample 2x2 image\nimage = tensor([[1.0, 2.0],\n                [3.0, 4.0]])\n\n# Flatten the image to a column vector\nx = image.flatten()  # x = [1.0, 2.0, 3.0, 4.0] with shape [4]\n\n# The weights and biases are already initialized by nn.Linear\nw = layer.weight  # shape [3, 4]\nb = layer.bias      # shape [3]\n\nprint(f\"Weights {w.shape}: {w.data}, Bias {b.shape}: {b.data}\")\n\n# Matrix multiplication: w @ x (shape [3,4] @ [4] = [3])\noutput = w @ x + b\nprint(f\"Output shape: {output.shape}, Output: {output.data}\")\n\n# Now apply ReLU activation\noutput2 = F.relu(output)\nprint(f\"Output after ReLU shape: {output2.shape}, Output: {output2.data}\")\n\n# Last layer\nlayer2 = nn.Linear(3, 1)  # 3 inputs, 1 output\n# Weights and biases for the last layer\nw2 = layer2.weight  # shape [1, 3]\nb2 = layer2.bias      # shape [1]\n# Final output\nfinal_output = w2 @ output2 + b2\nprint(f\"Final output shape: {final_output.shape}, Final output: {final_output.data}\")\n# Final output after sigmoid activation\nfinal_output_sigmoid = torch.sigmoid(final_output)\nprint(f\"Final output after sigmoid shape: {final_output_sigmoid.shape}, Final output: {final_output_sigmoid.data}\")\n\nWeights torch.Size([3, 4]): tensor([[ 0.0777,  0.2776,  0.4277,  0.0007],\n        [-0.4654, -0.0821,  0.1229,  0.2742],\n        [ 0.1707, -0.2651,  0.3052,  0.1164]]), Bias torch.Size([3]): tensor([-0.1928, -0.2575,  0.3110])\nOutput shape: torch.Size([3]), Output: tensor([1.7258, 0.5783, 1.3328])\nOutput after ReLU shape: torch.Size([3]), Output: tensor([1.7258, 0.5783, 1.3328])\nFinal output shape: torch.Size([1]), Final output: tensor([-0.2682])\nFinal output after sigmoid shape: torch.Size([1]), Final output: tensor([0.4333])\n\n\n&lt;mark&gt;This tiny model is visualized below. The first layer has 3 nodes and the second linear layer has 1 node.&lt;/mark&gt;\n\n\n\nSuper-tiny net. Made with NN-SVG\n\n\n\nplot_function(F.relu)\n\n\n\n\n\n\n\n\n\nsimple_net = nn.Sequential(\n    nn.Linear(28*28,30),\n    nn.ReLU(),\n    nn.Linear(30,1)\n)\n\n\nlearn = Learner(dls, simple_net, opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\n\n\nlearn.fit(40, 0.1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nbatch_accuracy\ntime\n\n\n\n\n0\n0.325859\n0.411601\n0.504907\n00:00\n\n\n1\n0.150409\n0.233516\n0.798332\n00:00\n\n\n2\n0.082649\n0.116558\n0.913150\n00:00\n\n\n3\n0.054023\n0.078485\n0.940628\n00:00\n\n\n4\n0.040843\n0.061257\n0.954858\n00:00\n\n\n5\n0.034143\n0.051604\n0.963199\n00:00\n\n\n6\n0.030300\n0.045509\n0.965653\n00:00\n\n\n7\n0.027805\n0.041319\n0.967615\n00:00\n\n\n8\n0.026009\n0.038248\n0.968597\n00:00\n\n\n9\n0.024617\n0.035891\n0.969578\n00:00\n\n\n10\n0.023491\n0.034015\n0.972031\n00:00\n\n\n11\n0.022551\n0.032479\n0.973994\n00:00\n\n\n12\n0.021751\n0.031189\n0.974485\n00:00\n\n\n13\n0.021059\n0.030088\n0.975957\n00:00\n\n\n14\n0.020454\n0.029132\n0.975957\n00:00\n\n\n15\n0.019918\n0.028290\n0.975957\n00:00\n\n\n16\n0.019438\n0.027543\n0.976938\n00:00\n\n\n17\n0.019006\n0.026874\n0.977920\n00:00\n\n\n18\n0.018614\n0.026271\n0.978410\n00:00\n\n\n19\n0.018255\n0.025726\n0.978901\n00:00\n\n\n20\n0.017924\n0.025230\n0.979392\n00:00\n\n\n21\n0.017619\n0.024777\n0.979882\n00:00\n\n\n22\n0.017335\n0.024363\n0.980373\n00:00\n\n\n23\n0.017070\n0.023981\n0.980864\n00:00\n\n\n24\n0.016821\n0.023629\n0.981354\n00:00\n\n\n25\n0.016588\n0.023304\n0.981354\n00:00\n\n\n26\n0.016368\n0.023003\n0.981354\n00:00\n\n\n27\n0.016160\n0.022723\n0.981354\n00:00\n\n\n28\n0.015963\n0.022462\n0.981845\n00:00\n\n\n29\n0.015775\n0.022220\n0.981845\n00:00\n\n\n30\n0.015597\n0.021993\n0.982336\n00:00\n\n\n31\n0.015427\n0.021782\n0.982336\n00:00\n\n\n32\n0.015265\n0.021584\n0.982826\n00:00\n\n\n33\n0.015110\n0.021398\n0.982826\n00:00\n\n\n34\n0.014961\n0.021224\n0.982336\n00:00\n\n\n35\n0.014819\n0.021060\n0.981845\n00:00\n\n\n36\n0.014682\n0.020906\n0.981845\n00:00\n\n\n37\n0.014550\n0.020760\n0.981845\n00:00\n\n\n38\n0.014423\n0.020622\n0.981845\n00:00\n\n\n39\n0.014301\n0.020492\n0.981845\n00:00\n\n\n\n\n\n\nplt.plot(L(learn.recorder.values).itemgot(2));\n\n\n\n\n\n\n\n\n\nlearn.recorder.values[-1][2]\n\n0.981844961643219\n\n\n\nGoing Deeper\n\ndls = ImageDataLoaders.from_folder(path)\nlearn = vision_learner(dls, resnet18, pretrained=False,\n                    loss_func=F.cross_entropy, metrics=accuracy)\nlearn.fit_one_cycle(1, 0.1)\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.082135\n\n\n0.014342\n\n\n0.996565\n\n\n02:26"
  },
  {
    "objectID": "posts/mnist-basics/index.html#jargon-recap",
    "href": "posts/mnist-basics/index.html#jargon-recap",
    "title": "MNIST basics - Training a Digit Classifier",
    "section": "Jargon Recap",
    "text": "Jargon Recap\nCongratulations: you now know how to create and train a deep neural network from scratch! We’ve gone through quite a few steps to get to this point, but you might be surprised at how simple it really is.\nNow that we are at this point, it is a good opportunity to define, and review, some jargon and key concepts.\nA neural network contains a lot of numbers, but they are only of two types: numbers that are calculated, and the parameters that these numbers are calculated from. This gives us the two most important pieces of jargon to learn:\n\nActivations: Numbers that are calculated (both by linear and nonlinear layers)\nParameters: Numbers that are randomly initialized, and optimized (that is, the numbers that define the model)\n\nWe will often talk in this book about activations and parameters. Remember that they have very specific meanings. They are numbers. They are not abstract concepts, but they are actual specific numbers that are in your model. Part of becoming a good deep learning practitioner is getting used to the idea of actually looking at your activations and parameters, and plotting them and testing whether they are behaving correctly.\nOur activations and parameters are all contained in tensors. These are simply regularly shaped arrays—for example, a matrix. Matrices have rows and columns; we call these the axes or dimensions. The number of dimensions of a tensor is its rank. There are some special tensors:\n\nRank zero: scalar\nRank one: vector\nRank two: matrix\n\nA neural network contains a number of layers. Each layer is either linear or nonlinear. We generally alternate between these two kinds of layers in a neural network. Sometimes people refer to both a linear layer and its subsequent nonlinearity together as a single layer. Yes, this is confusing. Sometimes a nonlinearity is referred to as an activation function.\n\n\n\n\n\n\n\nTerm\nMeaning\n\n\n\n\nReLU\nFunction that returns 0 for negative numbers and doesn’t change positive numbers.\n\n\nMini-batch\nA small group of inputs and labels gathered together in two arrays. A gradient descent step is updated on this batch (rather than a whole epoch).\n\n\nForward pass\nApplying the model to some input and computing the predictions.\n\n\nLoss\nA value that represents how well (or badly) our model is doing.\n\n\nGradient\nThe derivative of the loss with respect to some parameter of the model.\n\n\nBackward pass\nComputing the gradients of the loss with respect to all model parameters.\n\n\nGradient descent\nTaking a step in the directions opposite to the gradients to make the model parameters a little bit better.\n\n\nLearning rate\nThe size of the step we take when applying SGD to update the parameters of the model.\n\n\n\n&lt;mark&gt;And that concludes lesson 3 of the course (and chapter 4 of the book) for me. I’ll move on with chapter 5 of the book instead of lesson 4 of the course. Let’s get all the computer vision stuff done before moving to NLP.&lt;/mark&gt;"
  },
  {
    "objectID": "posts/key-navigation-video-snapshots/index.html",
    "href": "posts/key-navigation-video-snapshots/index.html",
    "title": "Key navigation & Video snapshots",
    "section": "",
    "text": "Mark Pors\n\n2025-04-27"
  },
  {
    "objectID": "posts/key-navigation-video-snapshots/index.html#keyboard-navigation",
    "href": "posts/key-navigation-video-snapshots/index.html#keyboard-navigation",
    "title": "Key navigation & Video snapshots",
    "section": "Keyboard navigation",
    "text": "Keyboard navigation\nNow that we can control the TT with code, let’s extend that to a human in the ground-bound pilot seat. The most simple approach is to use specific keyboard strokes to map to drone navigation commands.\nKeyboard navigation is implemented as an example in the DJSTelloPy repo: manual-control-pygame.py. This is quite a cool piece of code. It uses pygame, which I never used before, so lets go through some of the interesting parts of the code.\n\nThe secret sauce: Event-based controls\nThe example uses pygame’s event system rather than simple polling, and this creates nice and responsive controls.\n\n# Set up a timer that triggers events at the specified frame rate\npygame.time.set_timer(pygame.USEREVENT + 1, 1000 // FPS)\n\n# In the main loop, process all pending events\nfor event in pygame.event.get():\n    if event.type == pygame.USEREVENT + 1:\n        self.update()\n    elif event.type == pygame.KEYDOWN:\n        self.keydown(event.key)\n    elif event.type == pygame.KEYUP:\n        self.keyup(event.key)\n\nInstead of constantly checking “is this key pressed?” in a loop, the code waits for pygame to tell it when keys are pressed or released. This means:\n\nno missed keypresses, even if they happen very quickly\nimmediate detection of key events\nclear separation between “key is pressed” and “key is released” logic\n\nPretty cool.\n\n\nSmooth flying with velocity controls\nAnother interesting bit from the example code is how smoothly it makes the drone fly. This is done through velocity-based control (not up, down, left etc. commands).\n# Initialize velocity variables\nself.for_back_velocity = 0   # Forward/backward\nself.left_right_velocity = 0  # Left/right\nself.up_down_velocity = 0     # Up/down\nself.yaw_velocity = 0         # Rotation\n\n# When UP arrow is pressed\ndef keydown(self, key):\n    if key == pygame.K_UP:\n        self.for_back_velocity = S  # Set to speed value (60)\n    # ...other keys...\n\n# When UP arrow is released\ndef keyup(self, key):\n    if key == pygame.K_UP or key == pygame.K_DOWN:\n        self.for_back_velocity = 0  # Stop movement\n    # ...other keys...\nInstead of sending a “move forward” command when we press UP, it sets a forward velocity that remains until we release the key. This creates a very natural movement, especially when:\n\nmoving diagonally (pressing UP and RIGHT simultaneously)\ntransitioning between movements (release UP while still holding RIGHT)\nmaking subtle adjustments to flight path\n\n\n\nThe Heartbeat: Why frame rate matters\nSomething that reminds me of my hardware/telco days: the pygame library work synchronously with fixed time intervals. This line sets the heartbeat:\npygame.time.set_timer(pygame.USEREVENT + 1, 1000 // FPS)\nBasically at each interval it listens for user events and handles them as needed. At 120 FPS, it generates an event every 8.3 milliseconds, triggering e.g. our update function:\ndef update(self):\n    if self.send_rc_control:\n        self.tello.send_rc_control(\n            self.left_right_velocity,\n            self.for_back_velocity,\n            self.up_down_velocity, \n            self.yaw_velocity\n        )\n\n\nWhy not send commands immediately?\nThe naive alternative is to have a while True: loop that reads key strokes and react to that instantly. What’s wrong with that?\n\nCommand Rate Control: Drones can get overwhelmed if we send too many commands too quickly\nCommand Combination: If we press multiple keys in one frame, they’re combined into a single efficient command\nSmooth Motion: Even, regular command timing creates more natural drone movement\n\nI tried the while True approach and it doesn’t work that great. There is no feel between pressing keys and the drone following up on that.\nSo, I learned something new: pygame, which I’m sure will come in handy soon (spoiler alert: after this I want to use the GameSire controller to fly the Tello. Guess which library is great to speak to gaming consoles!)."
  },
  {
    "objectID": "posts/key-navigation-video-snapshots/index.html#video-streaming-snapshots",
    "href": "posts/key-navigation-video-snapshots/index.html#video-streaming-snapshots",
    "title": "Key navigation & Video snapshots",
    "section": "Video streaming & snapshots",
    "text": "Video streaming & snapshots\nThere is no autonomous flying without vision, and now is the first time we can have a peek at streaming video and capturing it.\nThe same example from above contains the video streaming as well.\n\nVideo streaming\n\nSetting Up the Stream\nFirst, the code initializes the video stream:\n# Make sure streaming is off before we start\nself.tello.streamoff()\n# Then turn streaming on\nself.tello.streamon()\n\n# Get the object that will give us frames\nframe_read = self.tello.get_frame_read()\nThis pattern of turning streaming off then on again is a good practice to ensure we’re starting with a clean slate. I had it hang a couple of times before applying this trick.\n\n\nCapturing and processing frames\nIn the main loop, the code grabs frames from the drone and processes them:\n# Get the latest frame\nframe = frame_read.frame\n\n# Add battery information to the frame\ntext = \"Battery: {}%\".format(self.tello.get_battery())\ncv2.putText(frame, text, (5, 720 - 5),\n    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\nThis overlays the battery percentage in red text at the bottom left of the frame. A handy feature when we’re flying!\n\n\nFrame transformation\nThe next three lines are needed for displaying the frame correctly in pygame:\n# OpenCV uses BGR, pygame needs RGB\nframe = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n# Rotate the frame 90 degrees\nframe = np.rot90(frame)\n\n# Flip the frame upside down\nframe = np.flipud(frame)\nWhy all this transformation? Cameras often capture images in orientations or color formats that aren’t immediately displayable. OpenCV uses BGR color format while pygame expects RGB, and the Tello camera orientation needs adjusting to appear correctly on screen.\n\n\nDisplaying in pygame\nFinally, the frame is displayed in the pygame window:\n# Convert numpy array to a pygame surface\nframe = pygame.surfarray.make_surface(frame)\n\n# Draw the surface to the screen\nself.screen.blit(frame, (0, 0))\n\n# Update the display\npygame.display.update()\nThis process happens every frame (120 times per second with the default settings), creating a smooth video feed.\n\n\n\nVideo snapshots\nNot included in the example code is the ability to take a snapshot by pressing a key and saving the current frame to disk.\nSo let’s add it:\n\n1. First, add the necessary import\nAt the top of the file, make sure we have:\nimport os\n\n\n2. Create a directory to store images\nAdd this near the beginning of our __init__ method:\n# Create a directory to store images if it doesn't exist\nif not os.path.exists('tello_images'):\n    os.makedirs('tello_images')\n\n\n3. Add a key handler for taking snapshots\nIn the keyup method, add a case for a new key (I’ll use ‘p’ for “picture”):\ndef keyup(self, key):\n    \"\"\" Update velocities based on key released \"\"\"\n    if key == pygame.K_UP or key == pygame.K_DOWN:  # set zero forward/backward velocity\n        self.for_back_velocity = 0\n    elif key == pygame.K_LEFT or key == pygame.K_RIGHT:  # set zero left/right velocity\n        self.left_right_velocity = 0\n    elif key == pygame.K_w or key == pygame.K_s:  # set zero up/down velocity\n        self.up_down_velocity = 0\n    elif key == pygame.K_a or key == pygame.K_d:  # set zero yaw velocity\n        self.yaw_velocity = 0\n    elif key == pygame.K_t:  # takeoff\n        self.tello.takeoff()\n        self.send_rc_control = True\n    elif key == pygame.K_l:  # land\n        not self.tello.land()\n        self.send_rc_control = False\n    elif key == pygame.K_p:  # take a snapshot\n        self.take_snapshot()\n\n\n4. Add the snapshot method\nAdd this new method to our FrontEnd class:\ndef take_snapshot(self):\n    \"\"\"\n    Take a snapshot of the current frame and save it to disk\n    \"\"\"\n    # Get the current frame\n    frame = self.tello.get_frame_read().frame\n    \n    if frame is not None:\n        # Create a filename with timestamp\n        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n        filename = f\"tello_images/tello_snapshot_{timestamp}.jpg\"\n        \n        # Save the image - note we save the original frame before any transformations\n        cv2.imwrite(filename, frame)\n        \n        print(f\"Snapshot saved: {filename}\")\nThe resulting code can be found here.\n\n\n\nLet’s try it out!\nIt all works just fine, here are some low res snapshots I made:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNavigating with a keyboard is a disaster, so…."
  },
  {
    "objectID": "posts/key-navigation-video-snapshots/index.html#whats-next",
    "href": "posts/key-navigation-video-snapshots/index.html#whats-next",
    "title": "Key navigation & Video snapshots",
    "section": "What’s next?",
    "text": "What’s next?\nTo improve navigation we are going to hook up the GameSir T1d, read on...\n\n\n\n\n\n\n\nSeries: Code, Fly & AI\n\n\n\n← Previous: Crash the Tello\n\n\nNext: GameSir T1d & pygame →"
  },
  {
    "objectID": "posts/fly-drone-with-image-classification/index.html",
    "href": "posts/fly-drone-with-image-classification/index.html",
    "title": "Fly a drone with: Image classification",
    "section": "",
    "text": "Mark Pors\n\n2025-05-15"
  },
  {
    "objectID": "posts/fly-drone-with-image-classification/index.html#the-path-to-autonomous-flying",
    "href": "posts/fly-drone-with-image-classification/index.html#the-path-to-autonomous-flying",
    "title": "Fly a drone with: Image classification",
    "section": "The path to autonomous flying",
    "text": "The path to autonomous flying\nNow that we have the Tello / RoboMaster TT under control with a mobile app, a keyboard, and a game controller we can make the first steps to remove control all together.\nI’m sure there have been written many books about this subject and there is loads of scientific research:\n\n\n\nGoogle scholar search for “autonomous drones”\n\n\nOver 17k results since 2024; OMG! Good thing that LLM’s have read all that :)\nI’m not going to do anything with it for now, let’s just try out a couple of naive ideas and see were we end up."
  },
  {
    "objectID": "posts/fly-drone-with-image-classification/index.html#naive-approach-1-image-classification",
    "href": "posts/fly-drone-with-image-classification/index.html#naive-approach-1-image-classification",
    "title": "Fly a drone with: Image classification",
    "section": "Naive approach #1: Image classification",
    "text": "Naive approach #1: Image classification\nNo, of course image classification is not the solution to autonomous flying, but probably it is in there somewhere, and I am following the Fast.ai course which starts off with image classification.\nJeremy Howard is the man behind both the Fast.ai library and the course, and his approach is to create a simple baseline model first:\n\nBaseline: A simple model which you are confident should perform reasonably well. It should be very simple to implement, and very easy to test, so that you can then test each of your improved ideas, and make sure they are always better than your baseline. Without starting with a sensible baseline, it is very difficult to know whether your super-fancy models are actually any good.\n\nSo, that’s what I am going to do: fine-tune an image classifier with a limited amount of data and with a simple base model.\nThe testing ground for my drone has a couple of very obvious obstacles: lamps hanging from the ceiling. So this is going to be a lamp or no-lamp in the room image classifier."
  },
  {
    "objectID": "posts/fly-drone-with-image-classification/index.html#lamp-or-no-lamp-image-classifier",
    "href": "posts/fly-drone-with-image-classification/index.html#lamp-or-no-lamp-image-classifier",
    "title": "Fly a drone with: Image classification",
    "section": "Lamp or no-lamp image classifier 😞 → 😐 → 🙂 → 😊 → 😃 → 😁",
    "text": "Lamp or no-lamp image classifier 😞 → 😐 → 🙂 → 😊 → 😃 → 😁\n\nFirst attempt 😞\nMy first attempt can be seen here: Baseline model: Is there a lamp?.\nNot a great result (understatement), it over-fits from the start, and the reason is obvious: label noise (jargon for have data labelled wrong).\nHere are some wonderful samples from the training set:\n\n\n\nlabel noise\n\n\nIt is a bit hard to Google/DDG for interior without hanging lamps, so let’s try training the model another time after manual cleanup.\n\n\nCleanup of the image labels\nI said manual cleanup, but I’m going to cheat and use some advanced AI model (CLIP) to be able to train a very simple model.\nCode is written by chatgpt: Lamp Dataset Cleaner (CLIP-based).\nAnd the result is…. MEH! Not much better than what I started with.\nSo, back to manual cleaning. I’ll be back soon…\nOne hour later: DONE!\nThe result is a huge amount of images with a lamp, and very few without it. Reason being: if you use ceiling in a search term, lamps will show up very often. The query home interior wall  -lamp was quite a bit better, not as many lamps! :)\nMore manual cleaning to do…\nAnother hour later: DONE!\n\n\nSecond attempt 😐\nWith the cleaned up data I did another run, see: Baseline model: Is there a lamp? Second attempt.\nThe result I quite a bit better with a ~10% error rate, but there still is over-fitting starting at epoch 2. The 16 top losses look like this:\n\n\n\nTop losses: Prediction/Actual/Loss/Probability\n\n\nTo be fair, some of these images are pretty tough to classify IMO. Anyway, let’s see if we can do better.\n\n\nThird attempt 😐: Better resizing and Augmentation\nThere’s a couple of things we can try, here is what I did try:\nResizing with different methods:\n* item_tfms=[Resize(400, ResizeMethod.Pad, pad_mode='zeros')]\n* item_tfms=[Resize(400, ResizeMethod.Squish)]\n* item_tfms=[Resize(400)]\n* item_tfms=[RandomResizedCrop(400, min_scale=0.6)]\n* item_tfms=[RandomResizedCrop(400, min_scale=0.3)]\n* item_tfms=[RandomResizedCrop(400, min_scale=0.8)]\nAugmentation:\n* item_tfms=[Resize(400, ResizeMethod.Pad, pad_mode='zeros')],\n* batch_tfms=aug_transforms()\nThe runs can be seen here: Baseline model: Is there a lamp? Third attempt.\nSomething very annoying is that subsequent runs - without changing anything - have different results, which makes doing experiments impossible. I asked Gemini about it and it said:\n\nYou’ve done a good job providing a seed for reproducibility with seed_everything(42). However, even with careful seeding, variations in results can sometimes occur in deep learning training for a few reasons, especially within an environment like Colab:\n\nGPU Non-Determinism: While you’ve set torch.backends.cudnn.deterministic = True, not all operations on the GPU are guaranteed to be completely deterministic across different runs or hardware configurations. Some highly optimized kernels might involve non-deterministic algorithms for performance reasons.\nBackground Processes in Colab: Colab is a shared environment. While unlikely to cause significant variations, very minor background processes or resource allocation differences could theoretically have a minuscule impact on timing and execution flow, though this is generally not the primary cause of noticeable differences.\n\n\nHmm, not great. What is great, is that Gemini can see the notebook you are working with, so it can be very helpful when needed. Not this time though :)\nConclusion: nothing really helps. The error rate stays around 10%.\nProbably we got everything out of the data we have? Let’s try a couple more things.\n\n\nFourth attempt 🙂: smaller batch size\nSo far we had a mini-batch size of 32. Let’s have look what happens when we decrease that:\n\nNo more over-fitting with 3 epochs.\nError-rate goes down to 6-8%!\n\nMaybe I am over-fitting with my hyper-parameters? It feels a bit like it, but I don’t have a test set to confirm that… Let’s try that later, when we have a bit more data.\nHere is the notebook: Baseline model: Is there a lamp? Fourth attempt.\nI think we have a good baseline here. So let’s go and throw some more power behind it:\n\nUse a better base model\nUse more training data\n\n\n\nFifth attempt 😊: use a better base model\nI tried a larger resnet model: resnet34, which (with a smaller mini-batch size of 8) resulted in a 4% error rate, without over-fitting for the first few epochs.\nI also had a look here: Which image models are best?, and picked convnext_tiny. This model resulted in a 3% error rate, but is significantly slower. Because we want speedy inference later on, we might want to stick with the resnet models.\nNotebook for this experiment: Is there a lamp? Fifth attempt. I removed the phrase Baseline model, because we are now starting to ramp up the actual base model. Not sure if this is the way, but so far it seems to work. Still a bit worried about the test set that is coming up though…\n\n\nSixth attempt 🤨: collect more data\nMy plan was to now collect more data and do some of the previous steps again, but I changed my mind. The possibility that I am over-fitting by trying different hyper-parameters and picking the winners is substantial.\nThe logical next step would then be to use a test set and see what is going on.\nAlso: I don’t need to create the best lamp classifier in the world, we are here to learn!"
  },
  {
    "objectID": "posts/fly-drone-with-image-classification/index.html#whats-next",
    "href": "posts/fly-drone-with-image-classification/index.html#whats-next",
    "title": "Fly a drone with: Image classification",
    "section": "What’s next?",
    "text": "What’s next?\nAs said, in the next post I will create a test set (from drone frames), figure out test-set best practices, and finalize our classifier. Read on…\n\n\n\n\n\n\n\nSeries: Code, Fly & AI\n\n\n\n← Previous: Tello controller navigation - Part 2"
  },
  {
    "objectID": "posts/tello-controller-navigation-part-1/index.html",
    "href": "posts/tello-controller-navigation-part-1/index.html",
    "title": "Tello controller navigation - Part 1",
    "section": "",
    "text": "Mark Pors\n\n2025-05-07"
  },
  {
    "objectID": "posts/tello-controller-navigation-part-1/index.html#what-happened-with-the-tello",
    "href": "posts/tello-controller-navigation-part-1/index.html#what-happened-with-the-tello",
    "title": "Tello controller navigation - Part 1",
    "section": "What happened with the Tello?",
    "text": "What happened with the Tello?\nThe whole point was to hook up the controller with the Tello/TT, but it was not as simple as I thought. Now that is behind us we can again focus on our original goal: control the RoboMaster TT with the GameSir T1d through a Python script running on a computer.\n\n\n\n\nGameSir T1d controlling the Tello (simulation)"
  },
  {
    "objectID": "posts/tello-controller-navigation-part-1/index.html#simulation-first",
    "href": "posts/tello-controller-navigation-part-1/index.html#simulation-first",
    "title": "Tello controller navigation - Part 1",
    "section": "Simulation first",
    "text": "Simulation first\nI am traveling right now, and didn’t bring my drone, but I did bring the controller so I could at least get some work done. What can we do without the Tello drone? We ask Claude.ai to whip up a simple simulator!\nI am interested in the using simulators anyway. The software/model development cycle with an actual drone in the loop is a bit of a hassle. Especially later on with a larger drone. So, in the style of this blog, we start very simple with the most basic of simulators."
  },
  {
    "objectID": "posts/tello-controller-navigation-part-1/index.html#the-end-result",
    "href": "posts/tello-controller-navigation-part-1/index.html#the-end-result",
    "title": "Tello controller navigation - Part 1",
    "section": "The end result",
    "text": "The end result\nBefore we dive into some of the code, let’s see how it looks like:\n\n\n\nTello simulation\n\n\nAnd you can see it in action here: Gamesir T1d controller with Tello drone simulation demo\nIt all looks a bit lame, but it has already been very useful!\nFirst of all there was a nasty bug in the controller package that periodically told the drone to land (it pressed the A key out of nowhere every now and then).\nSecondly, I needed to think about the architecture of the code, which doesn’t really change when we connect the actual drone.\nThe code is here: tello_controller_sim.py."
  },
  {
    "objectID": "posts/tello-controller-navigation-part-1/index.html#how-does-it-work",
    "href": "posts/tello-controller-navigation-part-1/index.html#how-does-it-work",
    "title": "Tello controller navigation - Part 1",
    "section": "How does it work?",
    "text": "How does it work?\nThe application simulates a Tello drone’s physics and behavior, providing a visual interface that shows:\n\nTop-down and side views of the drone\nTelemetry data (position, rotation, battery)\nController state visualization\nFlight path trail\n\nThe simulation includes realistic features like:\n\nGradual takeoff and landing sequences\nBattery consumption\nPhysical constraints (can’t go below ground)\nInput filtering for smoother control\n\nThe code consists of three classes:\n\nTelloSimulator - Simulates the physics and state of a virtual drone\nFlightController - Handles controller input and sends commands to the drone\nDroneSimulatorApp - Main app that integrates everything with visualization\n\nLet’s have a look at each class in detail:\n\nTelloSimulator\nThis class creates a virtual model of a Tello drone with (more or less) realistic physics. This will be swapped out with the actual drone through the djitellopy library in my next post.\n\nKey Properties:\n\nposition - 3D position vector [x, y, z] in meters\nrotation - Yaw rotation in degrees (0-360°)\nvelocity - 4D vector [left/right, forward/back, up/down, yaw]\nis_connected, is_flying, battery - Drone state tracking\nis_taking_off, is_landing - Transitional states\n\n\n\nKey Methods:\n\nupdate(dt) - Updates position and state based on time delta\ntakeoff() - Initiates gradual ascent to target height\nland() - Initiates gradual descent to ground\nemergency() - Immediately stops motors (safety feature)\nsend_rc_control() - Accepts control values and updates velocity\n\nThe update() method handles all physics calculations:\n\nDifferent behavior during takeoff/landing phases\nVelocity-based position updates with proper trigonometry for directional movement\nBattery drain simulation\nPrevents clipping through the ground\n\n\n\n\nFlightController\nThis class processes raw controller inputs and translates them into drone commands.\n\nKey Features:\n\nInput smoothing with filter_strength (0.8 = heavy smoothing)\ndeadband (0.03) to ignore tiny accidental joystick movements\nSpeed control with speed_multiplier (adjustable via L1/R1 buttons)\nFixed rate command sending (20Hz)\nButton edge detection (reacts to press, not hold)\nMapping follows “European style” (right stick for primary movement)\n\n\n\nMethods:\n\nprocess_input() - Processes controller inputs with filtering\nprocess_buttons() - Handles button presses with edge detection\n\nThe control flow works like this:\n\nRead raw joystick values\nApply deadband (zero out very small inputs)\nApply smoothing filter\nConvert to integer values (-100 to 100)\nSend commands to drone at fixed intervals\n\n\n\n\nDroneSimulatorApp\nThe main application class that brings everything together. It is more or less a pygame application that takes care of the control loop and visualization. The details are not too interesting."
  },
  {
    "objectID": "posts/tello-controller-navigation-part-1/index.html#whats-next",
    "href": "posts/tello-controller-navigation-part-1/index.html#whats-next",
    "title": "Tello controller navigation - Part 1",
    "section": "What’s next?",
    "text": "What’s next?\nIn Part 2 of the Tello controller navigation I will replace the simulator with my RoboMaster TT. Read on…\n\n\n\n\n\n\n\nSeries: Code, Fly & AI\n\n\n\n← Previous: GameSir T1d controller & pygame\n\n\nNext: Tello controller navigation - Part 2 →"
  },
  {
    "objectID": "posts/i-am-a-pilot/index.html",
    "href": "posts/i-am-a-pilot/index.html",
    "title": "Interlude: I’m a Pilot!",
    "section": "",
    "text": "Mark Pors\n\n2025-04-25"
  },
  {
    "objectID": "posts/i-am-a-pilot/index.html#fulfilling-a-dream",
    "href": "posts/i-am-a-pilot/index.html#fulfilling-a-dream",
    "title": "Interlude: I’m a Pilot!",
    "section": "Fulfilling a dream",
    "text": "Fulfilling a dream\nAs a kid I always wanted to be a pilot (not true, it just makes for a more interesting story), and now I just passed the EASA A1/A3 pilot exam!\nYeah, I know, this is a certification for UAV’s. UAV stands for Unmanned Aerial Vehicle, where you as a pilot stay on the ground. We can’t have it all.\nI will need this certification to fly drones outdoors, even the tiny Tello requires the ground-bound pilot to have the authorization to fly drones."
  },
  {
    "objectID": "posts/i-am-a-pilot/index.html#easa-a1a3-exam-practice-app",
    "href": "posts/i-am-a-pilot/index.html#easa-a1a3-exam-practice-app",
    "title": "Interlude: I’m a Pilot!",
    "section": "EASA A1/A3 Exam Practice App",
    "text": "EASA A1/A3 Exam Practice App\nAnyway, to help me study, I (Claude.ai) created a practice app to go over all possible questions for the exam. I thought it might be handy for others so I put it up online here: EASA A1/A3 exam test app. The source code is here.\nBeware: the source PDF with questions was used as the basis for the app, and the LLM made some mistakes. I think I have corrected most/all of them, but if you want to be sure please double check."
  },
  {
    "objectID": "posts/getting-reproducible-results/index.html",
    "href": "posts/getting-reproducible-results/index.html",
    "title": "Interlude: Getting reproducible training results with Fast.ai + PyTorch",
    "section": "",
    "text": "Mark Pors\n\n2025-05-25\nWhile training an image classifier, I became a bit annoyed by the fact that subsequent runs have different results, even though I didn’t change anything.\nHow can we conduct experiments if we don’t know whether the one parameter I modified caused the change, or if something else was responsible, under the hood?\nJump to Conclusion"
  },
  {
    "objectID": "posts/getting-reproducible-results/index.html#what-have-i-tried",
    "href": "posts/getting-reproducible-results/index.html#what-have-i-tried",
    "title": "Interlude: Getting reproducible training results with Fast.ai + PyTorch",
    "section": "What have I tried?",
    "text": "What have I tried?\nWhen I did my experiments, I used this at the start of my notebooks:\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_everything(42)\nAnd in the DataBlock, I provided a seed for the splitter like:\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\nI picked up these snippets in the Fast.ai course, but I don’t really know what each of these statements does in detail. I understand the general idea: ensure that when a random number is generated, it starts from the same seed, so that the random number is consistent each time. Like:\nimport random\n\nprint(\"Without seed:\", random.randint(1, 100), random.randint(1, 100), random.randint(1, 100))\nrandom.seed(42)\nprint(\"With seed 42:\", random.randint(1, 100), random.randint(1, 100), random.randint(1, 100))\nrandom.seed(42)\nprint(\"With seed 42 again:\", random.randint(1, 100), random.randint(1, 100), random.randint(1, 100))\n\nWithout seed: 15 60 38\nWith seed 42: 81 14 3\nWith seed 42 again: 81 14 3\n\nUnfortunately, it is not that simple, proven by the fact that it didn’t work for me. Here is an experiment that shows that loss numbers are not reproducible despite seeding “everything”: Getting reproducible results with Fast.ai / PyTorch - Attempt #1.\nPerhaps a good time to read the documentation."
  },
  {
    "objectID": "posts/getting-reproducible-results/index.html#rtfm",
    "href": "posts/getting-reproducible-results/index.html#rtfm",
    "title": "Interlude: Getting reproducible training results with Fast.ai + PyTorch",
    "section": "RTFM",
    "text": "RTFM\n\nFast.ai docs\nThere is currently no section to be found in the Fast.ai docs covering reproducibility, but there was in version one (deprecated): Getting reproducible results:\n\nIn some situations you may want to remove randomness for your tests. To get identical reproducible results set, you’ll need to set num_workers=1 (or 0) in your DataLoader/DataBunch, and depending on whether you are using torch’s random functions, or python’s (numpy) or both:\n\nseed = 42\n\n# python RNG\nimport random\nrandom.seed(seed)\n\n# pytorch RNGs\nimport torch\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\nif torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n\n# numpy RNG\nimport numpy as np\nnp.random.seed(seed)\nThe Python and numpy parts speak for themselves. I will dive into the PyTorch statements in a moment.\nIf we compare this code with what I did in my first attempt, the only thing that is new is num_workers=1 for dataloaders.\nI tried that, and nope: still not reproducible.\nThe current docs don’t have a section like the one above, but there is a function set_seed (only available in Fast.ai). Let’s have a look at the source code:\ndef set_seed(s, reproducible=False):\n    \"Set random seed for `random`, `torch`, and `numpy` (where available)\"\n    try: torch.manual_seed(s)\n    except NameError: pass\n    try: torch.cuda.manual_seed_all(s)\n    except NameError: pass\n    try: np.random.seed(s%(2**32-1))\n    except NameError: pass\n    random.seed(s)\n    if reproducible:\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\nSo, mostly the same stuff, except: torch.backends.cudnn.benchmark = False. Which leads us to the PyTorch docs…\n\n\nPyTorch docs\nLuckily, there is a note about reproducibility in the PyTorch docs: Reproducibility.\nThere is a warning at the top of the document that I will repeat here.\n\n\n\n\n\n\nWarning\n\n\n\nDeterministic operations are often slower than nondeterministic operations, so single-run performance may decrease for your model. However, determinism may save time in development by facilitating experimentation, debugging, and regression testing.\n\n\nSo it might be worth trying to make everything deterministic during our early experiments (with a smaller dataset and smaller base model), and when we are confident we are on the right track, we switch to nondeterministic.\nSounds great! But first we have to get the deterministic part working in the first place…\nThe PyTorch doc consists of three sections. Let’s go through them one by one.\n\n1. Controlling sources of randomness\nApart from reiterating the assignment of a fixed seed for Python, NumPy, and PyTorch, it also discusses the cudnn.benchmark feature we saw earlier. cudnn is a library built on top of CUDA that accelerates the training of neural networks. Apparently, it starts by trying out a couple of approaches (that’s the benchmarking), and picks the winner for the rest of the training. There is randomness involved in this benchmarking, so setting it to False should make the process deterministic.\nWhat happened when I tried this?\n-&gt; Still no reproducible results!\n\n\n2. Avoiding nondeterministic algorithms\nNow it gets interesting! PyTorch provides a method that might solve our problems: torch.use_deterministic_algorithms(True).\nThis statement instructs all other Torch code to use the deterministic variant of its algorithms, and if this is not possible, to throw an exception.\nAnd that is exactly what I got, an exception:\n\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-12-25c053374517&gt; in &lt;cell line: 0&gt;()\n      1 learn = vision_learner(dls, resnet18, metrics=error_rate)\n----&gt; 2 learn.fine_tune(3)\n\n21 frames\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n    123 \n    124     def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 125         return F.linear(input, self.weight, self.bias)\n    126 \n    127     def extra_repr(self) -&gt; str:\n\nRuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA &gt;= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility\n\nIt didn’t happen in some exotic part of the library, but at the most elemental level that even I understand: return F.linear(input, self.weight, self.bias).\nThe error message is super specific and helpful:\nDeterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA &gt;= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility\nLet’s see what version of CUDA we are using on Colab:\nprint(torch.version.cuda)\n\n12.4\n\nFollowing the advice and setting this at the top of the notebook\nos.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'   # or ':16:8'\ngot me over this hurdle!\n\n\n\n\n\n\nTip\n\n\n\nJust a “Restart session and run all” is not enough after introducing this environment var. You have to actually disconnect from the colab runtime, connect again, and run all.\n\n\nWe got a step further, but another error pops up, now in the backward pass:\n\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-11-af2d886d9870&gt; in &lt;cell line: 0&gt;()\n      1 print(os.environ['CUBLAS_WORKSPACE_CONFIG'])\n      2 learn = vision_learner(dls, resnet18, metrics=error_rate)\n----&gt; 3 learn.fine_tune(3)\n\n22 frames\n/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py in _engine_run_backward(t_outputs, *args, **kwargs)\n    821         unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n    822     try:\n--&gt; 823         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    824             t_outputs, *args, **kwargs\n    825         )  # Calls into the C++ engine to run the backward pass\n\nRuntimeError: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True)'. You can turn off determinism just for this operation, or you can use the 'warn_only=True' option, if that's acceptable for your application. You can also file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation.\n\nOh, oh, bad news. We have no other choice now to set the warn_only flag to True on the use_deterministic_algorithms call. This indeed allows the fine-tuning run to finish, but still not reproducible :(\n\n\n3. DataLoader\nFinally, the PyTorch doc about reproducibility mentions the possibility of seeding dataloader workers identically. As we previously used only one worker, I assume this won’t have any impact on our case. And guess what: it doesn’t 1!\nHere is the notebook where I did all the experiments as recommended in the docs: Getting reproducible results with Fast.ai / PyTorch - Attempt #2. BTW the Colab GPU was of type L4.\nSpeaking of GPUs: would we get reproducible results if we run it on a CPU? The answer is NO (I tried) 2."
  },
  {
    "objectID": "posts/getting-reproducible-results/index.html#lets-get-some-help-hello-claude",
    "href": "posts/getting-reproducible-results/index.html#lets-get-some-help-hello-claude",
    "title": "Interlude: Getting reproducible training results with Fast.ai + PyTorch",
    "section": "Let’s get some help (hello Claude)",
    "text": "Let’s get some help (hello Claude)\nThe documentation didn’t really help. All the tricks to keep everything deterministic failed. It could be the backward pass, in which case we don’t have much hope of resolving it. But how do people train their models if you can’t do any experiments where just one meta-parameter changes?\nAnyway, we are not giving up! Let’s ask our friend the LLM.\nClaude speaks:\n\nWhen you create a DataLoader in PyTorch, it establishes its own internal random state that becomes decoupled from the global random seed. The DataLoader contains several stateful components that persist between training runs:\nThe DataLoader’s internal random number generator gets initialized when you first create it, and this generator maintains its own state independently of any subsequent global seed resets. Even though you call seed_everything(42) before each run, the DataLoader’s internal RNG has already advanced its state from the previous run and won’t reset back to its initial position.\nThe data shuffling mechanism also maintains state. Even with a fixed seed in your RandomSplitter, the underlying shuffling operations within the DataLoader can accumulate state changes that persist across runs, leading to subtle differences in the exact order that batches are presented to your model.\n\nAh, of course, state is the key thing here! Random generators can start nicely with the same seed again, but if the state is not the same as it was before, that has no impact!\nSo let’s apply that wisdom by setting the seeds and recreate the DataLoader before starting a new training run…\nVictory, we have reproducible results now! Have a look at this winning notebook: Getting reproducible results with Fast.ai / PyTorch - Attempt #3.\n\n\n\n\n\n\nWarning\n\n\n\nEven with the most conservative setup, like in this notebook, there can be small differences between runs. I noticed this when changing the GPU model from L4 to A100. This is acknowledged in the PyTorch documentation.\n\n\nThere is just one problem with it: it has become really slow."
  },
  {
    "objectID": "posts/getting-reproducible-results/index.html#hhhhh",
    "href": "posts/getting-reproducible-results/index.html#hhhhh",
    "title": "Interlude: Getting reproducible training results with Fast.ai + PyTorch",
    "section": "^H^H^H^H^H",
    "text": "^H^H^H^H^H\nFor you younger kids: ^H is the backspace character in Unix-type terminals. In other words, let’s delete some of the things we have done to increase reproducibility at the cost of performance.\n\nObservations\n\nnum_workers:\nChanging from num_workers=0 to num_workers=12 3 maintains reproducibility, but only if the workers are seeded as follows (from the PyTorch docs):\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    numpy.random.seed(worker_seed)\n    random.seed(worker_seed)\n\ng = torch.Generator()\ng.manual_seed(42)\n\nDataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    num_workers=num_workers,\n    worker_init_fn=seed_worker,\n    generator=g,\n)\nThis also maintains reproducibility across sessions.\nI’m very happy this works as it speeds up things significantly!\n\n\nuse_deterministic_algorithms:\nRemoving torch.use_deterministic_algorithms(True) also maintains reproducibility, both in the notebook and across sessions 4.\n\n\nset_seed:\nSetting the second parameter of set_seed, reproducible to False (which is the default) basically sets torch.backends.cudnn.benchmark = False in our case. This also maintains reproducibility, both in the notebook and across sessions.\n\n\ncudnn.deterministic:\nSetting torch.backends.cudnn.deterministic = False breaks reproducibility, so it is essential when running meaningful experiments. I hardly saw any performance degradation by setting this to True, but that might be different for other use cases.\nKey findings: as long as you seed every DataLoader worker and keep torch.backends.cudnn.deterministic = True, you can crank num_workers back up, drop torch.use_deterministic_algorithms(True), and rely on the default set_seed(..., reproducible=False): reproducibility still holds across notebook sessions while you recover full training speed.\nThe notebook that has applied the above, and is fast and reproducible, is here: Getting reproducible results with Fast.ai / PyTorch - Attempt #4"
  },
  {
    "objectID": "posts/getting-reproducible-results/index.html#conclusion",
    "href": "posts/getting-reproducible-results/index.html#conclusion",
    "title": "Interlude: Getting reproducible training results with Fast.ai + PyTorch",
    "section": "Conclusion",
    "text": "Conclusion\n\n\n\n\n\n\nNote\n\n\n\nThese conclusions are probably not generic for all training pipelines and base models, but they probably are for convolutional networks like Resnet18. It might be different for transformer-based models, who knows. At least we know what knobs we can turn to get reproducible results.\n\n\nReproducible training comes down to three levers:\n\nSeed every RNG – Python random, NumPy, and each DataLoader worker (worker_init_fn + torch.Generator).\nForce deterministic kernels torch.backends.cudnn.deterministic = True (leave benchmarking off while you experiment).\nStart from the same state: rebuild the DataLoader before each fresh run so its internal sampler is reset.\n\nIf you are using Fast.ai, use the Fast.ai notebook as a reference for reproducible training.\nIn the end it comes down to:\n# First run\nseed_everything(42)\ng = torch.Generator()\ng.manual_seed(42)\ntorch.backends.cudnn.deterministic = True\ndls = create_dataloaders(g)\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(3)\n\n# Next run(s)\nseed_everything(42)\ng = torch.Generator()\ng.manual_seed(42)\ndls = create_dataloaders(g)\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(3)\nIf you prefer to just use PyTorch, use the PyTorch notebook (Claude generated this based on the Fast.ai version) 5.\nOnce you move from experimentation to full-scale training, you can flip torch.backends.cudnn.deterministic = False to potentially regain speed. Just remember that it will sacrifice strict repeatability."
  },
  {
    "objectID": "posts/getting-reproducible-results/index.html#whats-next",
    "href": "posts/getting-reproducible-results/index.html#whats-next",
    "title": "Interlude: Getting reproducible training results with Fast.ai + PyTorch",
    "section": "What’s next",
    "text": "What’s next\nAnother interlude that got a bit out of hand. Time to go back to our image classifier and see if we can improve it. Read on…"
  },
  {
    "objectID": "posts/getting-reproducible-results/index.html#footnotes",
    "href": "posts/getting-reproducible-results/index.html#footnotes",
    "title": "Interlude: Getting reproducible training results with Fast.ai + PyTorch",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs I found out later, it certainly does matter when using more than one worker, which is what you always want in order to have acceptable performance.↩︎\nWith what I know now, this is because I didn’t recreate the dataloaders between runs. If we did that and used a CPU, the results would be reproducible.↩︎\nThis is the default in my case; it is calculated as min(16, os.cpu_count()).↩︎\nThis stays repeatable unless your model calls a layer that can’t guarantee identical results on the GPU; in that case, the numbers may shift a bit between runs, or PyTorch will throw a warning.↩︎\nI found a few excellent blog posts mainly focused on reproducibility with PyTorch: Reproducible Deep Learning Using PyTorch, and PyTorch Reproducibility: A Practical Guide.↩︎"
  },
  {
    "objectID": "posts/tello-controller-navigation-part-2/index.html",
    "href": "posts/tello-controller-navigation-part-2/index.html",
    "title": "Tello controller navigation - Part 2",
    "section": "",
    "text": "Mark Pors\n\n2025-05-11"
  },
  {
    "objectID": "posts/tello-controller-navigation-part-2/index.html#time-to-fly",
    "href": "posts/tello-controller-navigation-part-2/index.html#time-to-fly",
    "title": "Tello controller navigation - Part 2",
    "section": "Time to fly",
    "text": "Time to fly\nThis blog is supposed to show my progress with machine learning, but we haven’t touched that yet at all! That will still be the case in this post, but we are close to wrapping up the drone control.\nIt is time to combine our previous efforts and connect the drone interface (djitellopy) with the controller interface (gamesir-t1d).\n\n\n\n\nGameSir T1d controlling the Tello / RoboMaster TT"
  },
  {
    "objectID": "posts/tello-controller-navigation-part-2/index.html#from-simulator-to-tello",
    "href": "posts/tello-controller-navigation-part-2/index.html#from-simulator-to-tello",
    "title": "Tello controller navigation - Part 2",
    "section": "From simulator to Tello",
    "text": "From simulator to Tello\nLet’s start with the most bare approach and replace the TelloSimulator with a new class TelloDrone, with a similar interface, but driving the Tello via djitellopy.\nThe FlightController stays as it was, so we have all the deadbanding and smoothing goodies there.\nFinally, we need to glue these classes together in a similar way we did with the simulator, but without the “fancy” visualization.\nThe code looks like this at this stage: tt-fly WIP.\nAnd yes, it flies!"
  },
  {
    "objectID": "posts/tello-controller-navigation-part-2/index.html#add-video-streaming",
    "href": "posts/tello-controller-navigation-part-2/index.html#add-video-streaming",
    "title": "Tello controller navigation - Part 2",
    "section": "Add video streaming",
    "text": "Add video streaming\nWe accomplished what we were set out to do! At least, I was :) Now that we have some momentum, let’s add the video stream.\nThe djitellopy library makes it straightforward. In the TelloDrone class we initialize video streaming:\n# Make sure stream is off before turning it on\ntry:\n    self.tello.streamoff()\n    print(\"Stopped any existing video stream\")\nexcept Exception as e:\n    print(f\"Note: {e}\")\n\n# Initialize frame reading with low quality settings\nself.tello.set_video_resolution(self.tello.RESOLUTION_480P)\nself.tello.set_video_fps(self.tello.FPS_30)\nself.tello.set_video_bitrate(self.tello.BITRATE_4MBPS)\n\n# Now turn on the stream\nself.tello.streamon()\n\n# Get the frame reader\nself.frame_read = self.tello.get_frame_read()\n\nprint(\"Video stream initialized successfully\")\nAlso, we add a method to expose the frame reader to the main app:\ndef get_video_frame(self):\n    \"\"\"Get the current video frame from the drone.\"\"\"\n    if self.frame_read is None:\n        return None\n\n    try:\n        frame = self.frame_read.frame\n        if frame is None or frame.size == 0:\n            print(\"Warning: Received empty frame\")\n            return None\n        return frame\n    except Exception as e:\n        print(f\"Error getting video frame: {e}\")\n        return None\nFinally, we read frames from the main app and display it in the pygame window:\n# Display video frame if available\nframe = self.drone.get_video_frame()\nif frame is not None:\n    # Track FPS\n    self.frame_count += 1\n    now = time.time()\n    if now - self.last_frame_time &gt;= 1.0:  # Calculate FPS every second\n        fps = self.frame_count / (now - self.last_frame_time)\n        self.fps_stats.append(fps)\n        if len(self.fps_stats) &gt; 10:\n            self.fps_stats.pop(0)\n        self.frame_count = 0\n        self.last_frame_time = now\n\n    # Convert numpy array to pygame surface\n    try:\n        # Ensure frame has the right format for pygame\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n        # Create a PyGame surface\n        h, w = frame.shape[:2]\n        pygame_frame = pygame.Surface((w, h))\n        pygame.surfarray.blit_array(pygame_frame, np.swapaxes(frame, 0, 1))\n\n        # Scale if needed\n        if pygame_frame.get_size() != (\n            self.video_rect.width,\n            self.video_rect.height,\n        ):\n            pygame_frame = pygame.transform.scale(\n                pygame_frame, (self.video_rect.width, self.video_rect.height)\n            )\n\n        self.screen.blit(pygame_frame, self.video_rect)\n    except Exception as e:\n        print(f\"Error displaying frame: {e}\")\n        # Draw a red border to indicate error\n        pygame.draw.rect(self.screen, (255, 0, 0), self.video_rect, 2)\nelse:\n    # Draw a placeholder for video\n    pygame.draw.rect(self.screen, (40, 40, 60), self.video_rect)\n    no_video = self.font.render(\"No Video Feed\", True, (150, 150, 150))\n    text_rect = no_video.get_rect(center=self.video_rect.center)\n    self.screen.blit(no_video, text_rect)\nSource for the complete code base: tt-fly"
  },
  {
    "objectID": "posts/tello-controller-navigation-part-2/index.html#test-flight",
    "href": "posts/tello-controller-navigation-part-2/index.html#test-flight",
    "title": "Tello controller navigation - Part 2",
    "section": "Test flight!",
    "text": "Test flight!\nTime to try it out with an audience:\n\nI think I worked on this more than enough, and I have a plenty of experience with remote controlling a drone now. Time to move on…"
  },
  {
    "objectID": "posts/tello-controller-navigation-part-2/index.html#whats-next",
    "href": "posts/tello-controller-navigation-part-2/index.html#whats-next",
    "title": "Tello controller navigation - Part 2",
    "section": "What’s next?",
    "text": "What’s next?\nIt is about time to introduce some machine learning. If the goal is to work towards an autonomous drone, at some point I have to get my feet wet. I have plenty of ideas on what to experiment with. Read on for the first one…\n\n\n\n\n\n\n\nSeries: Code, Fly & AI\n\n\n\n← Previous: Tello controller navigation - Part 1\n\n\nNext: Fly a drone with: Image classification →"
  },
  {
    "objectID": "series/index.html",
    "href": "series/index.html",
    "title": "Code, Fly & AI Series",
    "section": "",
    "text": "Welcome to the Code, Fly & AI series — a blog journey into coding autonomous drones with baby steps.\nFollow along as I:\n\nLearn to fly (and crash) drones\nHack with Python and SDKs\nUse AI to give them brains\n\n👇 Posts in this series:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEnter the Tello\n\n\nDiscover how to start building an autonomous scouting drone for hiking using the DJI RoboMaster TT (Tello Talent). Follow a beginner’s journey into drones, coding, and computer vision.\n\n\n\n\n\nApr 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCrash the Tello (with and without code)\n\n\nLearn how to fly and program the DJI Tello (RoboMaster TT) drone, avoid common beginner crashes, and get started with Python coding using the Tello SDK.\n\n\n\n\n\nApr 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nKey navigation & Video snapshots\n\n\nHands-on guide to controlling a Tello drone with Python and pygame: learn responsive keyboard navigation, smooth velocity-based movement, live video streaming, and how to capture snapshots from the drone feed.\n\n\n\n\n\nApr 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGameSir T1d controller & pygame\n\n\nLearn how to connect and use the GameSir T1d controller with your computer for Tello drone control using Python, BLE hacking, and a pygame-compatible wrapper. Step-by-step guide with working code.\n\n\n\n\n\nMay 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTello controller navigation - Part 1\n\n\nSimulate and control a Tello drone using Python, pygame, and a GameSir T1d controller—learn practical code architecture, input smoothing, and debugging techniques for drone development.\n\n\n\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTello controller navigation - Part 2\n\n\nLearn how to connect a GameSir T1d controller to a real Tello drone using Python, integrate live video streaming with Pygame, and set up a foundation for future autonomous drone experiments.\n\n\n\n\n\nMay 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFly a drone with: Image classification\n\n\nLearn how to train a drone to detect obstacles using image classification with fast.ai. This post covers practical steps for building, cleaning, and improving an image classifier, including tips on data collection, augmentation, and model selection.\n\n\n\n\n\nMay 15, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/which-image-models-are-best-updated/index.html",
    "href": "posts/which-image-models-are-best-updated/index.html",
    "title": "Interlude: Which image models are best? UPDATED",
    "section": "",
    "text": "Mark Pors\n\n2025-05-19\nIn my attempts to create an image classifier, following the Fast.ai method as learned in lesson 2, I wanted to try other base-models. A way to help pick the most suitable model was presented in lesson 3.\nJeremy Howard created this amazing notebook that helps selecting a model for your use case: Which image models are best?. Unfortunately the notebook is two years old and is no longer working.\nThe notebook gets its data from the timm computer vision library. It is currently part of Huggingface here."
  },
  {
    "objectID": "posts/which-image-models-are-best-updated/index.html#what-is-timm",
    "href": "posts/which-image-models-are-best-updated/index.html#what-is-timm",
    "title": "Interlude: Which image models are best? UPDATED",
    "section": "What is timm?",
    "text": "What is timm?\nAs far as I understand it, timm is a collection of PyTorch compatible models focussed on computer vision, so all about images: classification, segmentation and more. Apart from a lot of models (old and new, small and large) it has other helpful stuff (helpful for people who know what this all about, not me…, yet! :)).\nFrom the Huggingface docs:\n\ntimm is a library containing SOTA computer vision models, layers, utilities, optimizers, schedulers, data-loaders, augmentations, and training/evaluation scripts.\nIt comes packaged with &gt;700 pretrained models, and is designed to be flexible and easy to use.\n\nIt started as Ross Wightman’s solo project, but now it’s part of the Hugging Face ecosystem.\nFor me timm is relevant, because it allows me to try different models that can improve the is there a lamp classifier, and in the future, for more advanced stuff we will do."
  },
  {
    "objectID": "posts/which-image-models-are-best-updated/index.html#great-visualization-outdated",
    "href": "posts/which-image-models-are-best-updated/index.html#great-visualization-outdated",
    "title": "Interlude: Which image models are best? UPDATED",
    "section": "Great visualization (outdated)",
    "text": "Great visualization (outdated)\nJeremy Howard used the timm data to make is easier to select models to try out. He created a notebook that includes graphs like this:\n\n\n\nScreenshot: Inference top1 accuracy / speed scatter\n\n\nAs said, unfortunately it no longer works if you copy it to your own account (the data sources have moved to Huggingface), and worst: it doesn’t contain information from more recent models.\nThe timm leaderboard is meant to replace it, but it is not as clear IMO. I want the same happy colorful interactive bubbles!\nShould be easy to fix, so let’s get to it!\n\n\n\n\n\n\nWhy not use an LLM?\n\n\n\nYeah, great question! We could also dump all data into an LLM and ask it to create some useful visualizations. It has the advantage that you can chat about the data as well to make the right choice.\nSometimes, however, it feels just easier to play around in a notebook IMO. Also, in this specific case, it is not so easy to get the same results: you are basically describing to the LLM in text what we (Jeremy) did here with code."
  },
  {
    "objectID": "posts/which-image-models-are-best-updated/index.html#visualization-repaired",
    "href": "posts/which-image-models-are-best-updated/index.html#visualization-repaired",
    "title": "Interlude: Which image models are best? UPDATED",
    "section": "Visualization repaired",
    "text": "Visualization repaired\nAs said, it was mostly a matter of pointing to the new data sources and to include a couple of newer models that are SOTA (I asked ChatGPT DeepSearch which ones are). I also found the original data sources and recreated the original charts next to the new ones.\nThe notebook I used for my experiments, with both the new and the old data sources, is here: Which image models are best? (updated).\nI also created a copy on Kaggle here. Kaggle has some problems rendering the plotly charts (which I fixed in part): they don’t show up unless in edit mode…\nFinally I created a version with less text, and more flexibility:\n\nyou can select the benchmark file\nlimit the included model families\nchange what charts will display on x- and y-axis\nwhen new benchmarks are published, they wil be included\n\nIt can be found here: Which image models are best? (improved)."
  },
  {
    "objectID": "posts/which-image-models-are-best-updated/index.html#deploying-it-on-huggingface-with-gradio",
    "href": "posts/which-image-models-are-best-updated/index.html#deploying-it-on-huggingface-with-gradio",
    "title": "Interlude: Which image models are best? UPDATED",
    "section": "Deploying it on Huggingface with Gradio",
    "text": "Deploying it on Huggingface with Gradio\nI asked Claude to port the notebook to a Gradio app, and after some tweaks it seems to work just fine:\nImage Model Performance Analysis\n\n\n\nThe Gradio app on Huggingface\n\n\nPretty cool, no?"
  },
  {
    "objectID": "posts/which-image-models-are-best-updated/index.html#whats-next",
    "href": "posts/which-image-models-are-best-updated/index.html#whats-next",
    "title": "Interlude: Which image models are best? UPDATED",
    "section": "What’s next",
    "text": "What’s next\nThis interlude got a bit out of hand. Time to go back to our image classifier and pick a better model. Read on…"
  },
  {
    "objectID": "posts/how-does-a-neural-net-really-work/index.html",
    "href": "posts/how-does-a-neural-net-really-work/index.html",
    "title": "How does a neural net really work?",
    "section": "",
    "text": "Mark Pors\n\n2025-06-08"
  },
  {
    "objectID": "posts/how-does-a-neural-net-really-work/index.html#fitting-a-function-with-gradient-descent",
    "href": "posts/how-does-a-neural-net-really-work/index.html#fitting-a-function-with-gradient-descent",
    "title": "How does a neural net really work?",
    "section": "Fitting a function with gradient descent",
    "text": "Fitting a function with gradient descent\n\n\n\n\n\n\nNote\n\n\n\nThis notebook is a copy of How does a neural net really work? by Jeremy Howard as part of the Fast.ai course Practical Deep Learning for Coders.\nI added and removed code and markdown here and there reflecting my own understanding of the topic. Not very interesting (aka boring) for 99+% of you, it is mostly for my own reference. My comments are marked with &lt;mark&gt;…&lt;/mark&gt; 1.\nFor drone related stuff, see the Code, Fly & AI Series.\n\n\nA neural network is just a mathematical function. In the most standard kind of neural network, the function:\n\nMultiplies each input by a number of values. These values are known as parameters\nAdds them up for each group of values\nReplaces the negative numbers with zeros\n\nThis represents one “layer”. Then these three steps are repeated, using the outputs of the previous layer as the inputs to the next layer. Initially, the parameters in this function are selected randomly. Therefore a newly created neural network doesn’t do anything useful at all – it’s just random!\n&lt;mark&gt;If you don’t use random parameters, but the same number for each parameter, the function won’t be able to learn anything, because each node in the layer will be the same, and the output will be the same for all inputs.&lt;/mark&gt;\nTo get the function to “learn” to do something useful, we have to change the parameters to make them “better” in some way. We do this using gradient descent. Let’s see how this works…\n\nfrom ipywidgets import interact\nfrom fastai.basics import *\n\nplt.rc('figure', dpi=90)\n\ndef plot_function(f, title=None, min=-2.1, max=2.1, color='r', ylim=None):\n    x = torch.linspace(min,max, 100)[:,None]\n    if ylim: plt.ylim(ylim)\n    plt.plot(x, f(x), color)\n    if title is not None: plt.title(title)\n\nTo learn how gradient descent works, we’re going to start by fitting a quadratic, since that’s a function most of us are probably more familiar with than a neural network. Here’s the quadratic we’re going to try to fit:\n\ndef f(x): return 3*x**2 + 2*x + 1\n\nplot_function(f, \"$3x^2 + 2x + 1$\")\n\n\n\n\n\n\n\n\nThis quadratic is of the form \\(ax^2+bx+c\\), with parameters \\(a=3\\), \\(b=2\\), \\(c=1\\). To make it easier to try out different quadratics for fitting a model to the data we’ll create, let’s create a function that calculates the value of a point on any quadratic:\n\ndef quad(a, b, c, x): return a*x**2 + b*x + c\n\nIf we fix some particular values of a, b, and c, then we’ll have made a quadratic. To fix values passed to a function in python, we use the partial function, like so:\n\ndef mk_quad(a,b,c): return partial(quad, a,b,c)\n\n&lt;mark&gt;So what we did here is freeze the function for a set of fixed parameters. We can now use this function to calculate the value of the quadratic for any x we pass to it. It is like we stored a model to use for inference.&lt;/mark&gt;\nSo for instance, we can recreate our previous quadratic:\n\nf2 = mk_quad(3,2,1)\nplot_function(f2)\n\n\n\n\n\n\n\n\nNow let’s simulate making some noisy measurements of our quadratic f. We’ll then use gradient descent to see if we can recreate the original function from the data.\nHere’s a couple of functions to add some random noise to data:\n\ndef noise(x, scale): return np.random.normal(scale=scale, size=x.shape)\ndef add_noise(x, mult, add): return x * (1+noise(x,mult)) + noise(x,add)\n\nLet’s use the now to create our noisy measurements based on the quadratic above:\n\nnp.random.seed(42)\n\nx = torch.linspace(-2, 2, steps=20)[:,None]\ny = add_noise(f(x), 0.15, 1.5)\n\n&lt;mark&gt;What is that [:,None] doing there? Let’s inspect what is happening…&lt;/mark&gt;\n\na = torch.linspace(-2, 2, steps=4)\nprint(\"Original tensor:\", a)\nprint(\"Shape:\", a.shape)\nprint(\"Number of dimensions:\", a.ndim)\n\nOriginal tensor: tensor([-2.0000, -0.6667,  0.6667,  2.0000])\nShape: torch.Size([4])\nNumber of dimensions: 1\n\n\n\nb = a[:, None]\nprint(\"Shape after [:, None]:\", b.shape)\nprint(\"Number of dimensions now:\", b.ndim)\nprint(\"The actual tensor looks like:\")\nprint(b)\n\nShape after [:, None]: torch.Size([4, 1])\nNumber of dimensions now: 2\nThe actual tensor looks like:\ntensor([[-2.0000],\n        [-0.6667],\n        [ 0.6667],\n        [ 2.0000]])\n\n\n&lt;mark&gt;[:] after a list is typically a slice. In PyTorch and Numpy you can actually use commas to address multiple dimensions of the array/tensor. Apparently, if you use [:, None], it adds a new axis to the array. In this case, it is reshaping the 1D array of four values into a 2D array with one column.&lt;/mark&gt;\n&lt;mark&gt;The reason this is done is that the np.random.normal function expects a 2D array as input, where each row represents a sample and each column represents a feature. By reshaping the array to have one column, we ensure that the input meets this requirement.&lt;/mark&gt;\nHere’s the first few values of each of x and y:\n\nx[:5],y[:5]\n\n(tensor([[-2.0000],\n         [-1.7895],\n         [-1.5789],\n         [-1.3684],\n         [-1.1579]]),\n tensor([[11.8690],\n         [ 6.5433],\n         [ 5.9396],\n         [ 2.6304],\n         [ 1.7947]], dtype=torch.float64))\n\n\nAs you can see, they’re tensors. A tensor is just like an array in numpy. A tensor can be a single number (a scalar or rank-0 tensor), a list of numbers (a vector or rank-1 tensor), a table of numbers (a matrix or rank-0 tensor), a table of tables of numbers (a rank-3 tensor), and so forth.\nWe’re not going to learn much about our data by just looking at the raw numbers, so let’s draw a picture:\n\nplt.scatter(x,y);\n\n\n\n\n\n\n\n\nHow do we find values of a, b, and c which fit this data? One approach is to try a few values and see what fits. Here’s a function which overlays a quadratic on top of our data, along with some sliders to change a, b, and c, and see how it looks:\n\n@interact(a=1.1, b=1.1, c=1.1)\ndef plot_quad(a, b, c):\n    plt.scatter(x,y)\n    plot_function(mk_quad(a,b,c), ylim=(-3,13))\n\n\n\n\n&lt;mark&gt;The widget with sliders doesn’t work here in the blog, but it does in the notebook. You can try it out in the notebook on Colab.&lt;/mark&gt;\nTry moving slider a a bit to the left. Does that look better or worse? How about if you move it a bit to the right? Find out which direction seems to improve the fit of the quadratic to the data, and move the slider a bit in that direction. Next, do the same for slider b: first figure out which direction improves the fit, then move it a bit in that direction. Then do the same for c.\nOK, now go back to slider a and repeat the process. Do it again for b and c as well.\nDid you notice that by going back and doing the sliders a second time that you were able to improve things a bit further? That’s an important insight – it’s only after changing b and c, for instance, that you realise that a actually needs some adjustment based on those new values.\nOne thing that’s making this tricky is that we don’t really have a great sense of whether our fit is really better or worse. It would be easier if we had a numeric measure of that. On easy metric we could use is mean absolute error – which is the distance from each data point to the curve:\n\ndef mae(preds, acts): return (torch.abs(preds-acts)).mean()\n\n&lt;mark&gt;Let’s see how this works. We use the torch.abs function to calculate the absolute value of each element in a tensor, and then we can use the mean tensor method to calculate the mean of those absolute values. The error is reduced to a single number as this is an easy way to see how the model is doing so far.&lt;/mark&gt;\n\na=1.1\nb=1.1\nc=1.1\nf = mk_quad(a,b,c)\nprint(\"Function parameters:\", a, b, c)\nprint(\"Function output for x[:5]:\", f(x[:5]))\nprint(\"Actual values for y[:5]:\", y[:5])\nprint(abs(f(x[:5]) - y[:5]))\nprint(\"Mean Absolute Error:\", mae(f(x), y))\n\nFunction parameters: 1.1 1.1 1.1\nFunction output for x[:5]: tensor([[3.3000],\n        [2.6540],\n        [2.1055],\n        [1.6546],\n        [1.3011]])\nActual values for y[:5]: tensor([[11.8690],\n        [ 6.5433],\n        [ 5.9396],\n        [ 2.6304],\n        [ 1.7947]], dtype=torch.float64)\ntensor([[8.5690],\n        [3.8893],\n        [3.8341],\n        [0.9758],\n        [0.4936]], dtype=torch.float64)\nMean Absolute Error: tensor(2.4219, dtype=torch.float64)\n\n\nWe’ll update our interactive function to print this at the top for us.\nUse this to repeat the approach we took before to try to find the best fit, but this time just use the value of the metric to decide which direction to move each slider, and how far to move it.\nThis time around, try doing it in the opposite order: c, then b, then a.\nYou’ll probably find that you have to go through the set of sliders a couple of times to get the best fit.\n\n@interact(a=1.1, b=1.1, c=1.1)\ndef plot_quad(a, b, c):\n    f = mk_quad(a,b,c)\n    plt.scatter(x,y)\n    loss = mae(f(x), y)\n    plot_function(f, ylim=(-3,12), title=f\"MAE: {loss:.2f}\")\n\n\n\n\nIn a modern neural network we’ll often have tens of millions of parameters to fit, or more, and thousands or millions of data points to fit them to. We’re not going to be able to do that by moving sliders around! We’ll need to automate this process.\nThankfully, that turns out to be pretty straightforward. We can use calculus to figure out, for each parameter, whether we should increase or decrease it."
  },
  {
    "objectID": "posts/how-does-a-neural-net-really-work/index.html#automating-gradient-descent",
    "href": "posts/how-does-a-neural-net-really-work/index.html#automating-gradient-descent",
    "title": "How does a neural net really work?",
    "section": "Automating gradient descent",
    "text": "Automating gradient descent\nThe basic idea is this: if we know the gradient of our mae() function with respect to our parameters, a, b, and c, then that means we know how adjusting (for instance) a will change the value of mae(). If, say, a has a negative gradient, then we know that increasing a will decrease mae(). Then we know that’s what we need to do, since we trying to make mae() as low as possible.\nSo, we find the gradient of mae() for each of our parameters, and then adjust our parameters a bit in the opposite direction to the sign of the gradient.\nTo do this, first we need a function that takes all the parameters a, b, and c as a single vector input, and returns the value mae() based on those parameters:\n\ndef quad_mae(params):\n    f = mk_quad(*params)\n    return mae(f(x), y)\n\nLet’s try it:\n\nquad_mae([1.1, 1.1, 1.1])\n\ntensor(2.4219, dtype=torch.float64)\n\n\nYup, that’s the same as the starting mae() we had in our plot before.\nWe’re first going to do exactly the same thing as we did manually – pick some arbritrary starting point for our parameters. We’ll put them all into a single tensor:\n\nabc = torch.tensor([1.1,1.1,1.1])\n\nTo tell PyTorch that we want it to calculate gradients for these parameters, we need to call requires_grad_():\n\nabc.requires_grad_()\n\ntensor([1.1000, 1.1000, 1.1000], requires_grad=True)\n\n\nWe can now calculate mae(). Generally, when doing gradient descent, the thing we’re trying to minimise is called the loss:\n\nloss = quad_mae(abc)\nloss\n\ntensor(2.4219, dtype=torch.float64, grad_fn=&lt;MeanBackward0&gt;)\n\n\nTo get PyTorch to now calculate the gradients, we need to call backward()\n\nloss.backward()\n\nThe gradients will be stored for us in an attribute called grad:\n\nabc.grad\n\ntensor([-1.3529, -0.0316, -0.5000])\n\n\n&lt;mark&gt;The backward() function calculates the gradient of the loss with respect to the parameters. The gradients are stored in the grad attribute of each parameter tensor. But what do these actual values represent? Jeremy answered this question in the video lesson: the numbers represent the change of the loss when the parameters increase by a value of one (if the slope stayed constant, which is not the case obviously).&lt;/mark&gt;\nAccording to these gradients, all our parameters are a little low. So let’s increase them a bit. If we subtract the gradient, multiplied by a small number, that should improve them a bit:\n\nwith torch.no_grad():\n    abc -= abc.grad*0.01\n    loss = quad_mae(abc)\n\nprint(f'loss={loss:.2f}')\n\nloss=2.40\n\n\nYes, our loss has gone down!\nThe “small number” we multiply is called the learning rate, and is the most important hyper-parameter to set when training a neural network.\nBTW, you’ll see we had to wrap our calculation of the new parameters in with torch.no_grad(). That disables the calculation of gradients for any operations inside that context manager. We have to do that, because abc -= abc.grad*0.01 isn’t actually part of our quadratic model, so we don’t want derivatives to include that calculation.\n&lt;mark&gt;To understand this, I had to go back to the part where the loss was calculated the first time, which resulted in tensor(2.4219, dtype=torch.float64, grad_fn=&lt;MeanBackward0&gt;). This grad_fn is a graph of mathematical functions that have been applied to our tensor abc. When loss.backward() is called, this graph of functions is the basis for the calculation of the derivative of it. The resulting values are assigned to abc.grad.\n&lt;mark&gt;So PyTorch keeps track of all operations that are applied to this tensor, but we don’t want to apply it to the step function (abc -= abc.grad*0.01), because that is not part of the loss function, but rather a step in the optimization process. So we use with torch.no_grad() to tell PyTorch not to track this operation. &lt;/mark&gt;\n&lt;mark&gt;The fact that loss = quad_mae(abc) is also under the no_grad() context is because the loss was already calculated in a previous cell. Yeah, that makes it a bit confusing. Normally we would’t do that, as can be seen below in the loop.&lt;/mark&gt;\nWe can use a loop to do a few more iterations of this:\n\nfor i in range(10):\n    loss = quad_mae(abc)\n    loss.backward()\n    with torch.no_grad(): abc -= abc.grad*0.01\n    print(f'step={i}; loss={loss:.2f}')\n\nstep=0; loss=2.40\nstep=1; loss=2.36\nstep=2; loss=2.30\nstep=3; loss=2.21\nstep=4; loss=2.11\nstep=5; loss=1.98\nstep=6; loss=1.85\nstep=7; loss=1.72\nstep=8; loss=1.58\nstep=9; loss=1.46\n\n\nAs you can see, our loss keeps going down!\nIf you keep running this loop for long enough however, you’ll see that the loss eventually starts increasing for a while. That’s because once the parameters get close to the correct answer, our parameter updates will jump right over the correct answer! To avoid this, we need to decrease our learning rate as we train. This is done using a learning rate schedule, and can be automated in most deep learning frameworks, such as fastai and PyTorch.\n&lt;mark&gt;This might be the case (overshooting), but the code above also contains a bug: the value of abc.grad is accumulated for each iteration of the loop. We therefore need to reset the gradient each iteration:&lt;/mark&gt;\n\nabc = torch.tensor([1.1,1.1,1.1])\nabc.requires_grad_()\n\nfor i in range(100):\n    loss = quad_mae(abc)\n    loss.backward()\n    with torch.no_grad(): abc -= abc.grad*0.1 # I used a larger learning rate here\n    abc.grad.zero_()  # Reset gradients to zero\n    print(f'step={i}; loss={loss:.2f}')\n\nstep=0; loss=2.42\nstep=1; loss=2.21\nstep=2; loss=2.01\nstep=3; loss=1.82\nstep=4; loss=1.67\nstep=5; loss=1.55\nstep=6; loss=1.46\nstep=7; loss=1.38\nstep=8; loss=1.31\nstep=9; loss=1.24\nstep=10; loss=1.19\nstep=11; loss=1.17\nstep=12; loss=1.14\nstep=13; loss=1.12\nstep=14; loss=1.10\nstep=15; loss=1.10\nstep=16; loss=1.10\nstep=17; loss=1.09\nstep=18; loss=1.09\nstep=19; loss=1.09\nstep=20; loss=1.09\nstep=21; loss=1.08\nstep=22; loss=1.08\nstep=23; loss=1.08\nstep=24; loss=1.07\nstep=25; loss=1.07\nstep=26; loss=1.07\nstep=27; loss=1.07\nstep=28; loss=1.06\nstep=29; loss=1.06\nstep=30; loss=1.06\nstep=31; loss=1.05\nstep=32; loss=1.05\nstep=33; loss=1.05\nstep=34; loss=1.05\nstep=35; loss=1.04\nstep=36; loss=1.04\nstep=37; loss=1.04\nstep=38; loss=1.03\nstep=39; loss=1.03\nstep=40; loss=1.03\nstep=41; loss=1.04\nstep=42; loss=1.03\nstep=43; loss=1.03\nstep=44; loss=1.03\nstep=45; loss=1.03\nstep=46; loss=1.02\nstep=47; loss=1.03\nstep=48; loss=1.02\nstep=49; loss=1.02\nstep=50; loss=1.02\nstep=51; loss=1.02\nstep=52; loss=1.01\nstep=53; loss=1.01\nstep=54; loss=1.02\nstep=55; loss=1.01\nstep=56; loss=1.01\nstep=57; loss=1.01\nstep=58; loss=1.01\nstep=59; loss=1.01\nstep=60; loss=1.01\nstep=61; loss=1.01\nstep=62; loss=1.01\nstep=63; loss=1.01\nstep=64; loss=1.01\nstep=65; loss=1.00\nstep=66; loss=1.01\nstep=67; loss=1.01\nstep=68; loss=1.01\nstep=69; loss=1.01\nstep=70; loss=1.00\nstep=71; loss=1.01\nstep=72; loss=1.00\nstep=73; loss=1.00\nstep=74; loss=1.00\nstep=75; loss=0.99\nstep=76; loss=1.00\nstep=77; loss=1.00\nstep=78; loss=1.01\nstep=79; loss=1.00\nstep=80; loss=1.00\nstep=81; loss=0.99\nstep=82; loss=1.00\nstep=83; loss=1.00\nstep=84; loss=1.00\nstep=85; loss=1.00\nstep=86; loss=1.00\nstep=87; loss=0.99\nstep=88; loss=0.99\nstep=89; loss=0.99\nstep=90; loss=1.00\nstep=91; loss=0.99\nstep=92; loss=0.99\nstep=93; loss=0.99\nstep=94; loss=0.99\nstep=95; loss=0.99\nstep=96; loss=1.00\nstep=97; loss=0.99\nstep=98; loss=0.99\nstep=99; loss=0.99"
  },
  {
    "objectID": "posts/how-does-a-neural-net-really-work/index.html#how-a-neural-network-approximates-any-given-function",
    "href": "posts/how-does-a-neural-net-really-work/index.html#how-a-neural-network-approximates-any-given-function",
    "title": "How does a neural net really work?",
    "section": "How a neural network approximates any given function",
    "text": "How a neural network approximates any given function\nBut neural nets are much more convenient and powerful than this example showed, because we can learn much more than just a quadratic with them. How does that work?\nThe trick is that a neural network is a very expressive function. In fact – it’s infinitely expressive. A neural network can approximate any computable function, given enough parameters. A “computable function” can cover just about anything you can imagine: understand and translate human speech; paint a picture; diagnose a disease from medical imaging; write an essay; etc…\nThe way a neural network approximates a function actually turns out to be very simple. The key trick is to combine two extremely basic steps:\n\nMatrix multiplication, which is just multiplying things together and then adding them up\nThe function \\(max(x,0)\\), which simply replaces all negative numbers with zero.\n\nIn PyTorch, the function \\(max(x,0)\\) is written as np.clip(x,0). The combination of a linear function and this max() is called a rectified linear function, and it can be implemented like this:\n\ndef rectified_linear(m,b,x):\n    y = m*x+b\n    return torch.clip(y, 0.)\n\nHere’s what it looks like:\n\nplot_function(partial(rectified_linear, 1,1))\n\n\n\n\n\n\n\n\nBTW, instead of torch.clip(y, 0.), we can instead use F.relu(x), which does exactly the same thing. In PyTorch, F refers to the torch.nn.functional module.\n\nimport torch.nn.functional as F\ndef rectified_linear2(m,b,x): return F.relu(m*x+b)\nplot_function(partial(rectified_linear2, 1,1))\n\n\n\n\n\n\n\n\nTo understand how this function works, try using this interactive version to play around with the parameters m and b:\n\n@interact(m=1.5, b=1.5)\ndef plot_relu(m, b):\n    plot_function(partial(rectified_linear, m,b), ylim=(-1,4))\n\n\n\n\nAs you see, m changes the slope, and b changes where the “hook” appears. This function doesn’t do much on its own, but look what happens when we add two of them together:\n\ndef double_relu(m1,b1,m2,b2,x):\n    return rectified_linear(m1,b1,x) + rectified_linear(m2,b2,x)\n\n@interact(m1=-1.5, b1=-1.5, m2=1.5, b2=1.5)\ndef plot_double_relu(m1, b1, m2, b2):\n    plot_function(partial(double_relu, m1,b1,m2,b2), ylim=(-1,6))\n\n\n\n\nIf you play around with that for a while, you notice something quite profound: with enough of these rectified linear functions added together, you could approximate any function with a single input, to whatever accuracy you like! Any time the function doesn’t quite match, you can just add a few more additions to the mix to make it a bit closer. As an experiment, perhaps you’d like to try creating your own plot_triple_relu interactive function, and maybe even include the scatter plot of our data from before, to see how close you can get?\nThis exact same approach can be expanded to functions of 2, 3, or more parameters.\n&lt;mark&gt;And here ends Jeremy’s notebook. We know that the trick to create a neural net is to add these two concepts together by connecting linear functions with a ReLU function in between. That will be something for the next lesson (chapter four of the book). See you then!&lt;/mark&gt;"
  },
  {
    "objectID": "posts/how-does-a-neural-net-really-work/index.html#footnotes",
    "href": "posts/how-does-a-neural-net-really-work/index.html#footnotes",
    "title": "How does a neural net really work?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDid you know I used the &lt;mark&gt; tag for that? I’m so grateful they named it like that!↩︎"
  },
  {
    "objectID": "posts/crash-the-tello/index.html",
    "href": "posts/crash-the-tello/index.html",
    "title": "Crash the Tello (with and without code)",
    "section": "",
    "text": "Mark Pors\n\n2025-04-24"
  },
  {
    "objectID": "posts/crash-the-tello/index.html#unboxed",
    "href": "posts/crash-the-tello/index.html#unboxed",
    "title": "Crash the Tello (with and without code)",
    "section": "Unboxed",
    "text": "Unboxed\nNow that the drone is out of the box, we’re gonna take it for a spin (and yes it crashed). First with the phone app, then with code.\n\n\n\nCrashed RoboMaster TT (generated by ChatGPT)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThese blog posts are intended to be read in order. If you want to follow along on my journey, start here: Enter the Tello. Or ignore this and keep on reading below."
  },
  {
    "objectID": "posts/crash-the-tello/index.html#first-tt-flight",
    "href": "posts/crash-the-tello/index.html#first-tt-flight",
    "title": "Crash the Tello (with and without code)",
    "section": "First TT flight",
    "text": "First TT flight\nTo fly a drone for the first time it is good to have a basic understanding how these quadcopters work, and what the basic movements are:\n\n\n\n\n\n\n\nMovement\nDescription\n\n\n\n\nthrottle\nincreases or decreases the height of the drone by adjusting all propellers equally\n\n\nyaw\nturns the drone clockwise or counterclockwise by varying propeller speeds\n\n\npitch\nmoves the drone forward or backward by changing speeds between front and back propellers\n\n\nroll\nmoves the drone left or right by varying left and right propeller speeds\n\n\n\n This video explains very clearly how it works: \nThe Tello doesn’t come with a controller, we fly it through the Tello app. I used the Tello app for iPhone. This app hasn’t been updated for a long time and unfortunately it is useless as it crashes all the time. When the app crashes the drone keeps on hovering in the air, so this first crash is just a software crash.\n\n\n\n\n\n\nTip\n\n\n\nIf you want to fly the TT or Tello manually I recommend you get the TelloFPV. It’s not free, but doesn’t crash either.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo really enjoy flying the TT manually you can get a controller that is customized for the Tello: the GameSir T1d. I got one, and it works straight out of the box — even with the TelloFPV app.\n\n\n\nBeware of the ceiling effect\nWhile flying the drone around a bit in my office, I went up a bit too high and the drone crashed when it came near the ceiling. This phenomenon even has a name and is called the “ceiling effect.” Basically, the drone gets sucked up to the ceiling because there is not enough air to push down through the propellers.\nAlright, now for the real fun — controlling the drone with code."
  },
  {
    "objectID": "posts/crash-the-tello/index.html#the-tello-python-sdk",
    "href": "posts/crash-the-tello/index.html#the-tello-python-sdk",
    "title": "Crash the Tello (with and without code)",
    "section": "The Tello Python SDK",
    "text": "The Tello Python SDK\nDJI provides an SDK for the Tello drone that can be used with Python. The SDK can be used to control the drone through its API. The API is fairly basic but supports:\n\nFlight control with takeoff, landing and movement commands\nFlight status information (battery level, height, acceleration, speed)\nStream camera feed\n\nThe Python library we will be using for this SDK is DJITelloPy.\nA couple of useful resources:\n\nSDK 3.0 user guide (includes the API spec).\nRoboMaster Developer Guide (also for RoboMasters on wheels).\nRoboMaster SDK (as provided by DJI).\nDJITelloPy API Reference.\n\nTo give it a try, let’s first install djitellopy:\n\nimport sys\n!{sys.executable} -m pip install djitellopy\n\n\nTake-off, fly, land\nLet’s start with a simple script to take-off, fly, and land the drone.\nTo make this work the computer running the script needs to be connected to the Wifi Access Point the Tello provides. The SSID is of the form TELLO-XXXXXX.\n\nfrom djitellopy import tello\nfrom time import sleep\n\nt = tello.Tello()\nt.connect()\n\nprint(f\"Bat: {t.get_battery()}\")\nprint(f\"Temp: {t.get_temperature()}\")\n\nt.takeoff()\n\n\"\"\"Send RC control via four channels. Command is sent every self.TIME_BTW_RC_CONTROL_COMMANDS seconds.\nArguments:\n    left_right_velocity: -100~100 (left/right)\n    forward_backward_velocity: -100~100 (forward/backward)\n    up_down_velocity: -100~100 (up/down)\n    yaw_velocity: -100~100 (yaw)\n\"\"\"\nt.send_rc_control(0, 50, 0, 0)\nsleep(2)\nt.send_rc_control(30, 0, 0, 0)\nsleep(2)\nt.send_rc_control(0, 0, 0, 0) # don't forget this!\n\nt.land()\nt.end()\n\n\n[INFO] tello.py - 129 - Tello instance was initialized. Host: '192.168.1.85'. Port: '8889'.\n[INFO] tello.py - 438 - Send command: 'command'\n[INFO] tello.py - 462 - Response command: 'ok'\nBat: 100\nTemp: 45.0\n[INFO] tello.py - 438 - Send command: 'takeoff'\n[INFO] tello.py - 462 - Response takeoff: 'ok'\n[INFO] tello.py - 471 - Send command (no response expected): 'rc 0 50 0 0'\n[INFO] tello.py - 471 - Send command (no response expected): 'rc 30 0 0 0'\n[INFO] tello.py - 471 - Send command (no response expected): 'rc 0 0 0 0'\n[INFO] tello.py - 438 - Send command: 'land'\n[INFO] tello.py - 462 - Response land: 'ok'\n\nIt is pretty straightforward. My first attempt made the drone crash though: not having the t.send_rc_control(0, 0, 0, 0) command there tells the drone to move sideways (to the right) while landing, with the expected result.\n\n\n\n\n\n\nNote\n\n\n\nThe code above was executed in my code editor and the output you see there is the actual output. Similar to a frozen Jupyter notebook. This is possible because this blog is powered by Quarto. If you haven’t tried Quarto, maybe check it out.\n\n\n\n\nUsing router mode\nIt is quite annoying that the computer you are working on is not connected to the Internet while testing the code (it is connected to the wifi of the drone). Every five seconds I MUST check with Claude/ChatGPT if I am doing things right, no?\nFor the original Tello there is nothing we can do about that, but the RoboMaster TT has two wifi connection modes:\n\nDirect connection mode (aka AP mode): this is what we did so far, the Tello provides an access point and the computer connects to that.\nRouter mode (aka STA mode): both the TT and the computer connect to the same wifi router, so we are still online if that router is our home router.\n\nThe TT has an expansion kit that contains a small microprocessor that provides wifi and Bluetooth connectivity: ESP32-D2WD. We will have a look later to see what we can do with it, for now we just use the wifi in router mode. There is a tiny switch on the expansion unit that we can toggle between the two modes. It does involve a couple of other steps though, which are outlined here: Connection examples. It didn’t work for me (there is no QR code I can get from the Tello app), so I will show you what I did:\n\n\n\n\n\n\nTip\n\n\n\nBefore continuing, the controller needs to be activated from the Tello app! This happens after the firmware update of this controller. This cost me a couple of days of my life, so next time I’ll RTFM.\n\n\n\nStep 1.\nSet the switch to AP mode first (down).\n\n\nStep 2.\nConnect the PC to the RMTT-xxx network. We are now in direct connection mode, provided by the Wifi on the expansion kit.\n\n\nStep 4:\nRun this script:\n\nimport socket\nimport time\nimport getpass\nimport os\n\n# Environment variables for Wi-Fi credentials just to make it run in a notebook\nos.environ[\"WIFI_SSID\"] = \"MyNetwork\"\nos.environ[\"WIFI_PASSWORD\"] = \"SuperSecret\"\n\nsock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\nsock.settimeout(5)\n\n\ndef get_wifi_credentials():\n    try:\n        ssid = input(\"Wi-Fi SSID: \")\n        password = getpass.getpass(\"Wi-Fi Password: \")\n    except (EOFError, getpass.GetPassWarning, Exception):\n        # Fall back to environment variables\n        print(\"Interactive input not available. Falling back to environment variables.\")\n        ssid = os.getenv(\"WIFI_SSID\")\n        password = os.getenv(\"WIFI_PASSWORD\")\n\n        if not ssid or not password:\n            raise RuntimeError(\n                \"Missing WIFI_SSID or WIFI_PASSWORD environment variable.\"\n            )\n\n    return ssid, password\n\n\n# Step 1: Enter SDK mode\nsock.sendto(b\"command\", (\"192.168.10.1\", 8889))\ntry:\n    response, _ = sock.recvfrom(1024)\n    print(\"Response 1:\", response)\nexcept Exception as e:\n    print(\"No response to command:\", e)\n\ntime.sleep(1)\n\n# Step 2: Send ap command\nssid, password = get_wifi_credentials()\nsock.sendto(b\"ap %s %s\" % (ssid.encode(), password.encode()), (\"192.168.10.1\", 8889))\ntry:\n    response, _ = sock.recvfrom(1024)\n    print(\"Response 2:\", response)\nexcept Exception as e:\n    print(\"No response to ap:\", e)\n\n\n\nStep 5:\nToggle the switch to STA mode (up).\n\n\nStep 6:\nConnect the PC to the wifi network you provided in the script above.\n\n\nStep 7:\nFind the IP address that was assigned to the TT (e.g. in your home router admin settings).\n\n\nStep 8:\nUse this IP address every time you connect to the Tello in your code:\n\nt = tello.Tello(host=\"192.168.1.85\") # the IP address from step 7\n\nt.connect()\n\nFrom here on we can connect to the TT and also be connected to the Internet. Yay!\n\n\n\nThanks!\nI want to thank Murtaza Hassan for getting me started through this video: Drone Programming With Python Course."
  },
  {
    "objectID": "posts/crash-the-tello/index.html#whats-next",
    "href": "posts/crash-the-tello/index.html#whats-next",
    "title": "Crash the Tello (with and without code)",
    "section": "What’s Next",
    "text": "What’s Next\nIn the next episode I’m going to implement keyboard control and stream video frames to the PC and save them on disk on demand (“inspired” by Murtaza 1).\n\n\n\n\n\n\n\nSeries: Code, Fly & AI\n\n\n\n← Previous: Enter the Tello\n\n\nNext: Key navigation & Video snapshots →"
  },
  {
    "objectID": "posts/crash-the-tello/index.html#footnotes",
    "href": "posts/crash-the-tello/index.html#footnotes",
    "title": "Crash the Tello (with and without code)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBorrowed is the real word here. No: stolen!↩︎"
  },
  {
    "objectID": "posts/enter-the-tello/index.html",
    "href": "posts/enter-the-tello/index.html",
    "title": "Enter the Tello",
    "section": "",
    "text": "Mark Pors\n\n2025-04-20"
  },
  {
    "objectID": "posts/enter-the-tello/index.html#combine-the-stuff-you-like",
    "href": "posts/enter-the-tello/index.html#combine-the-stuff-you-like",
    "title": "Enter the Tello",
    "section": "Combine the stuff you like",
    "text": "Combine the stuff you like\n“Combine the stuff you like” is good advice when looking for a new project to work on. I like to follow good advice and so I brainstormed a bit with ChatGPT to find interesting intersections between my interests. Some of my favorite activities — like sex, meditation, and reading — didn’t quite lend themselves to computer vision projects. Others had more potential: hiking in nature, coding, learning about new tech."
  },
  {
    "objectID": "posts/enter-the-tello/index.html#autonomous-scouting-drone",
    "href": "posts/enter-the-tello/index.html#autonomous-scouting-drone",
    "title": "Enter the Tello",
    "section": "Autonomous scouting drone",
    "text": "Autonomous scouting drone\nHaving a drone accompany us on hikes to do some scouting ahead hits the mark for me. It includes hiking and coding, and most important: I need to learn a lot to make a drone do what I want. I have zero experience with drones and very limited experience with computer vision models."
  },
  {
    "objectID": "posts/enter-the-tello/index.html#getting-ready-big-time",
    "href": "posts/enter-the-tello/index.html#getting-ready-big-time",
    "title": "Enter the Tello",
    "section": "Getting ready big time",
    "text": "Getting ready big time\nLike any self-respecting tech nerd, I decided to kick things off in style:\n\nBuy a domain name (most important!)\nGet a logo (as important!)\nBuy the best dev/DIY drone out there\nBuy a top line PC to train models\nBuy another PC to run simulation environments\n\nOK, guilty — I bought the domain (hello dronelab.dev), but tried to stop myself there 1 2. I don’t know anything about drones, and might get bored with the project after a couple of weeks, so better start small."
  },
  {
    "objectID": "posts/enter-the-tello/index.html#enter-the-tello",
    "href": "posts/enter-the-tello/index.html#enter-the-tello",
    "title": "Enter the Tello",
    "section": "Enter the Tello",
    "text": "Enter the Tello\nI stumbled upon a modest little drone from way back in 2018 3 — the DJI Tello — and it was perfect for my first steps. It can fly indoors, is programmable and has a camera.\nThere is an even better version of it with some extra goodies that might come in handy:\n\nan onboard processor\na matrix display\na distance sensor\n\nSo I bought the DJI RoboMaster TT. TT stands for Tello Talent — basically smart kids coding drones at an age I was still figuring out crayons.\nLook at that beauty!\n\n\n\nDJI RoboMaster TT"
  },
  {
    "objectID": "posts/enter-the-tello/index.html#whats-next",
    "href": "posts/enter-the-tello/index.html#whats-next",
    "title": "Enter the Tello",
    "section": "What’s next?",
    "text": "What’s next?\nIn the next episode I will play with the RoboMaster TT, both with and without code to control it. It includes different ways to crash it. Read on…\n\n\n\n\n\n\n\nSeries: Code, Fly & AI\n\n\n\n\n\n\nNext: Crash the Tello →"
  },
  {
    "objectID": "posts/enter-the-tello/index.html#footnotes",
    "href": "posts/enter-the-tello/index.html#footnotes",
    "title": "Enter the Tello",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI always buy the domain first, even before I know what I’m doing.↩︎\nI didn’t really stop myself there, I also created a logo. Technically ChatGPT did: ↩︎\n2018, also the last time I blogged something: decentralized.blog.↩︎"
  },
  {
    "objectID": "posts/gamesir-t1d-package/index.html",
    "href": "posts/gamesir-t1d-package/index.html",
    "title": "Interlude: GameSir T1d Python package",
    "section": "",
    "text": "Mark Pors\n\n2025-05-04"
  },
  {
    "objectID": "posts/gamesir-t1d-package/index.html#lets-package-up-the-gamesir-t1d-wrapper",
    "href": "posts/gamesir-t1d-package/index.html#lets-package-up-the-gamesir-t1d-wrapper",
    "title": "Interlude: GameSir T1d Python package",
    "section": "Let’s package up the Gamesir T1d wrapper",
    "text": "Let’s package up the Gamesir T1d wrapper\nFor some context: this post is a tangent on this article about making the GameSir T1d controller work with pygame.\nI know, no one is ever going to use it apart from me. The Gamesir T1d is ancient, and the use case (using it with pygame to control a Tello) is also not very common. But I’m here to learn and try things out, and this gives me the opportunity to play with:\n\nuv: this “new” package manager is pretty great, and I need to use it more often to get rid of old habits (pip, venv, pyenv, etc.).\nCreate a pypi package: I have never created and published a pypi package, so fun to give that a shot.\nClaude code: I have used this in the past and it seems promising, so let’s vibe-code our way to a package.\n\nBut first, a logo! :P\n\n\n\nGameSir T1d Python package"
  },
  {
    "objectID": "posts/gamesir-t1d-package/index.html#six-steps-to-a-published-package",
    "href": "posts/gamesir-t1d-package/index.html#six-steps-to-a-published-package",
    "title": "Interlude: GameSir T1d Python package",
    "section": "Six steps to a published package",
    "text": "Six steps to a published package\n\nStep 1: Ditch Claude code\nIt is really very easy to create a pypi package, but Claude code overcomplicated things, and in the end it got stuck. So the first step: ditch Claude code. I’m not a big fan of vibe coding (at least not today), and this confirms again why.\n\n\nStep 2: Create a project\nuv has a built-in way to create projects we want to publish as a package, using the --lib switch:\nuv init --lib gamesir-t1d\ncd gamesir-t1d\nNext, we need to add bleak as a dependency:\nuv add bleak\nFor the example code to work we also need to add pygame.\nuv add pygame\nIt creates the files (and virtual env) we need as a starting point:\n.\n├── pyproject.toml\n├── README.md\n├── src\n│   └── gamesir_t1d\n│       └── __init__.py\n└── uv.lock\n\n\nStep 3: Add modules\nThis is simply moving the code we created in the previous post post into the right spots. Which results in:\n.\n├── LICENSE\n├── pyproject.toml\n├── README.md\n├── src\n│   └── gamesir_t1d\n│       ├── __init__.py\n│       ├── controller.py\n│       ├── examples\n│       │   └── pygame_example.py\n│       └── tools\n│           ├── __init__.py\n│           └── ble_scanner.py\n└── uv.lock\nMake sure the pyproject.toml has the right metadata. The latest version is here: pyproject.toml.\nAn interesting feature is to include scripts in a package. After installing a package you can run these scripts on the command line. This is done in the pyproject.toml as:\n[project.scripts]\ngamesir-scan = \"gamesir_t1d.tools.ble_scanner:run_scanner\"\nThis is perfect for us to make the BLE scanner available, so users can figure out the name of their controller.\n\n\nStep 4: Build and publish\nuv build\nThis creates two files in the /dist directory:\n\ngamesir_t1d-0.1.1-py3-none-any.whl\ngamesir_t1d-0.1.1.tar.gz\n\nwhich are the binary and the source version of the package.\nLet’s publish it to test.pypi.org first:\n\nCreate an account on test.pypi.org\nObtain an API token\nRun this command:\n\nuv uv publish --token pypi-TEST_TOKEN_HERE --publish-url https://test.pypi.org/legacy/\n\n\nStep 5: Test\nTo test it, we create a new project and install the package there.\nuv init gstest\ncd gstest\npip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple gamesir-t1d\npython -c \"import gamesir_t1d; print(gamesir_t1d.__version__)\"\nIf that prints the version number, it all works!\nNow, let’s try the scanner (make sure the controller is switched on):\n$ gamesir-scan\n\nMake sure the GameSir-T1d controller is turned on and in pairing mode.\n(Typically hold power button until LEDs flash rapidly)\nPress Enter to start scanning...\nStarting BLE scan for GameSir-T1d controller...\nScanning for BLE devices (timeout: 3.0s)...\nFound 14 Bluetooth devices:\n1. Name: None, Address: E6682D99-DC5A-EE5A-9E95-DAC5BF163FC1\n...\n6. Name: Gamesir-T1d-39BD, Address: FDF00BC3-1DEE-1525-0B34-7E2D3391C401\n...\n13. Name: None, Address: 19EC6BCE-BE63-CD90-E9D6-9C91EA838008\n14. Name: None, Address: 3222F9B7-2970-77BF-6814-9FB82F843839\nFound controller: Gamesir-T1d-39BD, Address: FDF00BC3-1DEE-1525-0B34-7E2D3391C401\nAttempting to connect to Gamesir-T1d-39BD...\nSuccessfully connected to Gamesir-T1d-39BD!\n\nAvailable services and characteristics:\nService: 00008650-0000-1000-8000-00805f9b34fb\n  Characteristic: 00008651-0000-1000-8000-00805f9b34fb\n    Properties: ['read', 'write', 'notify', 'indicate']\n  Characteristic: 00008655-0000-1000-8000-00805f9b34fb\n    Properties: ['read', 'write', 'notify', 'indicate']\n  Characteristic: 0000865f-0000-1000-8000-00805f9b34fb\n    Properties: ['read', 'write', 'notify', 'indicate']\nService: 0000180a-0000-1000-8000-00805f9b34fb\n  Characteristic: 00002a24-0000-1000-8000-00805f9b34fb\n    Properties: ['read']\n  Characteristic: 00002a25-0000-1000-8000-00805f9b34fb\n    Properties: ['read']\n# Assuming you've installed the package with the [examples] extra\n  Characteristic: 00002a27-0000-1000-8000-00805f9b34fb\n    Properties: ['read']\n  Characteristic: 00002a26-0000-1000-8000-00805f9b34fb\n    Properties: ['read']\n  Characteristic: 00002a50-0000-1000-8000-00805f9b34fb\n    Properties: ['read']\n\nConnection successful. Press Ctrl+C to exit...\nNow we have the controller ID, let’s run a basic script:\nfrom gamesir_t1d import GameSirT1dPygame\n\n# Create the controller object with your controller's name\ncontroller = GameSirT1dPygame(\"Gamesir-T1d-39BD\")  # Replace XXXX with your controller ID\n\n# Initialize the controller (starts BLE connection)\ncontroller.init()\n\n# Read axes and buttons using the pygame-compatible interface\nleft_x = controller.get_axis(0)  # Range: -1.0 to 1.0\nleft_y = controller.get_axis(1)  # Range: -1.0 to 1.0\na_button = controller.get_button(0)  # 1 for pressed, 0 for not pressed\n\n# Check if the controller is connected\nif controller.is_connected():\n    print(\"Controller is connected!\")\n\n# Clean up when done\ncontroller.quit()\n\nScanning for Gamesir-T1d-39BD...\nFound Gamesir-T1d-39BD at FDF00BC3-1DEE-1525-0B34-7E2D3391C401\nConnecting...\nConnected!\nController is connected!\n\n\n\nStep 6: Publish for real!\nIt all seems to work, so now we can publish the package on pypi.org! Again, create an account and generate an API token first, then:\nuv publish --token pypi-YourToken\nAnd there we have it, our package: pypi.org/project/gamesir-t1d/!\nWe repeat the same steps as before for testing:\nuv init gstest2\ncd gstest2\nuv add gamesir-t1d\npython -c \"import gamesir_t1d; print(gamesir_t1d.__version__)\"\nNo need to run the scanner, the controller ID is still what is was :)\nNow, run the examples included in the package:\nfrom gamesir_t1d.examples.pygame_example import test_without_pygame\n\ntest_without_pygame(\"Gamesir-T1d-XXXX\")  # Replace XXXX with your controller ID\nand\nfrom gamesir_t1d.examples import run\n\nrun(\"Gamesir-T1d-XXXX\")  # Replace XXXX with your controller ID\nThis will run the test as shown in this video.\nThe source code can be found here."
  },
  {
    "objectID": "posts/gamesir-t1d-package/index.html#wrapping-it-up",
    "href": "posts/gamesir-t1d-package/index.html#wrapping-it-up",
    "title": "Interlude: GameSir T1d Python package",
    "section": "Wrapping it up!",
    "text": "Wrapping it up!\nHaha, dad joke there.\nThere is nothing to wrap up anyway.\nIf you are interested in reading more about drones, coding, and machine learning (soon), start here: Code, Fly & AI."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DroneLab - Coding Autonomous Drones in Baby Steps",
    "section": "",
    "text": "MNIST basics - Training a Digit Classifier\n\n\n\nmachine learning\n\nfast.ai\n\n\n\nA hands-on exploration of neural network training fundamentals using the MNIST dataset, covering pixel representation, broadcasting, gradient descent, and building basic digit classifiers with PyTorch.\n\n\n\n\n\nJun 11, 2025\n\n\nMark Pors\n\n\n\n\n\n\n\n\n\n\n\n\nHow does a neural net really work?\n\n\n\nmachine learning\n\nfast.ai\n\n\n\nA detailed exploration of neural networks fundamentals, including gradient descent and ReLU functions, with interactive examples and practical Python code using PyTorch.\n\n\n\n\n\nJun 8, 2025\n\n\nMark Pors\n\n\n\n\n\n\n\n\n\n\n\n\nInterlude: Getting reproducible training results with Fast.ai + PyTorch\n\n\n\nmachine learning\n\ncolab\n\ncomputer vision\n\nfast.ai\n\npytorch\n\n\n\nWhy your Fast.ai or PyTorch experiments keep giving different results even when you think you’ve seeded everything properly. Spoiler: it’s not just about random seeds - DataLoaders have hidden state that screws things up. Here’s how to actually fix it.\n\n\n\n\n\nMay 25, 2025\n\n\nMark Pors\n\n\n\n\n\n\n\n\n\n\n\n\nInterlude: Which image models are best? UPDATED\n\n\n\nmachine learning\n\ntimm\n\ncomputer vision\n\nfast.ai\n\nopen-source\n\n\n\nA practical guide to choosing the best image classification models using updated timm data, interactive notebooks, and a Gradio app. Learn how to compare, visualize, and select top-performing models for your computer vision projects.\n\n\n\n\n\nMay 19, 2025\n\n\nMark Pors\n\n\n\n\n\n\n\n\n\n\n\n\nFly a drone with: Image classification\n\n\n\ndrone\n\ncode\n\ntello\n\nmachine learning\n\nimage classification\n\nfast.ai\n\n\n\nLearn how to train a drone to detect obstacles using image classification with fast.ai. This post covers practical steps for building, cleaning, and improving an image classifier, including tips on data collection, augmentation, and model selection.\n\n\n\n\n\nMay 15, 2025\n\n\nMark Pors\n\n\n\n\n\n\n\n\n\n\n\n\nTello controller navigation - Part 2\n\n\n\ndrone\n\ncode\n\npygame\n\ntello\n\n\n\nLearn how to connect a GameSir T1d controller to a real Tello drone using Python, integrate live video streaming with Pygame, and set up a foundation for future autonomous drone experiments.\n\n\n\n\n\nMay 11, 2025\n\n\nMark Pors\n\n\n\n\n\n\n\n\n\n\n\n\nTello controller navigation - Part 1\n\n\n\ndrone\n\ncode\n\npygame\n\nsimulator\n\ntello\n\n\n\nSimulate and control a Tello drone using Python, pygame, and a GameSir T1d controller—learn practical code architecture, input smoothing, and debugging techniques for drone development.\n\n\n\n\n\nMay 7, 2025\n\n\nMark Pors\n\n\n\n\n\n\n\n\n\n\n\n\nInterlude: GameSir T1d Python package\n\n\n\npygame\n\ncode\n\nopen-source\n\ntello\n\n\n\nStep-by-step guide to packaging, publishing, and using a Python library for the GameSir T1d Bluetooth controller with pygame, using the modern uv toolchain.\n\n\n\n\n\nMay 4, 2025\n\n\nMark Pors\n\n\n\n\n\n\n\n\n\n\n\n\nGameSir T1d controller & pygame\n\n\n\ndrone\n\ncode\n\npygame\n\ntello\n\n\n\nLearn how to connect and use the GameSir T1d controller with your computer for Tello drone control using Python, BLE hacking, and a pygame-compatible wrapper. Step-by-step guide with working code.\n\n\n\n\n\nMay 2, 2025\n\n\nMark Pors\n\n\n\n\n\n\n\n\n\n\n\n\nKey navigation & Video snapshots\n\n\n\ndrone\n\ncode\n\npygame\n\ntello\n\n\n\nHands-on guide to controlling a Tello drone with Python and pygame: learn responsive keyboard navigation, smooth velocity-based movement, live video streaming, and how to capture snapshots from the drone feed.\n\n\n\n\n\nApr 27, 2025\n\n\nMark Pors\n\n\n\n\n\n\n\n\n\n\n\n\nInterlude: I’m a Pilot!\n\n\n\ndrone\n\nopen-source\n\n\n\nPassed the EASA A1/A3 drone pilot exam and built a free web-based practice app. Learn how to prepare for the EASA UAV certification and access open-source study tools.\n\n\n\n\n\nApr 25, 2025\n\n\nMark Pors\n\n\n\n\n\n\n\n\n\n\n\n\nCrash the Tello (with and without code)\n\n\n\ndrone\n\ncode\n\ntello\n\n\n\nLearn how to fly and program the DJI Tello (RoboMaster TT) drone, avoid common beginner crashes, and get started with Python coding using the Tello SDK.\n\n\n\n\n\nApr 24, 2025\n\n\nMark Pors\n\n\n\n\n\n\n\n\n\n\n\n\nEnter the Tello\n\n\n\ndrone\n\ntello\n\n\n\nDiscover how to start building an autonomous scouting drone for hiking using the DJI RoboMaster TT (Tello Talent). Follow a beginner’s journey into drones, coding, and computer vision.\n\n\n\n\n\nApr 20, 2025\n\n\nMark Pors\n\n\n\n\n\nNo matching items"
  }
]